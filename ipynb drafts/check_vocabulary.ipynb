{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ain t</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aren t</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can t</td>\n",
       "      <td>cannot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can t ve</td>\n",
       "      <td>cannot have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'cause</td>\n",
       "      <td>because</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      short         long\n",
       "0     ain t      are not\n",
       "1    aren t      are not\n",
       "2     can t       cannot\n",
       "3  can t ve  cannot have\n",
       "4    'cause      because"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr = pd.read_csv(\"./materials/contractions.csv\")\n",
    "contr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {}\n",
    "for key, val in zip(contr['short'],contr['long']):\n",
    "    contractions[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jimmy’s Breakfast</td>\n",
       "      <td>This is my friend Jimmy. He is from India. Jim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Girl from Green Valley</td>\n",
       "      <td>Green Valley was a small village near the Icy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Alice in Wonderland</td>\n",
       "      <td>Alice was beginning to get very tired of sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Volkswagen's emissions cheating will cost the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My Friends</td>\n",
       "      <td>My name is Sonya. I am 20 and I come from Moro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     Title  \\\n",
       "0   1         Jimmy’s Breakfast   \n",
       "1   2  A Girl from Green Valley   \n",
       "2   3       Alice in Wonderland   \n",
       "3   4                Volkswagen   \n",
       "4   5                My Friends   \n",
       "\n",
       "                                                Text  \n",
       "0  This is my friend Jimmy. He is from India. Jim...  \n",
       "1  Green Valley was a small village near the Icy ...  \n",
       "2  Alice was beginning to get very tired of sitti...  \n",
       "3  Volkswagen's emissions cheating will cost the ...  \n",
       "4  My name is Sonya. I am 20 and I come from Moro...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_excel(\"./materials/texts.xlsx\")\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660 50 81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_vocabulary = []\n",
    "with open(\"./materials/A1_vocab_processed.txt\", \"r\") as voc:\n",
    "    for word in voc.readlines():\n",
    "        basic_vocabulary.append(word[:-1])\n",
    "#basic_vocabulary = set(basic_vocabulary)\n",
    "basic_vocabulary\n",
    "\n",
    "adjectives = []\n",
    "with open(\"./materials/common_adj.txt\", \"r\") as common_adj:\n",
    "    for word in common_adj.readlines():\n",
    "        adjectives.append(word[:-1])\n",
    "        \n",
    "common_uncountable = []\n",
    "with open(\"./materials/common_unountable_manually_filtered.txt\", \"r\") as common_unctbl:\n",
    "    for word in common_unctbl.readlines():\n",
    "        common_uncountable.append(word[:-1])\n",
    "\n",
    "print(len(basic_vocabulary), len(adjectives), len(common_uncountable))\n",
    "final_basic_vocabulary = basic_vocabulary\n",
    "final_basic_vocabulary.extend(adjectives)\n",
    "final_basic_vocabulary.extend(common_uncountable)\n",
    "len(final_basic_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_basic_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_basic_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And343', 'RB'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('...', ':'),\n",
       " ('received', 'VBD'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"And343 now for... received something completely different\")\n",
    "pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receive\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"received\", pos = wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(string.punctuation)\n",
    "s = string.punctuation + \"’\"\n",
    "#\"'\" in s\n",
    "\"’\"in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do not'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions['don t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'\" in exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am going to visit U.S. and I wnat to go with you...', 'I want to say']\n"
     ]
    }
   ],
   "source": [
    "text = \" I am going to visit U.S. and I wnat to go with you... I want to say\"\n",
    "#print('\\n-----\\n'.join(sent_detector.tokenize(text.strip())))\n",
    "print(sent_detector.tokenize(text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sasdsa'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"   sasdsa\".lstrip()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i cannot visit u s  and i want to go with your daddy s gun  ',\n",
       " 'i want to say hello to her brother ']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "test_line = \"I can't  visit U.S. and I want to go with your daddy's gun ... I want to say hello to her brother\"#’'\n",
    "exclude = string.punctuation + \"’\" +\"“\" + \"”\"#exclude ' for processing of such type of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def punct_setnence_lower (line, sent_detector, debug = False, deep_debug = False):\n",
    "    initial_sentences_list = sent_detector.tokenize(line.strip())\n",
    "    final_sentences_list = []\n",
    "    \n",
    "    for sentence in initial_sentences_list:\n",
    "        if(debug): print(\"sentence:\", sentence)\n",
    "        cleaned_line = ''\n",
    "        for word in sentence.split():\n",
    "            if(debug): print(word)\n",
    "            clean_word =''\n",
    "            for char in word:\n",
    "                if(deep_debug): print(\"char before cleaning:\", char)\n",
    "                if char not in exclude:\n",
    "                    clean_word += char.lower()\n",
    "                    if(deep_debug): print(\"char after cleaning:\", char.lower())\n",
    "                else:\n",
    "                    clean_word += ' '\n",
    "                    if(deep_debug): print(\"char has been deleted\")\n",
    "            clean_word = clean_word.lstrip()\n",
    "            if(debug):print(\"non punctuation and lower:\",clean_word)       \n",
    "\n",
    "            if (clean_word in contractions):\n",
    "                if(debug):print(\"word before contractions parsing:\",clean_word)\n",
    "                clean_word = ''.join(contractions[clean_word])\n",
    "                if(debug):print(\"word after contractions parsing:\",clean_word)\n",
    "            if(debug): print(\"finally cleaned word/s:\",clean_word)    \n",
    "            #cleaned_element = ''.join(cleaned_element)\n",
    "            #print(\"cleaned_element\",cleaned_element)\n",
    "            cleaned_line += clean_word + ' '\n",
    "        final_sentences_list.append(cleaned_line)\n",
    "    return final_sentences_list\n",
    "            \n",
    "clean_line = punct_setnence_lower(test_line, sent_detector)\n",
    "clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ВЫДЕЛЯТЬ В ОЧИЩЕННОМ НЕЛЕММАТИЗИРОВАННОМ ТЕКСТЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i i PRON PRP nsubj {74: 94, 'PronType': 'prs'}\n",
      "can can VERB MD aux {74: 99, 'VerbType': 'mod'}\n",
      "not not ADV RB neg {74: 85, 'Degree': 'pos'}\n",
      "visit visit VERB VB ROOT {74: 99, 'VerbForm': 'inf'}\n",
      "u u PRON PRP det {74: 94, 'PronType': 'prs'}\n",
      "s s PART POS dobj {74: 93, 'Poss': 'yes'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-ac2f3d7d35da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n\u001b[0;32m      7\u001b[0m           \u001b[1;31m#token.shape_, token.is_alpha, token.is_stop,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m           nlp. vocab.morphology.tag_map[token.tag_])\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "#doc = nlp(u\"I can't  visit U.S. and I want to go with your daddy's gun ... I want to say hello to her brother\")\n",
    "doc = nlp(clean_line[0])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          #token.shape_, token.is_alpha, token.is_stop,\n",
    "          nlp.vocab.morphology.tag_map[token.tag_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{74: 99, 'VerbForm': 'fin', 'Tense': 'past'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [], 'text': 'word'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'before'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'contractions'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'parsing'},\n",
       " {'text': ':\\n'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "m.analyze(\"word before contractions parsing:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ВЫДЕЛЯТЬ В ЛЕММАТИЗИРОВАННОМ ТЕКСТЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: I can't  visit U.S. and I want to go with your daddy's gun...\n",
      "I\n",
      "non punctuation and lower: i\n",
      "finally cleaned word/s: i\n",
      "can't\n",
      "non punctuation and lower: can t\n",
      "word before contractions parsing: can t\n",
      "word after contractions parsing: cannot\n",
      "finally cleaned word/s: cannot\n",
      "visit\n",
      "non punctuation and lower: visit\n",
      "finally cleaned word/s: visit\n",
      "U.S.\n",
      "non punctuation and lower: u s \n",
      "finally cleaned word/s: u s \n",
      "and\n",
      "non punctuation and lower: and\n",
      "finally cleaned word/s: and\n",
      "I\n",
      "non punctuation and lower: i\n",
      "finally cleaned word/s: i\n",
      "want\n",
      "non punctuation and lower: want\n",
      "finally cleaned word/s: want\n",
      "to\n",
      "non punctuation and lower: to\n",
      "finally cleaned word/s: to\n",
      "go\n",
      "non punctuation and lower: go\n",
      "finally cleaned word/s: go\n",
      "with\n",
      "non punctuation and lower: with\n",
      "finally cleaned word/s: with\n",
      "your\n",
      "non punctuation and lower: your\n",
      "finally cleaned word/s: your\n",
      "daddy's\n",
      "non punctuation and lower: daddy s\n",
      "finally cleaned word/s: daddy s\n",
      "gun...\n",
      "non punctuation and lower: gun   \n",
      "finally cleaned word/s: gun   \n",
      "finally cleaned setnence: ['i', 'can', 'not', 'visit', 'u', 's', 'and', 'i', 'want', 'to', 'go', 'with', 'your', 'daddy', 's', 'gun']\n",
      "sentence: I want to say hello to them\n",
      "I\n",
      "non punctuation and lower: i\n",
      "finally cleaned word/s: i\n",
      "want\n",
      "non punctuation and lower: want\n",
      "finally cleaned word/s: want\n",
      "to\n",
      "non punctuation and lower: to\n",
      "finally cleaned word/s: to\n",
      "say\n",
      "non punctuation and lower: say\n",
      "finally cleaned word/s: say\n",
      "hello\n",
      "non punctuation and lower: hello\n",
      "finally cleaned word/s: hello\n",
      "to\n",
      "non punctuation and lower: to\n",
      "finally cleaned word/s: to\n",
      "them\n",
      "non punctuation and lower: them\n",
      "finally cleaned word/s: them\n",
      "finally cleaned setnence: ['i', 'want', 'to', 'say', 'hello', 'to', 'them']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'can',\n",
       "  'not',\n",
       "  'visit',\n",
       "  'u',\n",
       "  's',\n",
       "  'and',\n",
       "  'i',\n",
       "  'want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'with',\n",
       "  'your',\n",
       "  'daddy',\n",
       "  's',\n",
       "  'gun'],\n",
       " ['i', 'want', 'to', 'say', 'hello', 'to', 'them']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_line = \"I can't  visit U.S. and I want to go with your daddy's gun... I want to say hello to them\"#’'\n",
    "exclude = string.punctuation + \"’\" +\"“\" + \"”\"#exclude ' for processing of such type of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text_line (line, lemmatizer, debug = False, deep_debug = False):\n",
    "    initial_sentences_list = sent_detector.tokenize(line.strip())\n",
    "    final_sentences_list = []\n",
    "    for sentence in initial_sentences_list:\n",
    "        if(debug): print(\"sentence:\", sentence)\n",
    "        cleaned_line = ''\n",
    "        for word in sentence.split():\n",
    "            if(debug): print(word)\n",
    "            clean_word =''\n",
    "            for char in word:\n",
    "                if(deep_debug): print(\"char before cleaning:\", char)\n",
    "                if char not in exclude:\n",
    "                    clean_word += char.lower()\n",
    "                    if(deep_debug): print(\"char after cleaning:\", char.lower())\n",
    "                else:\n",
    "                    clean_word += ' '\n",
    "                    if(deep_debug): print(\"char has been deleted\")\n",
    "            clean_word = clean_word.lstrip()\n",
    "            if(debug):print(\"non punctuation and lower:\",clean_word)       \n",
    "\n",
    "            if (clean_word in contractions):\n",
    "                if(debug):print(\"word before contractions parsing:\",clean_word)\n",
    "                clean_word = ''.join(contractions[clean_word])\n",
    "                if(debug):print(\"word after contractions parsing:\",clean_word)\n",
    "            if(debug): print(\"finally cleaned word/s:\",clean_word)    \n",
    "            #cleaned_element = ''.join(cleaned_element)\n",
    "            #print(\"cleaned_element\",cleaned_element)\n",
    "            cleaned_line += clean_word + ' '\n",
    "\n",
    "        #print(\"cleaned_line\", cleaned_line)\n",
    "\n",
    "        #лемматизируем очищенные слова\n",
    "        text = word_tokenize(cleaned_line)\n",
    "        final_lemm_line = []\n",
    "        pos_text = pos_tag(text)\n",
    "        #print(pos_text)\n",
    "        for el in pos_text:\n",
    "            current_pos = get_wordnet_pos(el[1])\n",
    "            if(current_pos):\n",
    "                final_lemm_line.append(lemmatizer.lemmatize(el[0], pos = current_pos))\n",
    "                #print(el[0],\"pos = \", current_pos)\n",
    "            else:\n",
    "                final_lemm_line.append(lemmatizer.lemmatize(el[0]))\n",
    "        final_sentences_list.append(final_lemm_line)\n",
    "        if(debug): print(\"finally cleaned setnence:\",final_lemm_line) \n",
    "    return final_sentences_list\n",
    "preprocess_text_line(test_line, lemmatizer,debug = True, deep_debug = False)#False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"can t\" in contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is my friend Jimmy. He is from India. Jimmy is a vegetarian. The best breakfast for him is a glass of orange juice, two apples and three bananas. Jimmy  likes watermelons very much. A watermelon is green outside and red inside. It is hard and soft inside. It is juicy and sweet. What a lovely fruit!\\nA month ago Sally received a letter. It was an invitation for a wedding. Her sister Victoria is getting married. Last year Sally saw such a ceremony. It took place on Saturday. There were a lot of guests, people wore nice clothes, met a husband and a wife near the church, threw confetti, ate a very tasty cake, made videos and took photos.\\n\\nIn her letter Victoria asked Sally to be her bridesmaid. It means that on the day of wedding Sally must help Victoria when she puts her wedding dress on and brushes her hair. Sally must also accompany Victoria on her way to the church and carry a traditional bunch of flowers.\\nSally agreed with pleasure and bought a beautiful dress for the ceremony. It was white and green with a long nice silk skirt. But at home she understood that she couldn’t fit into it. What a pity!\\nThat is why Sally is on a diet now. She does not eat cakes, sweets and fried potatoes. She doesn’t even eat some very sweet fruit. She cooks vegetables, fish and boiled meat. She writes down everything she eats into a special notebook and adds up the calories. What is more, every morning she practices jogging and goes to the gym. She lost three kilos, but she wants to lose two kilos more. Everyone tells her she looks great, but she doesn’t agree. She is dreaming about a slice of wedding cake and does not want to give up. Dieting is not easy!\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[\"Text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_basic jimmy\n",
      "non_basic india\n",
      "non_basic jimmy\n",
      "non_basic vegetarian\n",
      "non_basic jimmy\n",
      "non_basic watermelon\n",
      "non_basic watermelon\n",
      "non_basic soft\n",
      "non_basic juicy\n",
      "non_basic lovely\n",
      "non_basic ago\n",
      "non_basic sally\n",
      "non_basic receive\n",
      "non_basic invitation\n",
      "non_basic wedding\n",
      "non_basic victoria\n",
      "non_basic married\n",
      "non_basic sally\n",
      "non_basic saw\n",
      "non_basic such\n",
      "non_basic ceremony\n",
      "non_basic guest\n",
      "non_basic church\n",
      "non_basic throw\n",
      "non_basic confetti\n",
      "non_basic ate\n",
      "non_basic tasty\n",
      "non_basic video\n",
      "non_basic victoria\n",
      "non_basic sally\n",
      "non_basic bridesmaid\n",
      "non_basic mean\n",
      "non_basic wed\n",
      "non_basic sally\n",
      "non_basic must\n",
      "non_basic victoria\n",
      "non_basic wed\n",
      "non_basic brush\n",
      "non_basic sally\n",
      "non_basic must\n",
      "non_basic accompany\n",
      "non_basic victoria\n",
      "non_basic way\n",
      "non_basic church\n",
      "non_basic traditional\n",
      "non_basic bunch\n",
      "non_basic sally\n",
      "non_basic agree\n",
      "non_basic pleasure\n",
      "non_basic ceremony\n",
      "non_basic silk\n",
      "non_basic fit\n",
      "non_basic pity\n",
      "non_basic sally\n",
      "non_basic diet\n",
      "non_basic fry\n",
      "non_basic even\n",
      "non_basic boil\n",
      "non_basic everything\n",
      "non_basic special\n",
      "non_basic notebook\n",
      "non_basic add\n",
      "non_basic calorie\n",
      "non_basic practice\n",
      "non_basic jog\n",
      "non_basic gym\n",
      "non_basic lose\n",
      "non_basic kilo\n",
      "non_basic lose\n",
      "non_basic kilo\n",
      "non_basic everyone\n",
      "non_basic agree\n",
      "non_basic dream\n",
      "non_basic slice\n",
      "non_basic wed\n",
      "non_basic dieting\n",
      "238 76 0.32\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#for text in texts[\"Text\"]:\n",
    "processed_line = preprocess_text_line(texts[\"Text\"][0], lemmatizer, debug = False, deep_debug = False)\n",
    "\n",
    "basic_words = []\n",
    "out_of_bacis_words = []\n",
    "for sentence in processed_line:\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in final_basic_vocabulary:\n",
    "            #print(\"basic\", word)\n",
    "            basic_words.append(word)\n",
    "        else:\n",
    "            print(\"non_basic\",word)\n",
    "            out_of_bacis_words.append(word)\n",
    "print(len(basic_words), len(out_of_bacis_words), round(len(out_of_bacis_words)/ len(basic_words), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this be my friend jimmy',\n",
       " 'he be from india',\n",
       " 'jimmy be a vegetarian',\n",
       " 'the best breakfast for him be a glass of orange juice two apple and three banana',\n",
       " 'jimmy like watermelon very much',\n",
       " 'a watermelon be green outside and red inside',\n",
       " 'it be hard and soft inside',\n",
       " 'it be juicy and sweet',\n",
       " 'what a lovely fruit',\n",
       " 'a month ago sally receive a letter',\n",
       " 'it be an invitation for a wedding',\n",
       " 'her sister victoria be get married',\n",
       " 'last year sally saw such a ceremony',\n",
       " 'it take place on saturday',\n",
       " 'there be a lot of guest people wear nice clothes meet a husband and a wife near the church throw confetti ate a very tasty cake make video and take photo',\n",
       " 'in her letter victoria ask sally to be her bridesmaid',\n",
       " 'it mean that on the day of wed sally must help victoria when she put her wed dress on and brush her hair',\n",
       " 'sally must also accompany victoria on her way to the church and carry a traditional bunch of flower',\n",
       " 'sally agree with pleasure and buy a beautiful dress for the ceremony',\n",
       " 'it be white and green with a long nice silk skirt',\n",
       " 'but at home she understand that she could not fit into it',\n",
       " 'what a pity',\n",
       " 'that be why sally be on a diet now',\n",
       " 'she do not eat cake sweet and fry potato',\n",
       " 'she do not even eat some very sweet fruit',\n",
       " 'she cook vegetable fish and boil meat',\n",
       " 'she write down everything she eat into a special notebook and add up the calorie',\n",
       " 'what be more every morning she practice jog and go to the gym',\n",
       " 'she lose three kilo but she want to lose two kilo more',\n",
       " 'everyone tell her she look great but she do not agree',\n",
       " 'she be dream about a slice of wed cake and do not want to give up',\n",
       " 'dieting be not easy']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "processed_line = preprocess_text_line(texts[\"Text\"][0], lemmatizer, debug = False, deep_debug = False)\n",
    "processed_sentences_list = [' '.join(sentence) for sentence in processed_line]\n",
    "processed_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'accompany', 'add', 'ago', 'agree', 'also', 'an', 'and', 'apple', 'ask', 'at', 'ate', 'banana', 'be', 'beautiful', 'best', 'boil', 'breakfast', 'bridesmaid', 'brush', 'bunch', 'but', 'buy', 'cake', 'calorie', 'carry', 'ceremony', 'church', 'clothes', 'confetti', 'cook', 'could', 'day', 'diet', 'dieting', 'do', 'down', 'dream', 'dress', 'easy', 'eat', 'even', 'every', 'everyone', 'everything', 'fish', 'fit', 'flower', 'for', 'friend', 'from', 'fruit', 'fry', 'get', 'give', 'glass', 'go', 'great', 'green', 'guest', 'gym', 'hair', 'hard', 'he', 'help', 'her', 'him', 'home', 'husband', 'in', 'india', 'inside', 'into', 'invitation', 'it', 'jimmy', 'jog', 'juice', 'juicy', 'kilo', 'last', 'letter', 'like', 'long', 'look', 'lose', 'lot', 'lovely', 'make', 'married', 'mean', 'meat', 'meet', 'month', 'more', 'morning', 'much', 'must', 'my', 'near', 'nice', 'not', 'notebook', 'now', 'of', 'on', 'orange', 'outside', 'people', 'photo', 'pity', 'place', 'pleasure', 'potato', 'practice', 'put', 'receive', 'red', 'sally', 'saturday', 'saw', 'she', 'silk', 'sister', 'skirt', 'slice', 'soft', 'some', 'special', 'such', 'sweet', 'take', 'tasty', 'tell', 'that', 'the', 'there', 'this', 'three', 'throw', 'to', 'traditional', 'two', 'understand', 'up', 'vegetable', 'vegetarian', 'very', 'victoria', 'video', 'want', 'watermelon', 'way', 'wear', 'wed', 'wedding', 'what', 'when', 'white', 'why', 'wife', 'with', 'write', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 137, 'be': 13, 'my': 98, 'friend': 49, 'jimmy': 75, 'he': 63, 'from': 50, 'india': 70, 'vegetarian': 146, 'the': 135, 'best': 15, 'breakfast': 17, 'for': 48, 'him': 66, 'glass': 55, 'of': 104, 'orange': 106, 'juice': 77, 'two': 142, 'apple': 8, 'and': 7, 'three': 138, 'banana': 12, 'like': 82, 'watermelon': 151, 'very': 147, 'much': 96, 'green': 58, 'outside': 107, 'red': 117, 'inside': 71, 'it': 74, 'hard': 62, 'soft': 126, 'juicy': 78, 'sweet': 130, 'what': 156, 'lovely': 87, 'fruit': 51, 'month': 93, 'ago': 3, 'sally': 118, 'receive': 116, 'letter': 81, 'an': 6, 'invitation': 73, 'wedding': 155, 'her': 65, 'sister': 123, 'victoria': 148, 'get': 53, 'married': 89, 'last': 80, 'year': 163, 'saw': 120, 'such': 129, 'ceremony': 26, 'take': 131, 'place': 111, 'on': 105, 'saturday': 119, 'there': 136, 'lot': 86, 'guest': 59, 'people': 108, 'wear': 153, 'nice': 100, 'clothes': 28, 'meet': 92, 'husband': 68, 'wife': 160, 'near': 99, 'church': 27, 'throw': 139, 'confetti': 29, 'ate': 11, 'tasty': 132, 'cake': 23, 'make': 88, 'video': 149, 'photo': 109, 'in': 69, 'ask': 9, 'to': 140, 'bridesmaid': 18, 'mean': 90, 'that': 134, 'day': 32, 'wed': 154, 'must': 97, 'help': 64, 'when': 157, 'she': 121, 'put': 115, 'dress': 38, 'brush': 19, 'hair': 61, 'also': 5, 'accompany': 1, 'way': 152, 'carry': 25, 'traditional': 141, 'bunch': 20, 'flower': 47, 'agree': 4, 'with': 161, 'pleasure': 112, 'buy': 22, 'beautiful': 14, 'white': 158, 'long': 83, 'silk': 122, 'skirt': 124, 'but': 21, 'at': 10, 'home': 67, 'understand': 143, 'could': 31, 'not': 101, 'fit': 46, 'into': 72, 'pity': 110, 'why': 159, 'diet': 33, 'now': 103, 'do': 35, 'eat': 40, 'fry': 52, 'potato': 113, 'even': 41, 'some': 127, 'cook': 30, 'vegetable': 145, 'fish': 45, 'boil': 16, 'meat': 91, 'write': 162, 'down': 36, 'everything': 44, 'special': 128, 'notebook': 102, 'add': 2, 'up': 144, 'calorie': 24, 'more': 94, 'every': 42, 'morning': 95, 'practice': 114, 'jog': 76, 'go': 56, 'gym': 60, 'lose': 85, 'kilo': 79, 'want': 150, 'everyone': 43, 'tell': 133, 'look': 84, 'great': 57, 'dream': 37, 'about': 0, 'slice': 125, 'give': 54, 'dieting': 34, 'easy': 39}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_ )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XIA4sogzbIU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=0.85,stop_words=set(stopwords.words('english')))\n",
    "word_count_vector=cv.fit_transform(processed_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32x128 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 161 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend',\n",
       " 'jimmy',\n",
       " 'india',\n",
       " 'vegetarian',\n",
       " 'best',\n",
       " 'breakfast',\n",
       " 'glass',\n",
       " 'orange',\n",
       " 'juice',\n",
       " 'two']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector=tfidf_transformer.transform(cv.transform(processed_sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32x128 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 161 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63303939, 0.77411958, 1.        , 0.77411958, 0.63303939,\n",
       "       0.30470998, 0.30470998, 0.34107051, 0.34107051, 0.34107051,\n",
       "       0.34107051, 0.34107051, 0.34107051, 0.34107051, 0.47981473,\n",
       "       0.53707021, 0.53707021, 0.43919132, 0.4261771 , 0.47703209,\n",
       "       0.47703209, 0.4261771 , 0.4261771 , 0.59781173, 0.53408077,\n",
       "       0.59781173, 0.63303939, 0.77411958, 0.74573913, 0.66623806,\n",
       "       0.31002157, 0.48783266, 0.48783266, 0.43582625, 0.48783266,\n",
       "       0.70710678, 0.70710678, 0.40140093, 0.52879652, 0.52879652,\n",
       "       0.52879652, 0.48783266, 0.48783266, 0.31002157, 0.48783266,\n",
       "       0.43582625, 0.53408077, 0.59781173, 0.59781173, 0.22903528,\n",
       "       0.22903528, 0.22903528, 0.22903528, 0.22903528, 0.2046185 ,\n",
       "       0.22903528, 0.22903528, 0.2046185 , 0.22903528, 0.22903528,\n",
       "       0.22903528, 0.22903528, 0.22903528, 0.22903528, 0.22903528,\n",
       "       0.22903528, 0.2046185 , 0.18729452, 0.22903528, 0.39052209,\n",
       "       0.32694662, 0.45961938, 0.51446498, 0.51446498, 0.52083842,\n",
       "       0.22126886, 0.18524715, 0.2914946 , 0.26041921, 0.2914946 ,\n",
       "       0.2914946 , 0.2914946 , 0.26041921, 0.2914946 , 0.2914946 ,\n",
       "       0.32314638, 0.24529522, 0.32314638, 0.20536212, 0.28869669,\n",
       "       0.32314638, 0.28869669, 0.32314638, 0.32314638, 0.32314638,\n",
       "       0.32314638, 0.26391849, 0.41528743, 0.37101486, 0.37101486,\n",
       "       0.41528743, 0.41528743, 0.37101486, 0.42271673, 0.42271673,\n",
       "       0.42271673, 0.37765214, 0.42271673, 0.37765214, 0.5       ,\n",
       "       0.5       , 0.5       , 0.5       , 1.        , 0.5363614 ,\n",
       "       0.84398841, 0.40856228, 0.49961514, 0.49961514, 0.40856228,\n",
       "       0.40856228, 0.46180939, 0.50452488, 0.56472898, 0.46180939,\n",
       "       0.4472136 , 0.4472136 , 0.4472136 , 0.4472136 , 0.4472136 ,\n",
       "       0.38723866, 0.38723866, 0.38723866, 0.38723866, 0.31666596,\n",
       "       0.38723866, 0.38723866, 0.40824829, 0.40824829, 0.40824829,\n",
       "       0.40824829, 0.40824829, 0.40824829, 0.2771033 , 0.2771033 ,\n",
       "       0.2771033 , 0.62033914, 0.62033914, 0.4565234 , 0.4565234 ,\n",
       "       0.4565234 , 0.4565234 , 0.40785478, 0.38935195, 0.38935195,\n",
       "       0.43581266, 0.43581266, 0.43581266, 0.35638755, 0.70710678,\n",
       "       0.70710678])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector.tocoo().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items=sort_coo(tf_idf_vector.tocoo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(88, 1.0),\n",
       " (56, 1.0),\n",
       " (27, 0.8439884142349315),\n",
       " (115, 0.774119582428778),\n",
       " (62, 0.774119582428778),\n",
       " (40, 0.774119582428778),\n",
       " (71, 0.7457391260354046),\n",
       " (123, 0.7071067811865476),\n",
       " (58, 0.7071067811865476),\n",
       " (31, 0.7071067811865476),\n",
       " (28, 0.7071067811865476),\n",
       " (41, 0.6662380624821364),\n",
       " (105, 0.6330393922184421),\n",
       " (59, 0.6330393922184421),\n",
       " (59, 0.6330393922184421),\n",
       " (69, 0.6203391352949469),\n",
       " (63, 0.6203391352949469),\n",
       " (103, 0.5978117297300675),\n",
       " (97, 0.5978117297300675),\n",
       " (89, 0.5978117297300675),\n",
       " (52, 0.5978117297300675),\n",
       " (33, 0.564728981664346),\n",
       " (79, 0.5370702079080142),\n",
       " (66, 0.5370702079080142),\n",
       " (96, 0.5363614048728764),\n",
       " (106, 0.5340807725375345),\n",
       " (57, 0.5340807725375345),\n",
       " (100, 0.5287965244798071),\n",
       " (73, 0.5287965244798071),\n",
       " (43, 0.5287965244798071),\n",
       " (122, 0.5208384209907003),\n",
       " (13, 0.5144649816247601),\n",
       " (6, 0.5144649816247601),\n",
       " (41, 0.5045248793258318),\n",
       " (113, 0.5),\n",
       " (54, 0.5),\n",
       " (38, 0.5),\n",
       " (25, 0.5),\n",
       " (91, 0.49961514110308547),\n",
       " (42, 0.49961514110308547),\n",
       " (127, 0.4878326589920418),\n",
       " (98, 0.4878326589920418),\n",
       " (94, 0.4878326589920418),\n",
       " (77, 0.4878326589920418),\n",
       " (64, 0.4878326589920418),\n",
       " (2, 0.4878326589920418),\n",
       " (119, 0.47981472641215),\n",
       " (95, 0.4770320942477113),\n",
       " (85, 0.4770320942477113),\n",
       " (105, 0.4618093889309774),\n",
       " (32, 0.4618093889309774),\n",
       " (65, 0.4596193770800157),\n",
       " (108, 0.4565234048602731),\n",
       " (68, 0.4565234048602731),\n",
       " (47, 0.4565234048602731),\n",
       " (35, 0.4565234048602731),\n",
       " (114, 0.447213595499958),\n",
       " (75, 0.447213595499958),\n",
       " (37, 0.447213595499958),\n",
       " (24, 0.447213595499958),\n",
       " (11, 0.447213595499958),\n",
       " (59, 0.43919131579906995),\n",
       " (65, 0.43582624834268974),\n",
       " (20, 0.43582624834268974),\n",
       " (102, 0.4358126621536298),\n",
       " (44, 0.4358126621536298),\n",
       " (29, 0.4358126621536298),\n",
       " (119, 0.4261771001650547),\n",
       " (57, 0.4261771001650547),\n",
       " (48, 0.4261771001650547),\n",
       " (124, 0.4227167279649903),\n",
       " (101, 0.4227167279649903),\n",
       " (99, 0.4227167279649903),\n",
       " (67, 0.4227167279649903),\n",
       " (90, 0.4152874333564036),\n",
       " (16, 0.4152874333564036),\n",
       " (9, 0.4152874333564036),\n",
       " (105, 0.40856228475027245),\n",
       " (32, 0.40856228475027245),\n",
       " (17, 0.40856228475027245),\n",
       " (92, 0.408248290463863),\n",
       " (78, 0.408248290463863),\n",
       " (60, 0.408248290463863),\n",
       " (50, 0.408248290463863),\n",
       " (46, 0.408248290463863),\n",
       " (34, 0.408248290463863),\n",
       " (3, 0.4078547820721643),\n",
       " (116, 0.4014009306095719),\n",
       " (116, 0.3905220870983632),\n",
       " (122, 0.3893519509725058),\n",
       " (118, 0.3893519509725058),\n",
       " (126, 0.3872386574237743),\n",
       " (104, 0.3872386574237743),\n",
       " (83, 0.3872386574237743),\n",
       " (36, 0.3872386574237743),\n",
       " (18, 0.3872386574237743),\n",
       " (1, 0.3872386574237743),\n",
       " (82, 0.37765213596264063),\n",
       " (48, 0.37765213596264063),\n",
       " (30, 0.371014856687852),\n",
       " (20, 0.371014856687852),\n",
       " (3, 0.371014856687852),\n",
       " (17, 0.3563875517852795),\n",
       " (84, 0.3410705106931752),\n",
       " (61, 0.3410705106931752),\n",
       " (45, 0.3410705106931752),\n",
       " (12, 0.3410705106931752),\n",
       " (10, 0.3410705106931752),\n",
       " (8, 0.3410705106931752),\n",
       " (5, 0.3410705106931752),\n",
       " (96, 0.32694662112428574),\n",
       " (120, 0.3231463831075568),\n",
       " (111, 0.3231463831075568),\n",
       " (39, 0.3231463831075568),\n",
       " (19, 0.3231463831075568),\n",
       " (15, 0.3231463831075568),\n",
       " (4, 0.3231463831075568),\n",
       " (0, 0.3231463831075568),\n",
       " (32, 0.31666596466907637),\n",
       " (96, 0.3100215665365859),\n",
       " (96, 0.3100215665365859),\n",
       " (112, 0.3047099827282304),\n",
       " (109, 0.3047099827282304),\n",
       " (93, 0.2914945953614586),\n",
       " (74, 0.2914945953614586),\n",
       " (53, 0.2914945953614586),\n",
       " (51, 0.2914945953614586),\n",
       " (26, 0.2914945953614586),\n",
       " (14, 0.2914945953614586),\n",
       " (80, 0.28869669387504765),\n",
       " (21, 0.28869669387504765),\n",
       " (118, 0.27710329869505046),\n",
       " (112, 0.27710329869505046),\n",
       " (109, 0.27710329869505046),\n",
       " (96, 0.26391849393217964),\n",
       " (80, 0.26041921049535016),\n",
       " (30, 0.26041921049535016),\n",
       " (116, 0.2452952182885305),\n",
       " (125, 0.22903528231852394),\n",
       " (121, 0.22903528231852394),\n",
       " (117, 0.22903528231852394),\n",
       " (110, 0.22903528231852394),\n",
       " (107, 0.22903528231852394),\n",
       " (87, 0.22903528231852394),\n",
       " (86, 0.22903528231852394),\n",
       " (81, 0.22903528231852394),\n",
       " (76, 0.22903528231852394),\n",
       " (72, 0.22903528231852394),\n",
       " (70, 0.22903528231852394),\n",
       " (55, 0.22903528231852394),\n",
       " (49, 0.22903528231852394),\n",
       " (23, 0.22903528231852394),\n",
       " (22, 0.22903528231852394),\n",
       " (7, 0.22903528231852394),\n",
       " (116, 0.22126885565454987),\n",
       " (96, 0.2053621176545107),\n",
       " (106, 0.2046185018388026),\n",
       " (82, 0.2046185018388026),\n",
       " (21, 0.2046185018388026),\n",
       " (17, 0.18729452038999048),\n",
       " (96, 0.18524715273804956)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=extract_topn_from_vector(feature_names,sorted_items,len(sorted_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pity 1.0\n",
      "india 1.0\n",
      "diet 0.844\n",
      "vegetarian 0.774\n",
      "juicy 0.774\n",
      "friend 0.774\n",
      "lovely 0.746\n",
      "wedding 0.707\n",
      "invitation 0.707\n",
      "easy 0.707\n",
      "dieting 0.707\n",
      "fruit 0.505\n",
      "sweet 0.409\n",
      "jimmy 0.439\n",
      "lose 0.62\n",
      "kilo 0.62\n",
      "soft 0.598\n",
      "saturday 0.598\n",
      "place 0.598\n",
      "hard 0.598\n",
      "even 0.565\n",
      "much 0.537\n",
      "like 0.537\n",
      "sally 0.185\n",
      "take 0.205\n",
      "inside 0.426\n",
      "sister 0.529\n",
      "married 0.529\n",
      "get 0.529\n",
      "wed 0.389\n",
      "bridesmaid 0.514\n",
      "ask 0.514\n",
      "understand 0.5\n",
      "home 0.5\n",
      "fit 0.5\n",
      "could 0.5\n",
      "potato 0.5\n",
      "fry 0.5\n",
      "year 0.488\n",
      "saw 0.488\n",
      "receive 0.488\n",
      "month 0.488\n",
      "last 0.488\n",
      "ago 0.488\n",
      "watermelon 0.426\n",
      "red 0.477\n",
      "outside 0.477\n",
      "eat 0.317\n",
      "letter 0.436\n",
      "tell 0.457\n",
      "look 0.457\n",
      "great 0.457\n",
      "everyone 0.457\n",
      "vegetable 0.447\n",
      "meat 0.447\n",
      "fish 0.447\n",
      "cook 0.447\n",
      "boil 0.447\n",
      "ceremony 0.371\n",
      "slice 0.436\n",
      "give 0.436\n",
      "dream 0.436\n",
      "green 0.378\n",
      "white 0.423\n",
      "skirt 0.423\n",
      "silk 0.423\n",
      "long 0.423\n",
      "pleasure 0.415\n",
      "buy 0.415\n",
      "beautiful 0.415\n",
      "cake 0.187\n",
      "practice 0.408\n",
      "morning 0.408\n",
      "jog 0.408\n",
      "gym 0.408\n",
      "go 0.408\n",
      "every 0.408\n",
      "agree 0.371\n",
      "victoria 0.221\n",
      "want 0.277\n",
      "write 0.387\n",
      "special 0.387\n",
      "notebook 0.387\n",
      "everything 0.387\n",
      "calorie 0.387\n",
      "add 0.387\n",
      "nice 0.205\n",
      "dress 0.26\n",
      "orange 0.341\n",
      "juice 0.341\n",
      "glass 0.341\n",
      "breakfast 0.341\n",
      "best 0.341\n",
      "banana 0.341\n",
      "apple 0.341\n",
      "way 0.323\n",
      "traditional 0.323\n",
      "flower 0.323\n",
      "carry 0.323\n",
      "bunch 0.323\n",
      "also 0.323\n",
      "accompany 0.323\n",
      "two 0.277\n",
      "three 0.277\n",
      "put 0.291\n",
      "mean 0.291\n",
      "help 0.291\n",
      "hair 0.291\n",
      "day 0.291\n",
      "brush 0.291\n",
      "must 0.26\n",
      "church 0.205\n",
      "wife 0.229\n",
      "wear 0.229\n",
      "video 0.229\n",
      "throw 0.229\n",
      "tasty 0.229\n",
      "photo 0.229\n",
      "people 0.229\n",
      "near 0.229\n",
      "meet 0.229\n",
      "make 0.229\n",
      "lot 0.229\n",
      "husband 0.229\n",
      "guest 0.229\n",
      "confetti 0.229\n",
      "clothes 0.229\n",
      "ate 0.229\n"
     ]
    }
   ],
   "source": [
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
