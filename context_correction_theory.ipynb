{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРОСМОТРЕТЬ РАЗБОР БОЛЬШЕГО КОЛИЧЕСТВА ТЕКСТОВ\n",
    "ПОСМОТРЕТЬ ГДЕ КОНТЕКСТ РЕАЛЬНО ВЫРУЧАЕТ А ГДЕ НЕТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "не общее количество слов в подчинении а количество слов в финальном подчинении (таких у которх нет относящихся к ним слов) -- не норм\n",
    "слова в предпоследнем уровне обесценятся\n",
    "\n",
    "ВАРИАНТ 1)\n",
    "Лучше просто исключить из списка заивсимых все малозначимые слова типа Closed class words + Other из (кроме ADP) http://universaldependencies.org/u/pos/index.html\n",
    "\n",
    "ВАРИАНТ 2\n",
    "ДУМАТЬ О ЗАДАЧЕ ПРЕДСКАЗАНИЯ ВЕРОЯТНОСТИ ДОГАДКИ О СМЫСЛЕ СЛОВА ИЗ ЕГО КОНТЕКСТА!!\n",
    "ДЕЛАТЬ НАОБОРОТ\n",
    "ЧЕМ БОЛШЕ У СЛОВА ПРЯМО СВЯЗАННЫХ СЛОВ ТЕМ БОЛЬШЕ ВЕРОЯТНОСТЬ ЧТО О ЕГО ЗНАЧЕНИИ МОЖНО ДОГАДАТЬСЯ ТЕМ МЕНЬШЕ ВЕРОЯТНОСТЬ ТОГО ЧТО ИМЕННО ЭТО СЛОВО УБЬЕТ ВСЕ ПОНИМАНИЕ ПРЕДЛОЖЕНИЯ\n",
    "\n",
    "В то же время из контекста надо исключить тот же список \n",
    "\n",
    "Итого штрафуем важность слова в совокупном понимании предложения на величину 1/(кол-во прямо зависимых слов)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(conllu_map, tf_idf_dict, apply_tf_idf = True):\n",
    "    text_map = []\n",
    "    sentence_ind = 0\n",
    "    for sentence in conllu_map:\n",
    "        sentence_map = []\n",
    "        real_index = 1\n",
    "        for word in sentence: \n",
    "            weight = OrderedDict([(\"word\", word[1]),(\"lemma\",word[2]), (\"vocabulary_prop\",(OrderedDict([(\"tf_idf\", 0),(\"nominal_index\",word[0]),(\"real_index\",real_index)])))])\n",
    "            real_index += 1\n",
    "            #print(word[2])\n",
    "            #weight = OrderedDict([(\"word\", word[1]),(\"lemma\",word[2]), (\"vocabulary_prop\",(OrderedDict([(\"tf_idf\", 0),(\"nominal_index\",word[0])])))])\n",
    "            lemma_lower = word[2].lower()\n",
    "            if (apply_tf_idf):\n",
    "                if (lemma_lower in tf_idf_dict):\n",
    "                    tf_idf_i = tf_idf_dict[lemma_lower][sentence_ind]\n",
    "                    if(word[3] not in pos_exclude_list):\n",
    "                        weight[\"vocabulary_prop\"][\"vocab_imp\"] = tf_idf_i\n",
    "                    elif(tf_idf_i > 0 ):\n",
    "                        #print(word)\n",
    "                        weight[\"vocabulary_prop\"][\"vocab_imp\"] = tf_idf_i * 0.5\n",
    "            sentence_map.append(weight)\n",
    "        text_map.append(sentence_map)\n",
    "        sentence_ind += 1\n",
    "    return text_map\n",
    "text_map_ex = create_map(conllu_text_map_ex, tf_idf_dict_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "учитывает tf idf и расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependencies (conllu_map, text_map_input):\n",
    "    assert len(conllu_map) == len(text_map_input) #sentences count is equal\n",
    "    text_map = copy.deepcopy(text_map_input)\n",
    "    for sentence_conllu, text_map_sentence in zip(conllu_map,text_map):\n",
    "        dep_dict = {}#включаем зависимости неисключенных слов от всех слов(если исключенные будут с контекстом то их просто оштрафует и нам не страшно)\n",
    "        #print(sentence_conllu)\n",
    "        for word_conllu,word_map in zip(sentence_conllu,text_map_sentence): \n",
    "            #print(word_conllu)\n",
    "            #print(word_map,'\\n')\n",
    "            #print(word[1], \"head_word_nominal_index =\", word[6])\n",
    "            if (word_conllu[3] not in pos_exclude_list):#make sure that dependent list will not count exclude pos\n",
    "                if(word_conllu[6] in dep_dict):\n",
    "                    #print(word_conllu)\n",
    "                    dep_dict[word_conllu[6]].append((word_map['vocabulary_prop']['tf_idf'],word_map['vocabulary_prop']['real_index']))\n",
    "                elif(int(word_conllu[6]) != 0):\n",
    "                    dep_dict[word_conllu[6]] = []\n",
    "                    dep_dict[word_conllu[6]].append((word_map['vocabulary_prop']['tf_idf'],word_map['vocabulary_prop']['real_index']))\n",
    "        #print(dep_dict,'\\n')\n",
    "        print(\"\\n====\\n\",text_map_sentence,\"====\\n\")\n",
    "        for map_word in text_map_sentence:\n",
    "            word_nominal_index = map_word[\"vocabulary_prop\"][\"nominal_index\"] \n",
    "            if(word_nominal_index in dep_dict):#calculate context fine\n",
    "                print(map_word)\n",
    "                tf_idf_per_dist_sum = 0 \n",
    "                word_real_index = map_word[\"vocabulary_prop\"][\"real_index\"]\n",
    "                print(dep_dict[word_nominal_index])\n",
    "                for dependent_element in dep_dict[word_nominal_index]:\n",
    "                    distance = abs(word_real_index - dependent_element[1])\n",
    "                    print(\"distance\", distance)\n",
    "                    dep_element_adjusted_tfidf = dependent_element[0] + 1\n",
    "                    print(\"dep_element_adjusted_tfidf\",dep_element_adjusted_tfidf)\n",
    "                    tf_idf_per_dist_element = dep_element_adjusted_tfidf/distance\n",
    "                    print(\"tf_idf_per_dist_element\",tf_idf_per_dist_element)\n",
    "                    tf_idf_per_dist_sum += tf_idf_per_dist_element\n",
    "                    print(\"tf_idf_per_dist_sum\",tf_idf_per_dist_sum)\n",
    "                context_fine = 1 / tf_idf_per_dist_sum\n",
    "                print(\"fine\", context_fine)\n",
    "                print(\"ADJUSTED tfidf for word \",map_word['word'], map_word[\"vocabulary_prop\"][\"tf_idf\"] * context_fine)\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"tf_idf\"] * context_fine\n",
    "            else:\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "        print()\n",
    "        \"\"\"\n",
    "        for map_word in text_map_sentence:\n",
    "            if(map_word[\"vocabulary_prop\"][\"nominal_index\"] in dep_dict):\n",
    "                map_word[\"vocabulary_prop\"][\"dep_words_count\"] = dep_dict[map_word[\"vocabulary_prop\"][\"nominal_index\"]] + 1\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"dep_words_count\"] * map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "            else:\n",
    "                map_word[\"vocabulary_prop\"][\"dep_words_count\"] = 1\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"dep_words_count\"] * map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "            \"\"\"\n",
    "    \n",
    "    return text_map\n",
    "text_map_dep = get_dependencies(conllu_text_map_ex, text_map_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "С ЛОГРАИФМАМИ БЕЗ ТФ ИДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_distances = []\n",
    "min_current_distance = 1\n",
    "min_dist_dict = {}\n",
    "for i in range (1,15):\n",
    "    #print(i)\n",
    "    minimal_distances.append(min_current_distance)\n",
    "    min_dist_dict [i] = copy.deepcopy(minimal_distances)\n",
    "    if(i%2 == 0):\n",
    "        min_current_distance += 1\n",
    "        #print(\"herak\",min_current_distance)\n",
    "    #print(min_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_min_dist = {}\n",
    "for key,values in  min_dist_dict.items():\n",
    "    #print(key, values, mean(values))\n",
    "    mean_min_dist[key] = mean(values)\n",
    "mean_min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_exclude_list = [\"AUX\",\"SYM\",\"CCONJ\",\"X\",\"DET\",\"NUM\",\"PART\",\"SCON\",\"INTJ\",\"PROPN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependencies (conllu_map, text_map_input, debug = False):\n",
    "    assert len(conllu_map) == len(text_map_input) #sentences count is equal\n",
    "    text_map = copy.deepcopy(text_map_input)\n",
    "    for sentence_conllu, text_map_sentence in zip(conllu_map,text_map):\n",
    "        nominal_index_correction_dict = {}\n",
    "        dep_dict = {}#включаем зависимости неисключенных слов от всех слов(если исключенные будут с контекстом то их просто оштрафует и нам не страшно)\n",
    "        #print(sentence_conllu)\n",
    "        nom2real_ind = {}\n",
    "        for word_conllu,word_map in zip(sentence_conllu,text_map_sentence): \n",
    "            nom2real_ind[word_conllu[0]] = word_map['vocabulary_prop']['real_index']\n",
    "            #print(word_conllu)\n",
    "            #print(word_map,'\\n')\n",
    "            #print(word[1], \"head_word_nominal_index =\", word[6])\n",
    "            if (word_conllu[3] not in pos_exclude_list):#make sure that dependent list will not count exclude pos\n",
    "                if(word_conllu[6] in dep_dict):\n",
    "                    dep_dict[word_conllu[6]].append(word_map['vocabulary_prop']['real_index'])\n",
    "                elif(int(word_conllu[6]) != 0):\n",
    "                    dep_dict[word_conllu[6]] = []\n",
    "                    dep_dict[word_conllu[6]].append(word_map['vocabulary_prop']['real_index'])\n",
    "            else:\n",
    "                nominal_index_correction_dict[word_conllu[0]] = (0.5,\"excluded_pos\")\n",
    "        if (debug == True):\n",
    "            print(\"sentence_conllu\", sentence_conllu)            \n",
    "            print(\"dep_dict\", dep_dict)\n",
    "            print(\"nom2real_ind\",nom2real_ind)\n",
    "        real_mean_dist = {}\n",
    "        \n",
    "        for head_element_nom_index, dep_elt_real_indexes in  dep_dict.items():\n",
    "            distances = []\n",
    "            \n",
    "            for dep_el_r_i in dep_elt_real_indexes:\n",
    "                if (debug == True):\n",
    "                    print(\"dep_el_r_i\",dep_el_r_i)\n",
    "                    print(\"distance\", abs(dep_el_r_i - nom2real_ind[head_element_nom_index]))\n",
    "                distances.append(abs(dep_el_r_i - nom2real_ind[head_element_nom_index]))\n",
    "\n",
    "            real_mean_distance = mean(distances)\n",
    "            real_mean_dist[head_element_nom_index] = real_mean_distance\n",
    "            real_context_element_count = len(dep_elt_real_indexes)\n",
    "            minimal_mean_distance = mean_min_dist[real_context_element_count]\n",
    "            \n",
    "            if real_context_element_count == 1: real_context_element_count += 1\n",
    "            real_context_mean_distance = real_mean_dist[head_element_nom_index]\n",
    "            \n",
    "            correction_coeff = (1+ np.log10(real_context_mean_distance/minimal_mean_distance))/(1+np.log(real_context_element_count))\n",
    "            if (debug == True):\n",
    "                print(\"mean_min_dist with\",real_context_element_count, \"elements\", minimal_mean_distance)\n",
    "                print(\"real_mean_dist\",real_context_mean_distance)\n",
    "                print(\"correction_coeff\",correction_coeff,\"vs\", 1/real_context_element_count,\"without log\")\n",
    "            nominal_index_correction_dict[head_element_nom_index] = [correction_coeff,dep_elt_real_indexes]\n",
    "            \n",
    "        #ШТРАФОВАТЬ ВАЖНОСТЬ ИСКЛЮЧАЕМЫХ СЛОВ \n",
    "        \n",
    "        \n",
    "        #print(\"\\n====\\n\",text_map_sentence,\"====\\n\")\n",
    "        for map_word in text_map_sentence:\n",
    "            word_nominal_index = map_word[\"vocabulary_prop\"][\"nominal_index\"] \n",
    "            if(word_nominal_index in nominal_index_correction_dict):#calculate context fine\n",
    "                print(nominal_index_correction_dict[word_nominal_index])\n",
    "                map_word[\"vocabulary_prop\"][\"dep_element_real_ind\"] = nominal_index_correction_dict[word_nominal_index][1]\n",
    "                map_word[\"vocabulary_prop\"][\"context_correction\"] = nominal_index_correction_dict[word_nominal_index][0]\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importance\"] = nominal_index_correction_dict[word_nominal_index][0] * map_word[\"vocabulary_prop\"]['tf_idf'] \n",
    "            else:\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importance\"] = map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "    return text_map\n",
    "                \n",
    "get_dependencies(conllu_text_map_ex, text_map_ex, debug = True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_analysis(conllu_map,text_map_input):\n",
    "    assert len(conllu_map) == len(text_map_input) #sentences count is equal\n",
    "    text_map = copy.deepcopy(text_map_input)\n",
    "    \n",
    "    for sentence_conllu, text_map_sentence in zip(conllu_map,text_map):\n",
    "        #СОБИРАЕМ СПИСОК СВАОЙСТВ ВСЕХ СЛОВ ПРЕДЛОЖЕНИЯ ДЛЯ ПОСЛЕДУЮЩЕГО ОБРАЩЕНИЯ\n",
    "        pos_word_dict = {}\n",
    "        for pos_word in sentence_conllu:\n",
    "            pos_word_dict[pos_word[0]] = (pos_word[0]+'_'+pos_word[1], pos_word[2:])\n",
    "        \n",
    "        print(\"POS\", pos_word_dict)# словарь \"номинальный индекс слова\" (номинальный-индекс_слово,  остальные conllu based свойства)\n",
    "        noun_phrase_sentence = False\n",
    "        \n",
    "        #СТРОИМ ПОДДЕРЕВЬЯ ГЛАГОЛЬНЫХ ГРУПП\n",
    "        verb_phrases_dict = {}#словарь \"индекс_слово(глагол в верштне)\" [список conllu based свойств зависимых эл-тов]\n",
    "        for word_leave in sentence_conllu: \n",
    "            print(word_leave[1], \"head_word_nominal_index =\", word_leave[6])\n",
    "            \n",
    "            if (word_leave[7] == \"root\" and  word_leave[3] != 'VERB'):\n",
    "                print(\"NOUN PHRASE BASED SENTENCE\")\n",
    "                noun_phrase_sentence = True\n",
    "                break\n",
    "\n",
    "            head_word_nominal_index = word_leave[6]#смотрим на номинальный индекс главного элемента \n",
    "            if (int(head_word_nominal_index)!= 0):\n",
    "                current_head_word = pos_word_dict[head_word_nominal_index][0]\n",
    "                current_head_pos = pos_word_dict[head_word_nominal_index][1][1]\n",
    "                #print(\"current_head_word\",pos_word_dict[head_word_nominal_index])\n",
    "                if(current_head_word in verb_phrases_dict):\n",
    "                    verb_phrases_dict[current_head_word].append(word_leave)\n",
    "                elif(current_head_pos == \"VERB\"):\n",
    "                    verb_phrases_dict[current_head_word] = []\n",
    "                    verb_phrases_dict[current_head_word].append(word_leave)\n",
    "        \n",
    "        if(noun_phrase_sentence):\n",
    "            print(\"SKIPPING THIS SENTENCE\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"VERB SUBTREES\")\n",
    "            for key, value in verb_phrases_dict.items():\n",
    "                print(key)\n",
    "                for el in value:\n",
    "                    print(el)\n",
    "                    \n",
    "        # АНАЛИЗ ПОДДЕРЕВЬЕВ, ПРИСВОЕНИЕ ВРЕМЕНИ, ЗАПИСЬ В ЛОГ\n",
    "        grammar_properties_log = {}\n",
    "        undefined_tense_stack = []\n",
    "        #head_of_subtree_index is from pos_word_dict\n",
    "        for head_of_subtree_plus_index, dependent_elements in verb_phrases_dict.items():\n",
    "            #исследуем корень поддерева\n",
    "            head_of_subtree_index = head_of_subtree_plus_index.split(\"_\")[0]\n",
    "            print(\"NOW HANDLING HEADWORD\",head_of_subtree_plus_index)\n",
    "            head_properties = pos_word_dict[head_of_subtree_index]\n",
    "            print(head_properties)\n",
    "            if (head_properties[0].endswith(\"ing\")): continious = True\n",
    "            if (\"Tense\" in head_properties[1][3]):#случай когда глагол уже маркирован временем\n",
    "                # в такой ситуации можно переходить на следующий ключ и сохранять всю херню в логи\n",
    "                grammar = head_properties[1][3].split(\"|\")\n",
    "                for gr_unit in grammar:\n",
    "                    if (\"Tense\" in gr_unit):\n",
    "                        print(\"TENSE DETECTED\", head_properties)\n",
    "                        tense = gr_unit.split(\"=\")[1]\n",
    "                        if(tense == \"Pres\" and not head_properties[0].endswith(\"ing\")):\n",
    "                            print(\"not ing\",head_properties[0])\n",
    "                            grammar_properties_log [head_of_subtree_index] = \"PrsSmpl\"\n",
    "                            \n",
    "                        elif(tense == \"Pres\" and head_properties[0].endswith(\"ing\")):\n",
    "                            print(\"INGGGGG\")\n",
    "                            for dep_el in dependent_elements:\n",
    "                                if (dep_el[1].lower() == \"am\" or dep_el[1].lower() == \"is\" or dep_el[1].lower() == \"are\"):\n",
    "                                    grammar_properties_log[head_of_subtree_index] = \"PrsCont\"\n",
    "                                elif(dep_el[1].lower() == \"was\" or dep_el[1].lower() == \"were\"):\n",
    "                                    grammar_properties_log[head_of_subtree_index] = \"PastCont\"\n",
    "                                elif(dep_el[2] == \"will\" or dep_el[2] == \"shall\"): \n",
    "                                    grammar_properties_log [head_of_subtree_index] = \"FutCont\"\n",
    "                            \n",
    "                                \n",
    "                        elif(tense == \"Past\"):\n",
    "                            perfect = False\n",
    "                            some_future = False\n",
    "                            for dep_el in dependent_elements:\n",
    "                                if (dep_el[1].lower() == \"have\" or dep_el[1].lower() == \"has\"):\n",
    "                                    grammar_properties_log [head_of_subtree_index] = \"PrPerf\"\n",
    "                                    perfect = True\n",
    "                                elif(dep_el[1].lower() == \"had\" ):\n",
    "                                    grammar_properties_log [head_of_subtree_index] = \"PastPerf\"\n",
    "                                    perfect = True\n",
    "                                elif(dep_el[2] == \"will\" or dep_el[2] == \"shall\"): \n",
    "                                    some_future = True\n",
    "                            if some_future and perfect:\n",
    "                                grammar_properties_log [head_of_subtree_index] = \"FutPerf\"\n",
    "                                    \n",
    "                            if not perfect:\n",
    "                                grammar_properties_log [head_of_subtree_index] = \"PastSmpl\"\n",
    "            \n",
    "            \n",
    "            else:#ищем маркер времени в зависимых\n",
    "                #undefined_tense_stack.append(head_of_subtree_index)\n",
    "                tense = None\n",
    "                for dep_el in dependent_elements:\n",
    "                    if (dep_el[3] == \"AUX\"):\n",
    "                        for gr_unit in dep_el[5].split(\"|\"):\n",
    "                            if \"Tense\" in gr_unit:\n",
    "                                tense = gr_unit.split(\"=\")[1]\n",
    "                                if(tense == \"Pres\"):\n",
    "                                    grammar_properties_log [head_of_subtree_index] = \"PrsSmpl\"\n",
    "                                elif(tense == \"Past\"):\n",
    "                                    grammar_properties_log [head_of_subtree_index] = \"PastSmpl\"\n",
    "                        if(dep_el[2] == \"will\" or dep_el[2] == \"shall\"):\n",
    "                            grammar_properties_log [head_of_subtree_index] = \"SOME_FUTURE\"\n",
    "                                    \n",
    "                                    \n",
    "        print(\"grammar_properties_log\", grammar_properties_log)        \n",
    "        for map_word in text_map_sentence:\n",
    "            word_index = map_word['vocabulary_prop']['nominal_index']\n",
    "            if (str(word_index) in grammar_properties_log):\n",
    "                map_word['grammar_prop'] = grammar_properties_log [word_index]\n",
    "\n",
    "    return text_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
