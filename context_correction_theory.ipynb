{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРОСМОТРЕТЬ РАЗБОР БОЛЬШЕГО КОЛИЧЕСТВА ТЕКСТОВ\n",
    "ПОСМОТРЕТЬ ГДЕ КОНТЕКСТ РЕАЛЬНО ВЫРУЧАЕТ А ГДЕ НЕТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "учитывает tf idf и расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependencies (conllu_map, text_map_input):\n",
    "    assert len(conllu_map) == len(text_map_input) #sentences count is equal\n",
    "    text_map = copy.deepcopy(text_map_input)\n",
    "    for sentence_conllu, text_map_sentence in zip(conllu_map,text_map):\n",
    "        dep_dict = {}#включаем зависимости неисключенных слов от всех слов(если исключенные будут с контекстом то их просто оштрафует и нам не страшно)\n",
    "        #print(sentence_conllu)\n",
    "        for word_conllu,word_map in zip(sentence_conllu,text_map_sentence): \n",
    "            #print(word_conllu)\n",
    "            #print(word_map,'\\n')\n",
    "            #print(word[1], \"head_word_nominal_index =\", word[6])\n",
    "            if (word_conllu[3] not in pos_exclude_list):#make sure that dependent list will not count exclude pos\n",
    "                if(word_conllu[6] in dep_dict):\n",
    "                    #print(word_conllu)\n",
    "                    dep_dict[word_conllu[6]].append((word_map['vocabulary_prop']['tf_idf'],word_map['vocabulary_prop']['real_index']))\n",
    "                elif(int(word_conllu[6]) != 0):\n",
    "                    dep_dict[word_conllu[6]] = []\n",
    "                    dep_dict[word_conllu[6]].append((word_map['vocabulary_prop']['tf_idf'],word_map['vocabulary_prop']['real_index']))\n",
    "        #print(dep_dict,'\\n')\n",
    "        print(\"\\n====\\n\",text_map_sentence,\"====\\n\")\n",
    "        for map_word in text_map_sentence:\n",
    "            word_nominal_index = map_word[\"vocabulary_prop\"][\"nominal_index\"] \n",
    "            if(word_nominal_index in dep_dict):#calculate context fine\n",
    "                print(map_word)\n",
    "                tf_idf_per_dist_sum = 0 \n",
    "                word_real_index = map_word[\"vocabulary_prop\"][\"real_index\"]\n",
    "                print(dep_dict[word_nominal_index])\n",
    "                for dependent_element in dep_dict[word_nominal_index]:\n",
    "                    distance = abs(word_real_index - dependent_element[1])\n",
    "                    print(\"distance\", distance)\n",
    "                    dep_element_adjusted_tfidf = dependent_element[0] + 1\n",
    "                    print(\"dep_element_adjusted_tfidf\",dep_element_adjusted_tfidf)\n",
    "                    tf_idf_per_dist_element = dep_element_adjusted_tfidf/distance\n",
    "                    print(\"tf_idf_per_dist_element\",tf_idf_per_dist_element)\n",
    "                    tf_idf_per_dist_sum += tf_idf_per_dist_element\n",
    "                    print(\"tf_idf_per_dist_sum\",tf_idf_per_dist_sum)\n",
    "                context_fine = 1 / tf_idf_per_dist_sum\n",
    "                print(\"fine\", context_fine)\n",
    "                print(\"ADJUSTED tfidf for word \",map_word['word'], map_word[\"vocabulary_prop\"][\"tf_idf\"] * context_fine)\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"tf_idf\"] * context_fine\n",
    "            else:\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "        print()\n",
    "        \"\"\"\n",
    "        for map_word in text_map_sentence:\n",
    "            if(map_word[\"vocabulary_prop\"][\"nominal_index\"] in dep_dict):\n",
    "                map_word[\"vocabulary_prop\"][\"dep_words_count\"] = dep_dict[map_word[\"vocabulary_prop\"][\"nominal_index\"]] + 1\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"dep_words_count\"] * map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "            else:\n",
    "                map_word[\"vocabulary_prop\"][\"dep_words_count\"] = 1\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importane\"] = map_word[\"vocabulary_prop\"][\"dep_words_count\"] * map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "            \"\"\"\n",
    "    \n",
    "    return text_map\n",
    "text_map_dep = get_dependencies(conllu_text_map_ex, text_map_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "С ЛОГРАИФМАМИ БЕЗ ТФ ИДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_distances = []\n",
    "min_current_distance = 1\n",
    "min_dist_dict = {}\n",
    "for i in range (1,15):\n",
    "    #print(i)\n",
    "    minimal_distances.append(min_current_distance)\n",
    "    min_dist_dict [i] = copy.deepcopy(minimal_distances)\n",
    "    if(i%2 == 0):\n",
    "        min_current_distance += 1\n",
    "        #print(\"herak\",min_current_distance)\n",
    "    #print(min_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_min_dist = {}\n",
    "for key,values in  min_dist_dict.items():\n",
    "    #print(key, values, mean(values))\n",
    "    mean_min_dist[key] = mean(values)\n",
    "mean_min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_exclude_list = [\"AUX\",\"SYM\",\"CCONJ\",\"X\",\"DET\",\"NUM\",\"PART\",\"SCON\",\"INTJ\",\"PROPN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "не общее количество слов в подчинении а количество слов в финальном подчинении (таких у которх нет относящихся к ним слов) -- не норм\n",
    "слова в предпоследнем уровне обесценятся\n",
    "\n",
    "ВАРИАНТ 1)\n",
    "Лучше просто исключить из списка заивсимых все малозначимые слова типа Closed class words + Other из (кроме ADP) http://universaldependencies.org/u/pos/index.html\n",
    "\n",
    "ВАРИАНТ 2\n",
    "ДУМАТЬ О ЗАДАЧЕ ПРЕДСКАЗАНИЯ ВЕРОЯТНОСТИ ДОГАДКИ О СМЫСЛЕ СЛОВА ИЗ ЕГО КОНТЕКСТА!!\n",
    "ДЕЛАТЬ НАОБОРОТ\n",
    "ЧЕМ БОЛШЕ У СЛОВА ПРЯМО СВЯЗАННЫХ СЛОВ ТЕМ БОЛЬШЕ ВЕРОЯТНОСТЬ ЧТО О ЕГО ЗНАЧЕНИИ МОЖНО ДОГАДАТЬСЯ ТЕМ МЕНЬШЕ ВЕРОЯТНОСТЬ ТОГО ЧТО ИМЕННО ЭТО СЛОВО УБЬЕТ ВСЕ ПОНИМАНИЕ ПРЕДЛОЖЕНИЯ\n",
    "\n",
    "В то же время из контекста надо исключить тот же список \n",
    "\n",
    "Итого штрафуем важность слова в совокупном понимании предложения на величину 1/(кол-во прямо зависимых слов)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependencies (conllu_map, text_map_input, debug = False):\n",
    "    assert len(conllu_map) == len(text_map_input) #sentences count is equal\n",
    "    text_map = copy.deepcopy(text_map_input)\n",
    "    for sentence_conllu, text_map_sentence in zip(conllu_map,text_map):\n",
    "        nominal_index_correction_dict = {}\n",
    "        dep_dict = {}#включаем зависимости неисключенных слов от всех слов(если исключенные будут с контекстом то их просто оштрафует и нам не страшно)\n",
    "        #print(sentence_conllu)\n",
    "        nom2real_ind = {}\n",
    "        for word_conllu,word_map in zip(sentence_conllu,text_map_sentence): \n",
    "            nom2real_ind[word_conllu[0]] = word_map['vocabulary_prop']['real_index']\n",
    "            #print(word_conllu)\n",
    "            #print(word_map,'\\n')\n",
    "            #print(word[1], \"head_word_nominal_index =\", word[6])\n",
    "            if (word_conllu[3] not in pos_exclude_list):#make sure that dependent list will not count exclude pos\n",
    "                if(word_conllu[6] in dep_dict):\n",
    "                    dep_dict[word_conllu[6]].append(word_map['vocabulary_prop']['real_index'])\n",
    "                elif(int(word_conllu[6]) != 0):\n",
    "                    dep_dict[word_conllu[6]] = []\n",
    "                    dep_dict[word_conllu[6]].append(word_map['vocabulary_prop']['real_index'])\n",
    "            else:\n",
    "                nominal_index_correction_dict[word_conllu[0]] = (0.5,\"excluded_pos\")\n",
    "        if (debug == True):\n",
    "            print(\"sentence_conllu\", sentence_conllu)            \n",
    "            print(\"dep_dict\", dep_dict)\n",
    "            print(\"nom2real_ind\",nom2real_ind)\n",
    "        real_mean_dist = {}\n",
    "        \n",
    "        for head_element_nom_index, dep_elt_real_indexes in  dep_dict.items():\n",
    "            distances = []\n",
    "            \n",
    "            for dep_el_r_i in dep_elt_real_indexes:\n",
    "                if (debug == True):\n",
    "                    print(\"dep_el_r_i\",dep_el_r_i)\n",
    "                    print(\"distance\", abs(dep_el_r_i - nom2real_ind[head_element_nom_index]))\n",
    "                distances.append(abs(dep_el_r_i - nom2real_ind[head_element_nom_index]))\n",
    "\n",
    "            real_mean_distance = mean(distances)\n",
    "            real_mean_dist[head_element_nom_index] = real_mean_distance\n",
    "            real_context_element_count = len(dep_elt_real_indexes)\n",
    "            minimal_mean_distance = mean_min_dist[real_context_element_count]\n",
    "            \n",
    "            if real_context_element_count == 1: real_context_element_count += 1\n",
    "            real_context_mean_distance = real_mean_dist[head_element_nom_index]\n",
    "            \n",
    "            correction_coeff = (1+ np.log10(real_context_mean_distance/minimal_mean_distance))/(1+np.log(real_context_element_count))\n",
    "            if (debug == True):\n",
    "                print(\"mean_min_dist with\",real_context_element_count, \"elements\", minimal_mean_distance)\n",
    "                print(\"real_mean_dist\",real_context_mean_distance)\n",
    "                print(\"correction_coeff\",correction_coeff,\"vs\", 1/real_context_element_count,\"without log\")\n",
    "            nominal_index_correction_dict[head_element_nom_index] = [correction_coeff,dep_elt_real_indexes]\n",
    "            \n",
    "        #ШТРАФОВАТЬ ВАЖНОСТЬ ИСКЛЮЧАЕМЫХ СЛОВ \n",
    "        \n",
    "        \n",
    "        #print(\"\\n====\\n\",text_map_sentence,\"====\\n\")\n",
    "        for map_word in text_map_sentence:\n",
    "            word_nominal_index = map_word[\"vocabulary_prop\"][\"nominal_index\"] \n",
    "            if(word_nominal_index in nominal_index_correction_dict):#calculate context fine\n",
    "                print(nominal_index_correction_dict[word_nominal_index])\n",
    "                map_word[\"vocabulary_prop\"][\"dep_element_real_ind\"] = nominal_index_correction_dict[word_nominal_index][1]\n",
    "                map_word[\"vocabulary_prop\"][\"context_correction\"] = nominal_index_correction_dict[word_nominal_index][0]\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importance\"] = nominal_index_correction_dict[word_nominal_index][0] * map_word[\"vocabulary_prop\"]['tf_idf'] \n",
    "            else:\n",
    "                map_word[\"vocabulary_prop\"][\"vocab_importance\"] = map_word[\"vocabulary_prop\"][\"tf_idf\"]\n",
    "    return text_map\n",
    "                \n",
    "get_dependencies(conllu_text_map_ex, text_map_ex, debug = True) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
