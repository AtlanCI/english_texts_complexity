{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ СЛОВАРЬ ДЛЯ РАЗВОРАЧИВАНИЯ СОКРАЩЕННЫХ ФРАЗ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ain t</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aren t</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can t</td>\n",
       "      <td>cannot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can t ve</td>\n",
       "      <td>cannot have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'cause</td>\n",
       "      <td>because</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      short         long\n",
       "0     ain t      are not\n",
       "1    aren t      are not\n",
       "2     can t       cannot\n",
       "3  can t ve  cannot have\n",
       "4    'cause      because"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr = pd.read_csv(\"./materials/contractions.csv\")\n",
    "contr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {}\n",
    "for key, val in zip(contr['short'],contr['long']):\n",
    "    contractions[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ БАЗОВЫЙ СЛОВАРЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660 50 81 196 196 3309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4296"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_vocabulary = []\n",
    "with open(\"./materials/A1_vocab_processed.txt\", \"r\") as voc:\n",
    "    for word in voc.readlines():\n",
    "        basic_vocabulary.append(word[:-1])\n",
    "#basic_vocabulary = set(basic_vocabulary)\n",
    "#basic_vocabulary\n",
    "\n",
    "adjectives = []\n",
    "with open(\"./materials/common_adj.txt\", \"r\") as common_adj:\n",
    "    for word in common_adj.readlines():\n",
    "        adjectives.append(word[:-1])\n",
    "        \n",
    "common_uncountable = []\n",
    "with open(\"./materials/common_unountable_manually_filtered.txt\", \"r\") as common_unctbl:\n",
    "    for word in common_unctbl.readlines():\n",
    "        common_uncountable.append(word[:-1])\n",
    "\n",
    "countries = []\n",
    "with open(\"./materials/countries.txt\", \"r\") as cntr:\n",
    "    for word in cntr.readlines():\n",
    "        countries.append(word[:-1])\n",
    "\n",
    "names = []\n",
    "with open(\"./materials/names.txt\", \"r\") as names_file:\n",
    "    for word in names_file.readlines():\n",
    "        names.append(word[:-1])\n",
    "        \n",
    "print(len(basic_vocabulary), len(adjectives), len(common_uncountable), len(countries), len(countries), len(names))\n",
    "final_basic_vocabulary = basic_vocabulary\n",
    "final_basic_vocabulary.extend(adjectives)\n",
    "final_basic_vocabulary.extend(common_uncountable)\n",
    "final_basic_vocabulary.extend(countries)\n",
    "final_basic_vocabulary.extend(names)\n",
    "len(final_basic_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ СЛОВАРЬ ФРАЗОВЫХ ГЛАГОЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasal_list = []\n",
    "with open(\"phrasal_verbs.txt\", \"r\") as pv_doc:\n",
    "    for pv in pv_doc:\n",
    "        phrasal_list.append(pv[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СЧИТЫВАЕМ ТЕКСТЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jimmy’s Breakfast</td>\n",
       "      <td>This is my friend Jimmy. He is from India. Jim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Girl from Green Valley</td>\n",
       "      <td>Green Valley was a small village near the Icy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Alice in Wonderland</td>\n",
       "      <td>Alice was beginning to get very tired of sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Volkswagen's emissions cheating will cost the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My Friends</td>\n",
       "      <td>My name is Sonya. I am 20 and I come from Moro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     Title  \\\n",
       "0   1         Jimmy’s Breakfast   \n",
       "1   2  A Girl from Green Valley   \n",
       "2   3       Alice in Wonderland   \n",
       "3   4                Volkswagen   \n",
       "4   5                My Friends   \n",
       "\n",
       "                                                Text  \n",
       "0  This is my friend Jimmy. He is from India. Jim...  \n",
       "1  Green Valley was a small village near the Icy ...  \n",
       "2  Alice was beginning to get very tired of sitti...  \n",
       "3  Volkswagen's emissions cheating will cost the ...  \n",
       "4  My name is Sonya. I am 20 and I come from Moro...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_excel(\"./materials/texts.xlsx\")\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РАЗБИВАЕМ НА ПРЕДЛОЖЕНИЯ, ЧИСТИМ ПУНКТУАЦИЮ, ПРИВОДИМ К НИЖНЕМУ РЕГИСТРУ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncuation_primary = string.punctuation + \"’\" +\"“\" + \"”\" \n",
    "puncuation_primary = puncuation_primary.replace(\".\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i cannot visit u.s. and i could not to go by your daddy s car',\n",
       " 'i want to say hello to her brother']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "test_line = \"I can't  visit U.S. and I couldn't   to go by your daddy's car. I want to say hello to her brother.\"#’'\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def punct_setnence_lower (line, sent_detector, debug = False, deep_debug = False):\n",
    "    initial_sentences_list = sent_detector.tokenize(line.strip())\n",
    "    final_sentences_list = []\n",
    "    \n",
    "    for sentence in initial_sentences_list:\n",
    "        if(debug): print(\"sentence:\", sentence)\n",
    "        cleaned_line = ''\n",
    "        #clean by word\n",
    "        for word in sentence.split():\n",
    "            if(debug): print(word)\n",
    "            clean_word =''\n",
    "            for char in word:\n",
    "                if(deep_debug): print(\"char before cleaning:\", char)\n",
    "                if char not in puncuation_primary:\n",
    "                    clean_word += char.lower()\n",
    "                    if(deep_debug): print(\"char after cleaning:\", char.lower())\n",
    "                else:\n",
    "                    clean_word += ' '\n",
    "                    if(deep_debug): print(\"char has been deleted\")\n",
    "            clean_word = clean_word.lstrip()\n",
    "            if(debug):print(\"non punctuation and lower:\",clean_word)       \n",
    "\n",
    "            if (clean_word in contractions):\n",
    "                if(debug):print(\"word before contractions parsing:\",clean_word)\n",
    "                clean_word = ''.join(contractions[clean_word])\n",
    "                if(debug):print(\"word after contractions parsing:\",clean_word)\n",
    "            if(debug): print(\"finally cleaned word/s:\",clean_word)    \n",
    "            cleaned_line += clean_word + ' '\n",
    "        #handle abbreviations delete dots only in the end    \n",
    "        for char_ind in range(len(cleaned_line) - 3, len(cleaned_line)):\n",
    "            max_dot_index = len(cleaned_line)\n",
    "            if cleaned_line[char_ind] == \".\":\n",
    "                if(char_ind < max_dot_index): max_dot_index = char_ind\n",
    "                break\n",
    "        cleaned_line = cleaned_line[:max_dot_index]    \n",
    "        cleaned_line = re.sub(' +', ' ', cleaned_line)\n",
    "        final_sentences_list.append(cleaned_line)\n",
    "    return final_sentences_list\n",
    "            \n",
    "clean_line = punct_setnence_lower(test_line, sent_detector,debug = False)\n",
    "clean_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ ФУНКЦИЮ ДЛЯ ПОЛУЧЕНИЯ ГРАММАТИЧЕСКОЙ КАРТЫ ТЕКСТА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is my friend jimmy',\n",
       " 'he is from india',\n",
       " 'jimmy is a vegetarian',\n",
       " 'the best breakfast for him is a glass of orange juice two apples and three bananas',\n",
       " 'jimmy likes watermelons very much',\n",
       " 'a watermelon is green outside and red inside',\n",
       " 'it is hard and soft inside',\n",
       " 'it is juicy and sweet',\n",
       " 'what a lovely fruit ',\n",
       " 'a month ago sally received a letter',\n",
       " 'it was an invitation for a wedding',\n",
       " 'her sister victoria is getting married',\n",
       " 'last year sally saw such a ceremony',\n",
       " 'it took place on saturday',\n",
       " 'there were a lot of guests people wore nice clothes met a husband and a wife near the church threw confetti ate a very tasty cake made videos and took photos',\n",
       " 'in her letter victoria asked sally to be her bridesmaid',\n",
       " 'it means that on the day of wedding sally must help victoria when she puts her wedding dress on and brushes her hair',\n",
       " 'sally must also accompany victoria on her way to the church and carry a traditional bunch of flowers',\n",
       " 'sally agreed with pleasure and bought a beautiful dress for the ceremony',\n",
       " 'it was white and green with a long nice silk skirt',\n",
       " 'but at home she understood that she could not fit into it',\n",
       " 'what a pity ',\n",
       " 'that is why sally is on a diet now',\n",
       " 'she does not eat cakes sweets and fried potatoes',\n",
       " 'she does not even eat some very sweet fruit',\n",
       " 'she cooks vegetables fish and boiled meat',\n",
       " 'she writes down everything she eats into a special notebook and adds up the calories',\n",
       " 'what is more every morning she practices jogging and goes to the gym',\n",
       " 'she lost three kilos but she wants to lose two kilos more',\n",
       " 'everyone tells her she looks great but she does not agree',\n",
       " 'she is dreaming about a slice of wedding cake and does not want to give up',\n",
       " 'dieting is not easy ']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = texts[\"Text\"][0]\n",
    "example_text_processed = punct_setnence_lower(example_text,sent_detector)\n",
    "example_text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', {74: 89}),\n",
       "  ('be',\n",
       "   {74: 99,\n",
       "    'Number': 'sing',\n",
       "    'Person': 3,\n",
       "    'Tense': 'pres',\n",
       "    'VerbForm': 'fin'}),\n",
       "  ('my', {74: 83, 'Poss': 'yes', 'PronType': 'prs'}),\n",
       "  ('friend', {74: 91, 'Number': 'sing'}),\n",
       "  ('jimmy', {74: 91, 'Number': 'sing'})],\n",
       " [('he', {74: 94, 'PronType': 'prs'}),\n",
       "  ('be',\n",
       "   {74: 99,\n",
       "    'Number': 'sing',\n",
       "    'Person': 3,\n",
       "    'Tense': 'pres',\n",
       "    'VerbForm': 'fin'}),\n",
       "  ('from', {74: 84}),\n",
       "  ('india', {74: 91, 'Number': 'sing'})]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sent_gramm_features_map(text):\n",
    "    text_grammar_map = []\n",
    "    for sentence in text:\n",
    "        split_sent_list = sentence.split()\n",
    "        grammar_map = [None] * len(split_sent_list)  \n",
    "        parsed_sentence =  nlp(sentence)   \n",
    "        assert (len(parsed_sentence) == len(grammar_map))\n",
    "        for gramm_ind in range(len(split_sent_list)):\n",
    "            #print(parsed_sentence[gramm_ind])\n",
    "            if(parsed_sentence[gramm_ind].lemma_[0] == \"-\"):\n",
    "                parsed_sentence[gramm_ind].lemma_ = parsed_sentence[gramm_ind].text\n",
    "            grammar_map[gramm_ind] = (parsed_sentence[gramm_ind].lemma_, nlp.vocab.morphology.tag_map[parsed_sentence[gramm_ind].tag_])\n",
    "        text_grammar_map.append(grammar_map)\n",
    "    return text_grammar_map\n",
    "\n",
    "grammar_map_example = get_sent_gramm_features_map(example_text_processed)\n",
    "grammar_map_example[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЛЕММАТИЗИРУЕМ ТЕКСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this be my friend jimmy', 'he be from india', 'jimmy be a vegetarian']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lemm_text(text_gramm_map):\n",
    "    lemm_text = []\n",
    "    for sentence in text_gramm_map:\n",
    "        sentence_lemm = ''\n",
    "        for word in sentence:\n",
    "            sentence_lemm += word[0] + ' '\n",
    "        sentence_lemm = sentence_lemm[:-1]\n",
    "        lemm_text.append(sentence_lemm)\n",
    "        \n",
    "    return lemm_text\n",
    "\n",
    "ex_text_lemm = get_lemm_text(grammar_map_example)\n",
    "ex_text_lemm[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВЫЧИСЛЯЕМ ТФИДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accompany  adds  ago  agree  agreed    apples  asked  ate   bananas  \\\n",
      "0        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
      "1        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
      "2        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
      "3        0.0   0.0  0.0    0.0     0.0  0.377964    0.0  0.0  0.377964   \n",
      "4        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
      "\n",
      "   beautiful  ...   wants  watermelon  watermelons  way  wedding  white  wife  \\\n",
      "0        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
      "1        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
      "2        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
      "3        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
      "4        0.0  ...     0.0         0.0     0.612137  0.0      0.0    0.0   0.0   \n",
      "\n",
      "   wore  writes  year  \n",
      "0   0.0     0.0   0.0  \n",
      "1   0.0     0.0   0.0  \n",
      "2   0.0     0.0   0.0  \n",
      "3   0.0     0.0   0.0  \n",
      "4   0.0     0.0   0.0  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_tf_df_dataframe(lemm_text_list):\n",
    "    vect = TfidfVectorizer(stop_words = 'english')\n",
    "    tfidf_matrix = vect.fit_transform(lemm_text_list)\n",
    "    df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "tf_df_dataframe = get_tf_df_dataframe (example_text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df_dataframe.to_csv(\"./ex_text_tfidf.csv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ СПИСОК ВЕСОВ СЛОВ ДЛЯ ПОСЛЕДУЮЩЕГО ЗАПОЛНЕНИЯ ФИЧАМИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0, 'word': 'this'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'my'},\n",
       "  {'weight': 0, 'word': 'friend'},\n",
       "  {'weight': 0, 'word': 'jimmy'}],\n",
       " [{'weight': 0, 'word': 'he'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'from'},\n",
       "  {'weight': 0, 'word': 'india'}],\n",
       " [{'weight': 0, 'word': 'jimmy'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'a'},\n",
       "  {'weight': 0, 'word': 'vegetarian'}]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weights_empty_list(gramm_map_text):\n",
    "    weights_list = []\n",
    "    for sentence in gramm_map_text:\n",
    "        #print(sentence)\n",
    "        sentence_weights = []\n",
    "        for element in sentence:\n",
    "            weight = {\"word\" : element[0], \"weight\": 0}\n",
    "            sentence_weights.append(weight)\n",
    "        weights_list.append(sentence_weights)\n",
    "    return weights_list\n",
    "test_weights = get_weights_empty_list(grammar_map_example)\n",
    "test_weights[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ ФУНКЦИЮ ДЛЯ ВЫЯВЛЕНИЯ СОЖНОЙ ГРАММАТИКИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0, 'word': 'this'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'my'},\n",
       "  {'weight': 0, 'word': 'friend'},\n",
       "  {'weight': 0, 'word': 'jimmy'}],\n",
       " [{'weight': 0, 'word': 'he'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'from'},\n",
       "  {'weight': 0, 'word': 'india'}],\n",
       " [{'weight': 0, 'word': 'jimmy'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'a'},\n",
       "  {'weight': 0, 'word': 'vegetarian'}]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_difficult_grammar(text_grammar_map, weights_list, debug = False):\n",
    "    for sentence_grammar_map, sentence_weights in zip(text_grammar_map,weights_list):\n",
    "        for el_ind in range(len(sentence_grammar_map)):\n",
    "            #print(sentence_grammar_map[el_ind])\n",
    "            \n",
    "            if('Aspect' in sentence_grammar_map[el_ind][1]):\n",
    "                #present perfect\n",
    "                if ( sentence_grammar_map[el_ind][1]['Aspect'] == 'perf' ):\n",
    "                    if(sentence_grammar_map[el_ind - 2][0] == 'have' or sentence_grammar_map[el_ind - 1][0] == 'have'):\n",
    "                        if(debug): print(\"PRESENT PERFECT\")\n",
    "                        if(debug): print(sentence_grammar_map[el_ind - 1])\n",
    "                        if(debug): print(sentence_grammar_map[el_ind])\n",
    "\n",
    "                        sentence_weights[el_ind][\"diff_grammar\"] = \"pr_perf\"\n",
    "                        sentence_weights[el_ind - 1][\"diff_grammar\"] = \"pr_perf\"\n",
    "                \n",
    "                \n",
    "                elif(sentence_grammar_map[el_ind][1]['Aspect'] == 'prog'):\n",
    "                    #future continious check \"will + be + v-ing\"\n",
    "                    if (sentence_grammar_map[el_ind - 1][0] == 'be' and sentence_grammar_map[el_ind - 2][0] == 'will'):\n",
    "                        if(debug): print(\"FUTURE CONTINIOUS\")                       \n",
    "                        if(debug): print(\"prev word is\", sentence_grammar_map[el_ind - 2])\n",
    "                        if(debug): print(\"prev word is\", sentence_grammar_map[el_ind - 1])\n",
    "                        if(debug): print(sentence_grammar_map[el_ind])\n",
    "                        sentence_weights[el_ind][\"diff_grammar\"] = \"fut_cont\"\n",
    "                        sentence_weights[el_ind - 1][\"diff_grammar\"] = \"fut_cont\"\n",
    "                        sentence_weights[el_ind - 2][\"diff_grammar\"] = \"fut_cont\"\n",
    "                   #past continious was/were + v-ing\"     \n",
    "                    elif (sentence_grammar_map[el_ind - 1][0] == 'be' and sentence_grammar_map[el_ind -1][1]['Tense'] == 'past'):\n",
    "                        if(debug): print(\"PAST CONTINIOUS\")\n",
    "                        if(debug): print(sentence_grammar_map[el_ind])\n",
    "                        if(debug): print(\"prev word is\", sentence_grammar_map[el_ind - 1])\n",
    "                        sentence_weights[el_ind][\"diff_grammar\"] = \"past_cont\"\n",
    "                        sentence_weights[el_ind - 1][\"diff_grammar\"] = \"past_cont\"\n",
    "                        \n",
    "                \n",
    "    return weights_list\n",
    "\n",
    "test_weights = get_difficult_grammar(grammar_map_example,test_weights, debug = True)\n",
    "test_weights[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ ФНУКЦИЮ ДЛЯ ПОИСКА ФРАЗОВЫХ ГЛАГОЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrasal Verb found: agree with\n",
      "Phrasal Verb found: add up\n",
      "Phrasal Verb found: give up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0, 'word': 'she'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'dream'},\n",
       "  {'weight': 0, 'word': 'about'},\n",
       "  {'weight': 0, 'word': 'a'},\n",
       "  {'weight': 0, 'word': 'slice'},\n",
       "  {'weight': 0, 'word': 'of'},\n",
       "  {'weight': 0, 'word': 'wedding'},\n",
       "  {'weight': 0, 'word': 'cake'},\n",
       "  {'weight': 0, 'word': 'and'},\n",
       "  {'weight': 0, 'word': 'do'},\n",
       "  {'weight': 0, 'word': 'not'},\n",
       "  {'weight': 0, 'word': 'want'},\n",
       "  {'weight': 0, 'word': 'to'},\n",
       "  {'phrasal_verb': 'yes', 'weight': 0, 'word': 'give'},\n",
       "  {'phrasal_verb': 'yes', 'weight': 0, 'word': 'up'}],\n",
       " [{'weight': 0, 'word': 'dieting'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'not'},\n",
       "  {'weight': 0, 'word': 'easy'}]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_phrasal_verbs(text_grammar_map, weights_list, debug = False):\n",
    "    for sentence_grammar_map, sentence_weights in zip(text_grammar_map,weights_list):\n",
    "        for el_ind in range(1, len(sentence_grammar_map)):  \n",
    "            for searh_ind in range(1, 3):\n",
    "                try_phrase = sentence_grammar_map[el_ind - searh_ind][0] + ' ' + sentence_grammar_map[el_ind][0]\n",
    "                #print(try_phrase)\n",
    "                if(try_phrase in phrasal_list):\n",
    "                    if(debug): print(\"Phrasal Verb found:\", try_phrase)\n",
    "                    sentence_weights[el_ind][\"phrasal_verb\"] = \"yes\"\n",
    "                    sentence_weights[el_ind - searh_ind][\"phrasal_verb\"] = \"yes\"\n",
    "    return weights_list\n",
    "        #print(\"\\n\")\n",
    "test_weights = get_phrasal_verbs(grammar_map_example,test_weights,debug = True)\n",
    "test_weights[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПРИСВАИВАЕМ ВЕСА ТФИДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accompany</th>\n",
       "      <th>adds</th>\n",
       "      <th>ago</th>\n",
       "      <th>agree</th>\n",
       "      <th>agreed</th>\n",
       "      <th>apples</th>\n",
       "      <th>asked</th>\n",
       "      <th>ate</th>\n",
       "      <th>bananas</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>...</th>\n",
       "      <th>wants</th>\n",
       "      <th>watermelon</th>\n",
       "      <th>watermelons</th>\n",
       "      <th>way</th>\n",
       "      <th>wedding</th>\n",
       "      <th>white</th>\n",
       "      <th>wife</th>\n",
       "      <th>wore</th>\n",
       "      <th>writes</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accompany  adds  ago  agree  agreed    apples  asked  ate   bananas  \\\n",
       "0        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "1        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "2        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "3        0.0   0.0  0.0    0.0     0.0  0.377964    0.0  0.0  0.377964   \n",
       "4        0.0   0.0  0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "\n",
       "   beautiful  ...   wants  watermelon  watermelons  way  wedding  white  wife  \\\n",
       "0        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
       "1        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
       "2        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
       "3        0.0  ...     0.0         0.0     0.000000  0.0      0.0    0.0   0.0   \n",
       "4        0.0  ...     0.0         0.0     0.612137  0.0      0.0    0.0   0.0   \n",
       "\n",
       "   wore  writes  year  \n",
       "0   0.0     0.0   0.0  \n",
       "1   0.0     0.0   0.0  \n",
       "2   0.0     0.0   0.0  \n",
       "3   0.0     0.0   0.0  \n",
       "4   0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dict = tf_df_dataframe.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0.01, 'word': 'this'},\n",
       "  {'weight': 0.01, 'word': 'be'},\n",
       "  {'weight': 0.01, 'word': 'my'},\n",
       "  {'weight': 0.774119582428778, 'word': 'friend'},\n",
       "  {'weight': 0.6330393922184421, 'word': 'jimmy'}],\n",
       " [{'weight': 0.01, 'word': 'he'},\n",
       "  {'weight': 0.01, 'word': 'be'},\n",
       "  {'weight': 0.01, 'word': 'from'},\n",
       "  {'weight': 1.0, 'word': 'india'}],\n",
       " [{'weight': 0.6330393922184421, 'word': 'jimmy'},\n",
       "  {'weight': 0.01, 'word': 'be'},\n",
       "  {'weight': 0.01, 'word': 'a'},\n",
       "  {'weight': 0.774119582428778, 'word': 'vegetarian'}]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_tf_idf(text_grammar_map, weights_list, tf_idf_dict):\n",
    "    assert (len(text_grammar_map) == len(weights_list))\n",
    "    for sentence_ind in range(len(text_grammar_map)):\n",
    "        for el_ind in range(len(text_grammar_map[sentence_ind])):\n",
    "            if (weights_list[sentence_ind][el_ind][\"word\"] in tf_idf_dict):\n",
    "                weights_list[sentence_ind][el_ind][\"weight\"] = tf_idf_dict[weights_list[sentence_ind][el_ind][\"word\"]][sentence_ind]\n",
    "            else:\n",
    "                weights_list[sentence_ind][el_ind][\"weight\"] = 0.01\n",
    "                \n",
    "    return weights_list\n",
    "test_weights = assign_tf_idf(grammar_map_example, test_weights, tf_idf_dict)\n",
    "test_weights[:3]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЕЛИМ НА СЛОЖНЫЕ И НЕ СЛОЖНЫЕ В СООТВЕТСВИИ С ПОЛУЧЕННЫМИ ФИЧАМИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_groups(text_weights):\n",
    "    difficult_vocabulary = []\n",
    "    easy_vocabulary = []\n",
    "    for sentence_weights in text_weights:\n",
    "        for word_weight in sentence_weights:\n",
    "            #print(word_weight)\n",
    "            if('diff_grammar' in word_weight or 'phrasal_verb' in word_weight or word_weight['word'] not in basic_vocabulary):\n",
    "                difficult_vocabulary.append(word_weight)\n",
    "            else:\n",
    "                easy_vocabulary.append(word_weight)\n",
    "    return easy_vocabulary, difficult_vocabulary\n",
    "                \n",
    "easy, difficult = split_into_groups(test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 62)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(easy), len(difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weight': 0.774119582428778, 'word': 'vegetarian'},\n",
       " {'weight': 0.0, 'word': 'watermelon'},\n",
       " {'weight': 0.4664399378418825, 'word': 'watermelon'},\n",
       " {'weight': 0.5978117297300675, 'word': 'soft'},\n",
       " {'weight': 0.7457391260354046, 'word': 'juicy'},\n",
       " {'weight': 0.7457391260354046, 'word': 'lovely'},\n",
       " {'weight': 0.01, 'word': 'receive'},\n",
       " {'weight': 0.774119582428778, 'word': 'invitation'},\n",
       " {'weight': 0.6330393922184421, 'word': 'wedding'},\n",
       " {'weight': 0.01, 'word': 'marry'},\n",
       " {'weight': 0.01, 'word': 'such'},\n",
       " {'weight': 0.49926416816836866, 'word': 'ceremony'},\n",
       " {'weight': 0.01, 'word': 'guest'},\n",
       " {'weight': 0.01, 'word': 'clothe'},\n",
       " {'weight': 0.20945709765470802, 'word': 'church'},\n",
       " {'weight': 0.01, 'word': 'throw'},\n",
       " {'weight': 0.23445125960680535, 'word': 'confetti'},\n",
       " {'weight': 0.23445125960680535, 'word': 'tasty'},\n",
       " {'weight': 0.01, 'word': 'video'},\n",
       " {'weight': 0.5144649816247601, 'word': 'bridesmaid'},\n",
       " {'weight': 0.01, 'word': 'mean'},\n",
       " {'weight': 0.5058593576427194, 'word': 'wedding'},\n",
       " {'weight': 0.01, 'word': 'must'},\n",
       " {'weight': 0.5058593576427194, 'word': 'wedding'},\n",
       " {'weight': 0.01, 'word': 'brush'},\n",
       " {'weight': 0.01, 'word': 'must'},\n",
       " {'weight': 0.35855816508414967, 'word': 'accompany'},\n",
       " {'weight': 0.35855816508414967, 'word': 'way'},\n",
       " {'weight': 0.3203333295153842, 'word': 'church'},\n",
       " {'weight': 0.35855816508414967, 'word': 'traditional'},\n",
       " {'weight': 0.35855816508414967, 'word': 'bunch'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.0, 'word': 'agree'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.01, 'word': 'with'},\n",
       " {'weight': 0.40824242900045854, 'word': 'pleasure'},\n",
       " {'weight': 0.36472089960766496, 'word': 'ceremony'},\n",
       " {'weight': 0.4227167279649903, 'word': 'silk'},\n",
       " {'weight': 0.5773502691896257, 'word': 'fit'},\n",
       " {'weight': 1.0, 'word': 'pity'},\n",
       " {'weight': 0.8439884142349315, 'word': 'diet'},\n",
       " {'weight': 0.01, 'word': 'fry'},\n",
       " {'weight': 0.01, 'word': 'even'},\n",
       " {'weight': 0.01, 'word': 'boil'},\n",
       " {'weight': 0.01, 'word': 'everything'},\n",
       " {'weight': 0.408248290463863, 'word': 'notebook'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.01, 'word': 'add'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.01, 'word': 'up'},\n",
       " {'weight': 0.01, 'word': 'calorie'},\n",
       " {'weight': 0.01, 'word': 'practice'},\n",
       " {'weight': 0.01, 'word': 'jog'},\n",
       " {'weight': 0.447213595499958, 'word': 'gym'},\n",
       " {'weight': 0.3779644730092272, 'word': 'lose'},\n",
       " {'weight': 0.01, 'word': 'kilo'},\n",
       " {'weight': 0.3779644730092272, 'word': 'lose'},\n",
       " {'weight': 0.01, 'word': 'kilo'},\n",
       " {'weight': 0.01, 'word': 'everyone'},\n",
       " {'weight': 0.4674628518451882, 'word': 'agree'},\n",
       " {'weight': 0.01, 'word': 'dream'},\n",
       " {'weight': 0.4452993196257502, 'word': 'slice'},\n",
       " {'weight': 0.3641453039680802, 'word': 'wedding'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.01, 'word': 'give'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.01, 'word': 'up'},\n",
       " {'weight': 0.7071067811865476, 'word': 'dieting'}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is very diffcult, diff realtive weight is more then 0.3\n"
     ]
    }
   ],
   "source": [
    "def calculate_level(easy_words_weights, difficults_words_weights, debug = False):\n",
    "    tf_idf_easy = 0\n",
    "    for easy_w_w in easy_words_weights:\n",
    "        tf_idf_easy += easy_w_w['weight']\n",
    "        \n",
    "    tf_idf_diff = 0\n",
    "    for diff_w_w in difficults_words_weights:\n",
    "        tf_idf_diff += diff_w_w['weight']\n",
    "        \n",
    "    if(debug):print(len(easy_words_weights), len(difficults_words_weights))\n",
    "    if(debug):print(tf_idf_easy, tf_idf_diff)\n",
    "    overall_weights = tf_idf_easy + tf_idf_diff\n",
    "    if(debug):print(round(tf_idf_easy/overall_weights, 2), round(tf_idf_diff/overall_weights, 2))\n",
    "    if(round(tf_idf_diff/overall_weights, 2) >= 0.3):\n",
    "        print(\"text is very diffcult, diff realtive weight is more then 0.3\")\n",
    "    if(round(tf_idf_diff/overall_weights, 2) > 0.2 and round(tf_idf_diff/overall_weights, 2) < 0.3):\n",
    "        print(\"text is moderate\")\n",
    "    if(round(tf_idf_diff/overall_weights, 2) <= 0.2):\n",
    "        print(\"text is quite easy\")\n",
    "calculate_level(easy, difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_level(raw_text, print_debug_message = False, show_calucated_weights = False):\n",
    "    processed_text = punct_setnence_lower(raw_text,sent_detector)\n",
    "    \n",
    "    grammar_map = get_sent_gramm_features_map(processed_text)\n",
    "    \n",
    "    text_lemm = get_lemm_text(grammar_map)\n",
    "    \n",
    "    tfidf_dict = get_tf_idf_dict(text_lemm)\n",
    "    print(tfidf_dict)\n",
    "    \n",
    "    weights = get_weights_empty_list(grammar_map)\n",
    "    \n",
    "    weights = get_difficult_grammar(grammar_map,weights, debug = print_debug_message)\n",
    "    weights = get_phrasal_verbs(grammar_map,weights, debug = print_debug_message)\n",
    "    weights = assign_tf_idf(grammar_map,weights, tfidf_dict)\n",
    "    if(show_calucated_weights): \n",
    "        print(\"WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS\")\n",
    "        for sent_weights in weights:\n",
    "            for word_weight in sent_weights:\n",
    "                print(word_weight)\n",
    "            print(\"\\n\")\n",
    "    easy, difficult = split_into_groups(weights)\n",
    "    \n",
    "    calculate_level(easy, difficult,  debug = print_debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'white': 1.0, 'way': 1.0, 'pleasure': 1.0, 'much': 1.0, 'make': 1.0, 'day': 1.0, 'could': 1.0, 'sister': 0.778, 'get': 0.628}\n",
      "PAST CONTINIOUS\n",
      "('begin', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "('peep', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "PAST CONTINIOUS\n",
      "('read', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "PAST CONTINIOUS\n",
      "('consider', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS\n",
      "{'word': 'alice', 'weight': 0.1}\n",
      "{'word': 'be', 'weight': 0.1, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'begin', 'weight': 0.1, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'to', 'weight': 0.1}\n",
      "{'word': 'get', 'weight': 0.628}\n",
      "{'word': 'very', 'weight': 0.1}\n",
      "{'word': 'tired', 'weight': 0.1}\n",
      "{'word': 'of', 'weight': 0.1}\n",
      "{'word': 'sit', 'weight': 0.1}\n",
      "{'word': 'by', 'weight': 0.1}\n",
      "{'word': 'her', 'weight': 0.1}\n",
      "{'word': 'sister', 'weight': 0.778}\n",
      "{'word': 'on', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'bank', 'weight': 0.1}\n",
      "{'word': 'and', 'weight': 0.1}\n",
      "{'word': 'of', 'weight': 0.1}\n",
      "{'word': 'have', 'weight': 0.1}\n",
      "{'word': 'nothing', 'weight': 0.1}\n",
      "{'word': 'to', 'weight': 0.1}\n",
      "{'word': 'do', 'weight': 0.1}\n",
      "{'word': 'once', 'weight': 0.1}\n",
      "{'word': 'or', 'weight': 0.1}\n",
      "{'word': 'twice', 'weight': 0.1}\n",
      "{'word': 'she', 'weight': 0.1}\n",
      "{'word': 'have', 'weight': 0.1, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'peep', 'weight': 0.1, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'into', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'book', 'weight': 0.1}\n",
      "{'word': 'her', 'weight': 0.1}\n",
      "{'word': 'sister', 'weight': 0.778}\n",
      "{'word': 'be', 'weight': 0.1, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'read', 'weight': 0.1, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'but', 'weight': 0.1}\n",
      "{'word': 'it', 'weight': 0.1}\n",
      "{'word': 'have', 'weight': 0.1}\n",
      "{'word': 'no', 'weight': 0.1}\n",
      "{'word': 'picture', 'weight': 0.1}\n",
      "{'word': 'or', 'weight': 0.1}\n",
      "{'word': 'conversation', 'weight': 0.1}\n",
      "{'word': 'in', 'weight': 0.1}\n",
      "{'word': 'it', 'weight': 0.1}\n",
      "{'word': 'and', 'weight': 0.1}\n",
      "{'word': 'what', 'weight': 0.1}\n",
      "{'word': 'be', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'use', 'weight': 0.1}\n",
      "{'word': 'of', 'weight': 0.1}\n",
      "{'word': 'a', 'weight': 0.1}\n",
      "{'word': 'book', 'weight': 0.1}\n",
      "{'word': 'think', 'weight': 0.1}\n",
      "{'word': 'alice', 'weight': 0.1}\n",
      "{'word': 'without', 'weight': 0.1}\n",
      "{'word': 'picture', 'weight': 0.1}\n",
      "{'word': 'or', 'weight': 0.1}\n",
      "{'word': 'conversation', 'weight': 0.1}\n",
      "\n",
      "\n",
      "{'word': 'so', 'weight': 0.1}\n",
      "{'word': 'she', 'weight': 0.1}\n",
      "{'word': 'be', 'weight': 0.1, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'consider', 'weight': 0.1, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'in', 'weight': 0.1}\n",
      "{'word': 'her', 'weight': 0.1}\n",
      "{'word': 'own', 'weight': 0.1}\n",
      "{'word': 'mind', 'weight': 0.1}\n",
      "{'word': 'as', 'weight': 0.1}\n",
      "{'word': 'well', 'weight': 0.1}\n",
      "{'word': 'as', 'weight': 0.1}\n",
      "{'word': 'she', 'weight': 0.1}\n",
      "{'word': 'could', 'weight': 1.0}\n",
      "{'word': 'for', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'hot', 'weight': 0.1}\n",
      "{'word': 'day', 'weight': 1.0}\n",
      "{'word': 'make', 'weight': 1.0}\n",
      "{'word': 'her', 'weight': 0.1}\n",
      "{'word': 'feel', 'weight': 0.1}\n",
      "{'word': 'very', 'weight': 0.1}\n",
      "{'word': 'sleepy', 'weight': 0.1}\n",
      "{'word': 'and', 'weight': 0.1}\n",
      "{'word': 'stupid', 'weight': 0.1}\n",
      "{'word': 'whether', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'pleasure', 'weight': 1.0}\n",
      "{'word': 'of', 'weight': 0.1}\n",
      "{'word': 'make', 'weight': 1.0}\n",
      "{'word': 'a', 'weight': 0.1}\n",
      "{'word': 'daisy', 'weight': 0.1}\n",
      "{'word': 'chain', 'weight': 0.1}\n",
      "{'word': 'would', 'weight': 0.1}\n",
      "{'word': 'be', 'weight': 0.1}\n",
      "{'word': 'worth', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'trouble', 'weight': 0.1}\n",
      "{'word': 'of', 'weight': 0.1}\n",
      "{'word': 'get', 'weight': 0.628}\n",
      "{'word': 'up', 'weight': 0.1}\n",
      "{'word': 'and', 'weight': 0.1}\n",
      "{'word': 'pick', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'daisy', 'weight': 0.1}\n",
      "{'word': 'when', 'weight': 0.1}\n",
      "{'word': 'suddenly', 'weight': 0.1}\n",
      "{'word': 'a', 'weight': 0.1}\n",
      "{'word': 'white', 'weight': 1.0}\n",
      "{'word': 'rabbit', 'weight': 0.1}\n",
      "{'word': 'with', 'weight': 0.1}\n",
      "{'word': 'pink', 'weight': 0.1}\n",
      "{'word': 'eye', 'weight': 0.1}\n",
      "{'word': 'run', 'weight': 0.1}\n",
      "{'word': 'close', 'weight': 0.1}\n",
      "{'word': 'by', 'weight': 0.1}\n",
      "{'word': 'her', 'weight': 0.1}\n",
      "\n",
      "\n",
      "{'word': 'there', 'weight': 0.1}\n",
      "{'word': 'be', 'weight': 0.1}\n",
      "{'word': 'nothing', 'weight': 0.1}\n",
      "{'word': 'so', 'weight': 0.1}\n",
      "{'word': 'very', 'weight': 0.1}\n",
      "{'word': 'remarkable', 'weight': 0.1}\n",
      "{'word': 'in', 'weight': 0.1}\n",
      "{'word': 'that', 'weight': 0.1}\n",
      "{'word': 'nor', 'weight': 0.1}\n",
      "{'word': 'do', 'weight': 0.1}\n",
      "{'word': 'alice', 'weight': 0.1}\n",
      "{'word': 'think', 'weight': 0.1}\n",
      "{'word': 'it', 'weight': 0.1}\n",
      "{'word': 'so', 'weight': 0.1}\n",
      "{'word': 'very', 'weight': 0.1}\n",
      "{'word': 'much', 'weight': 1.0}\n",
      "{'word': 'out', 'weight': 0.1}\n",
      "{'word': 'of', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'way', 'weight': 1.0}\n",
      "{'word': 'to', 'weight': 0.1}\n",
      "{'word': 'hear', 'weight': 0.1}\n",
      "{'word': 'the', 'weight': 0.1}\n",
      "{'word': 'rabbit', 'weight': 0.1}\n",
      "{'word': 'say', 'weight': 0.1}\n",
      "{'word': 'to', 'weight': 0.1}\n",
      "{'word': 'itself', 'weight': 0.1}\n",
      "{'word': 'oh', 'weight': 0.1}\n",
      "{'word': 'dear', 'weight': 0.1}\n",
      "\n",
      "\n",
      "{'word': 'oh', 'weight': 0.1}\n",
      "{'word': 'dear', 'weight': 0.1}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.1}\n",
      "{'word': 'shall', 'weight': 0.1}\n",
      "{'word': 'be', 'weight': 0.1}\n",
      "{'word': 'late', 'weight': 0.1}\n",
      "\n",
      "\n",
      "116 32\n",
      "19.412000000000017 5.0\n",
      "0.8 0.2\n",
      "text is quite easy\n"
     ]
    }
   ],
   "source": [
    "calculate_text_level(texts['Text'][2],print_debug_message = True, show_calucated_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
