{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ СЛОВАРЬ ДЛЯ РАЗВОРАЧИВАНИЯ СОКРАЩЕННЫХ ФРАЗ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ain t</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aren t</td>\n",
       "      <td>are not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cannot</td>\n",
       "      <td>can not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can t</td>\n",
       "      <td>can not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can t ve</td>\n",
       "      <td>can not have</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      short          long\n",
       "0     ain t       are not\n",
       "1    aren t       are not\n",
       "2    cannot       can not\n",
       "3     can t       can not\n",
       "4  can t ve  can not have"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr = pd.read_csv(\"./materials/contractions.csv\")\n",
    "contr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {}\n",
    "for key, val in zip(contr['short'],contr['long']):\n",
    "    contractions[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ БАЗОВЫЙ СЛОВАРЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660 50 81 196 196 3309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4296"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_vocabulary = []\n",
    "with open(\"./materials/A1_vocab_processed.txt\", \"r\") as voc:\n",
    "    for word in voc.readlines():\n",
    "        basic_vocabulary.append(word[:-1])\n",
    "#basic_vocabulary = set(basic_vocabulary)\n",
    "#basic_vocabulary\n",
    "\n",
    "adjectives = []\n",
    "with open(\"./materials/common_adj.txt\", \"r\") as common_adj:\n",
    "    for word in common_adj.readlines():\n",
    "        adjectives.append(word[:-1])\n",
    "        \n",
    "common_uncountable = []\n",
    "with open(\"./materials/common_unountable_manually_filtered.txt\", \"r\") as common_unctbl:\n",
    "    for word in common_unctbl.readlines():\n",
    "        common_uncountable.append(word[:-1])\n",
    "\n",
    "countries = []\n",
    "with open(\"./materials/countries.txt\", \"r\") as cntr:\n",
    "    for word in cntr.readlines():\n",
    "        countries.append(word[:-1])\n",
    "\n",
    "names = []\n",
    "with open(\"./materials/names.txt\", \"r\") as names_file:\n",
    "    for word in names_file.readlines():\n",
    "        names.append(word[:-1])\n",
    "        \n",
    "print(len(basic_vocabulary), len(adjectives), len(common_uncountable), len(countries), len(countries), len(names))\n",
    "final_basic_vocabulary = basic_vocabulary\n",
    "final_basic_vocabulary.extend(adjectives)\n",
    "final_basic_vocabulary.extend(common_uncountable)\n",
    "final_basic_vocabulary.extend(countries)\n",
    "final_basic_vocabulary.extend(names)\n",
    "len(final_basic_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ СЛОВАРЬ ФРАЗОВЫХ ГЛАГОЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasal_list = []\n",
    "with open(\"./materials/phrasal_verbs.txt\", \"r\") as pv_doc:\n",
    "    for pv in pv_doc:\n",
    "        phrasal_list.append(pv[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СЧИТЫВАЕМ ТЕКСТЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jimmy’s Breakfast</td>\n",
       "      <td>This is my friend Jimmy. He is from India. Jim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Girl from Green Valley</td>\n",
       "      <td>Green Valley was a small village near the Icy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Alice in Wonderland</td>\n",
       "      <td>Alice was beginning to get very tired of sitti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Volkswagen's emissions cheating will cost the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My Friends</td>\n",
       "      <td>My name is Sonya. I am 20 and I come from Moro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                     Title  \\\n",
       "0   1         Jimmy’s Breakfast   \n",
       "1   2  A Girl from Green Valley   \n",
       "2   3       Alice in Wonderland   \n",
       "3   4                Volkswagen   \n",
       "4   5                My Friends   \n",
       "\n",
       "                                                Text  \n",
       "0  This is my friend Jimmy. He is from India. Jim...  \n",
       "1  Green Valley was a small village near the Icy ...  \n",
       "2  Alice was beginning to get very tired of sitti...  \n",
       "3  Volkswagen's emissions cheating will cost the ...  \n",
       "4  My name is Sonya. I am 20 and I come from Moro...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_excel(\"./materials/texts.xlsx\")\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РАЗБИВАЕМ НА ПРЕДЛОЖЕНИЯ, ЧИСТИМ ПУНКТУАЦИЮ, ПРИВОДИМ К НИЖНЕМУ РЕГИСТРУ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncuation_primary = string.punctuation + \"’\" +\"“\" + \"”\" + \"…\"\n",
    "#puncuation_primary = puncuation_primary.replace(\".\",\"\")#можно было сделать для сохранения аббревиатур \n",
    "#но из-за некорректной работы sentence токенайзера вызывает ошибки ассерта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she dreamt about being on the stage one day director said today the king is going to visit our performance']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#test_line = \"  I can't  visit U.S. and I couldn't   to go by your daddy's car... I want to say hello to her brother.   \",\n",
    "test_line = \"she dreamt about being on the stage… one day director said today the king is going to visit our performance \"#’'\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def punct_setnence_lower (line, sent_detector, debug = False, deep_debug = False):\n",
    "    initial_sentences_list = sent_detector.tokenize(line.strip())\n",
    "    final_sentences_list = []\n",
    "    \n",
    "    for sentence in initial_sentences_list:\n",
    "        if(debug): print(\"sentence:\", sentence)\n",
    "        cleaned_line = ''\n",
    "        #clean by word\n",
    "        for word in sentence.split():\n",
    "            if(debug): print(word)\n",
    "            clean_word =''\n",
    "            for char in word:\n",
    "                if(deep_debug): print(\"char before cleaning:\", char)\n",
    "                if char not in puncuation_primary and char.isalpha():\n",
    "                    clean_word += char.lower()\n",
    "                    if(deep_debug): print(\"char after cleaning:\", char.lower())\n",
    "                else:\n",
    "                    clean_word += ' '\n",
    "                    if(deep_debug): print(\"char has been deleted\")\n",
    "            clean_word = clean_word.lstrip()\n",
    "            if(debug):print(\"non punctuation and lower:\",clean_word)       \n",
    "\n",
    "            if (clean_word in contractions):\n",
    "                if(debug):print(\"word before contractions parsing:\",clean_word)\n",
    "                clean_word = ''.join(contractions[clean_word])\n",
    "                if(debug):print(\"word after contractions parsing:\",clean_word)\n",
    "            if(debug): print(\"finally cleaned word/s:\",clean_word)    \n",
    "            cleaned_line += clean_word + ' '\n",
    "        \n",
    "        \"\"\"\n",
    "        #handle abbreviations delete dots only in the end    \n",
    "        for char_ind in range(len(cleaned_line) - 3, len(cleaned_line)):\n",
    "            max_dot_index = len(cleaned_line)\n",
    "            if cleaned_line[char_ind] == \".\":\n",
    "                if(char_ind < max_dot_index): max_dot_index = char_ind\n",
    "                break\n",
    "        cleaned_line = cleaned_line[:max_dot_index]   \"\"\" \n",
    "        cleaned_line = re.sub(' +', ' ', cleaned_line)\n",
    "        cleaned_line = cleaned_line.rstrip()\n",
    "        cleaned_line = cleaned_line.lstrip()\n",
    "        final_sentences_list.append(cleaned_line)\n",
    "    return final_sentences_list\n",
    "            \n",
    "clean_line = punct_setnence_lower(test_line, sent_detector,debug = False, deep_debug = False)\n",
    "clean_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ ФУНКЦИЮ ДЛЯ ПОЛУЧЕНИЯ ГРАММАТИЧЕСКОЙ КАРТЫ ТЕКСТА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['green valley was a small village near the icy mountains',\n",
       " 'mary lived happily there with her grandmother',\n",
       " 'but one day her grandma fell ill she could not work anymore and earn enough money',\n",
       " 'she asked mary to read a letter',\n",
       " 'it was her granny s letter to mary s uncle',\n",
       " 'he was a rich man and lived in a big city',\n",
       " 'you are asking him about the job in the circus',\n",
       " 'mary was very much surprised',\n",
       " 'she did not want to leave her sweet home did not want to part with her beloved grandmother did not want to work in the circus',\n",
       " 'but her granny said you are not a child already you are a grown up young lady',\n",
       " 'you must go to the city in the city everything was different practically no trees and flowers in the streets a lot of cars and high stone buildings and much noise',\n",
       " 'but people were as kind and friendly as in green valley',\n",
       " 'they explained her the way to the circus and she easily found the place turned right followed a wide street turned left and here it was',\n",
       " 'mary s uncle was very glad to see her',\n",
       " 'he was director of the circus',\n",
       " 'he needed more people to care about animals',\n",
       " 'so mary s duties were to clean the cages to feed animals to wash them',\n",
       " 'there were many different animals in the circus monkeys horses elephants tigers lions',\n",
       " 'the job was hard and difficult',\n",
       " 'mary did not complain',\n",
       " 'every week she wrote letters to her granny',\n",
       " 'in every letter she wrote i am fine i am happy but she was not happy at all',\n",
       " 'in the evening standing behind the curtains during the performance she dreamt about being on the stage… one day director said today the king is going to visit our performance',\n",
       " 'you should act as well as you can unfortunately one of the most popular gymnasts broke her leg and no one else could work with horses instead of her',\n",
       " 'suddenly mary said i can',\n",
       " 'i know her program i can try her uncle had no choice and let her take part in the performance',\n",
       " 'mary was beautiful that evening',\n",
       " 'she looked nice in the circus clothes and did all the tricks perfectly',\n",
       " 'the king liked her very much and applauded for a long time',\n",
       " 'when she came home she wrote to her granny i am really very happy and this time it was true']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_setnence_lower(texts['Text'][1],sent_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is my friend jimmy',\n",
       " 'he is from india',\n",
       " 'jimmy is a vegetarian',\n",
       " 'the best breakfast for him is a glass of orange juice two apples and three bananas',\n",
       " 'jimmy likes watermelons very much',\n",
       " 'a watermelon is green outside and red inside',\n",
       " 'it is hard and soft inside',\n",
       " 'it is juicy and sweet',\n",
       " 'what a lovely fruit ',\n",
       " 'a month ago sally received a letter',\n",
       " 'it was an invitation for a wedding',\n",
       " 'her sister victoria is getting married',\n",
       " 'last year sally saw such a ceremony',\n",
       " 'it took place on saturday',\n",
       " 'there were a lot of guests people wore nice clothes met a husband and a wife near the church threw confetti ate a very tasty cake made videos and took photos',\n",
       " 'in her letter victoria asked sally to be her bridesmaid',\n",
       " 'it means that on the day of wedding sally must help victoria when she puts her wedding dress on and brushes her hair',\n",
       " 'sally must also accompany victoria on her way to the church and carry a traditional bunch of flowers',\n",
       " 'sally agreed with pleasure and bought a beautiful dress for the ceremony',\n",
       " 'it was white and green with a long nice silk skirt',\n",
       " 'but at home she understood that she could not fit into it',\n",
       " 'what a pity ',\n",
       " 'that is why sally is on a diet now',\n",
       " 'she does not eat cakes sweets and fried potatoes',\n",
       " 'she does not even eat some very sweet fruit',\n",
       " 'she cooks vegetables fish and boiled meat',\n",
       " 'she writes down everything she eats into a special notebook and adds up the calories',\n",
       " 'what is more every morning she practices jogging and goes to the gym',\n",
       " 'she lost three kilos but she wants to lose two kilos more',\n",
       " 'everyone tells her she looks great but she does not agree',\n",
       " 'she is dreaming about a slice of wedding cake and does not want to give up',\n",
       " 'dieting is not easy ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = texts[\"Text\"][0]\n",
    "example_text_processed = punct_setnence_lower(example_text,sent_detector)\n",
    "example_text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', {74: 89}),\n",
       "  ('be',\n",
       "   {74: 99,\n",
       "    'Number': 'sing',\n",
       "    'Person': 3,\n",
       "    'Tense': 'pres',\n",
       "    'VerbForm': 'fin'}),\n",
       "  ('my', {74: 83, 'Poss': 'yes', 'PronType': 'prs'}),\n",
       "  ('friend', {74: 91, 'Number': 'sing'}),\n",
       "  ('jimmy', {74: 91, 'Number': 'sing'})],\n",
       " [('he', {74: 94, 'PronType': 'prs'}),\n",
       "  ('be',\n",
       "   {74: 99,\n",
       "    'Number': 'sing',\n",
       "    'Person': 3,\n",
       "    'Tense': 'pres',\n",
       "    'VerbForm': 'fin'}),\n",
       "  ('from', {74: 84}),\n",
       "  ('india', {74: 91, 'Number': 'sing'})]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sent_gramm_features_map(text):\n",
    "    text_grammar_map = []\n",
    "    for sentence in text:\n",
    "        split_sent_list = sentence.split()\n",
    "        grammar_map = [None] * len(split_sent_list)  \n",
    "        parsed_sentence =  nlp(sentence)   \n",
    "        #print(len(parsed_sentence), len(grammar_map))\n",
    "        if(len(parsed_sentence) != len(split_sent_list)):\n",
    "            print(\"ASSERTION ERROR!\")\n",
    "            print(\"parsed_sentence\", len(parsed_sentence), \"split_sent_list\", len(split_sent_list),len(grammar_map))\n",
    "            print( \"split_sent_list\", split_sent_list)\n",
    "            for element_ind in range(len(parsed_sentence)):\n",
    "                print(\"parsed lemma:\", parsed_sentence[element_ind].lemma_, element_ind)\n",
    "        assert (len(parsed_sentence) == len(grammar_map))\n",
    "        for gramm_ind in range(len(split_sent_list)):\n",
    "            #print(parsed_sentence[gramm_ind])\n",
    "            if(parsed_sentence[gramm_ind].lemma_[0] == \"-\"):\n",
    "                parsed_sentence[gramm_ind].lemma_ = parsed_sentence[gramm_ind].text\n",
    "            grammar_map[gramm_ind] = (parsed_sentence[gramm_ind].lemma_, nlp.vocab.morphology.tag_map[parsed_sentence[gramm_ind].tag_])\n",
    "        text_grammar_map.append(grammar_map)\n",
    "    return text_grammar_map\n",
    "\n",
    "grammar_map_example = get_sent_gramm_features_map(example_text_processed)\n",
    "grammar_map_example[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЛЕММАТИЗИРУЕМ ТЕКСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this be my friend jimmy', 'he be from india', 'jimmy be a vegetarian']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lemm_text(text_gramm_map):\n",
    "    lemm_text = []\n",
    "    for sentence in text_gramm_map:\n",
    "        sentence_lemm = ''\n",
    "        for word in sentence:\n",
    "            sentence_lemm += word[0] + ' '\n",
    "        sentence_lemm = sentence_lemm[:-1]\n",
    "        lemm_text.append(sentence_lemm)\n",
    "        \n",
    "    return lemm_text\n",
    "\n",
    "ex_text_lemm = get_lemm_text(grammar_map_example)\n",
    "ex_text_lemm[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВЫЧИСЛЯЕМ ТФИДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_dict(lemm_text_list, save_to_csv = False):\n",
    "    vect = TfidfVectorizer(stop_words = 'english')\n",
    "    tfidf_matrix = vect.fit_transform(lemm_text_list)\n",
    "    df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
    "    #print(df.head())\n",
    "    if (save_to_csv): df.to_csv(\"./text_0_tfidf.xlsx\", sep = '\\t')\n",
    "    tf_idf_dict = df.to_dict()\n",
    "    return tf_idf_dict\n",
    "tf_idf_dict_example = get_tf_idf_dict (ex_text_lemm, save_to_csv = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ СПИСОК ВЕСОВ СЛОВ ДЛЯ ПОСЛЕДУЮЩЕГО ЗАПОЛНЕНИЯ ФИЧАМИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0, 'word': 'this'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'my'},\n",
       "  {'weight': 0, 'word': 'friend'},\n",
       "  {'weight': 0, 'word': 'jimmy'}],\n",
       " [{'weight': 0, 'word': 'he'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'from'},\n",
       "  {'weight': 0, 'word': 'india'}],\n",
       " [{'weight': 0, 'word': 'jimmy'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'a'},\n",
       "  {'weight': 0, 'word': 'vegetarian'}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weights_empty_list(gramm_map_text):\n",
    "    weights_list = []\n",
    "    for sentence in gramm_map_text:\n",
    "        #print(sentence)\n",
    "        sentence_weights = []\n",
    "        for element in sentence:\n",
    "            weight = {\"word\" : element[0], \"weight\": 0}\n",
    "            sentence_weights.append(weight)\n",
    "        weights_list.append(sentence_weights)\n",
    "    return weights_list\n",
    "test_weights = get_weights_empty_list(grammar_map_example)\n",
    "test_weights[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ ФУНКЦИЮ ДЛЯ ВЫЯВЛЕНИЯ СОЖНОЙ ГРАММАТИКИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0.05, 'word': 'this'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'my'},\n",
       "  {'weight': 0.774119582428778, 'word': 'friend'},\n",
       "  {'weight': 0.6330393922184421, 'word': 'jimmy'}],\n",
       " [{'weight': 0.05, 'word': 'he'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'from'},\n",
       "  {'weight': 1.0, 'word': 'india'}],\n",
       " [{'weight': 0.6330393922184421, 'word': 'jimmy'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.774119582428778, 'word': 'vegetarian'}]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_difficult_grammar(text_grammar_map, weights_list, debug = False):\n",
    "    for sentence_grammar_map, sentence_weights in zip(text_grammar_map,weights_list):\n",
    "        for el_ind in range(len(sentence_grammar_map)):\n",
    "            #print(sentence_grammar_map[el_ind])\n",
    "            \n",
    "            if('Aspect' in sentence_grammar_map[el_ind][1]):\n",
    "                #present perfect\n",
    "                if ( sentence_grammar_map[el_ind][1]['Aspect'] == 'perf' ):\n",
    "                    if(sentence_grammar_map[el_ind - 2][0] == 'have' or sentence_grammar_map[el_ind - 1][0] == 'have'):\n",
    "                        if(debug): print(\"PRESENT PERFECT\")\n",
    "                        if(debug): print(sentence_grammar_map[el_ind - 1])\n",
    "                        if(debug): print(sentence_grammar_map[el_ind])\n",
    "\n",
    "                        sentence_weights[el_ind][\"diff_grammar\"] = \"pr_perf\"\n",
    "                        sentence_weights[el_ind - 1][\"diff_grammar\"] = \"pr_perf\"\n",
    "                \n",
    "                \n",
    "                elif(sentence_grammar_map[el_ind][1]['Aspect'] == 'prog'):\n",
    "                    #future continious check \"will + be + v-ing\"\n",
    "                    if (sentence_grammar_map[el_ind - 1][0] == 'be' and sentence_grammar_map[el_ind - 2][0] == 'will'):\n",
    "                        if(debug): print(\"FUTURE CONTINIOUS\")                       \n",
    "                        if(debug): print(\"prev word is\", sentence_grammar_map[el_ind - 2])\n",
    "                        if(debug): print(\"prev word is\", sentence_grammar_map[el_ind - 1])\n",
    "                        if(debug): print(sentence_grammar_map[el_ind])\n",
    "                        sentence_weights[el_ind][\"diff_grammar\"] = \"fut_cont\"\n",
    "                        sentence_weights[el_ind - 1][\"diff_grammar\"] = \"fut_cont\"\n",
    "                        sentence_weights[el_ind - 2][\"diff_grammar\"] = \"fut_cont\"\n",
    "                   #past continious was/were + v-ing\"     \n",
    "                    elif (sentence_grammar_map[el_ind - 1][0] == 'be' and 'Tense' in sentence_grammar_map[el_ind -1][1]):\n",
    "                        if(sentence_grammar_map[el_ind -1][1]['Tense'] == 'past'):\n",
    "                            if(debug): print(\"PAST CONTINIOUS\")\n",
    "                            if(debug): print(sentence_grammar_map[el_ind])\n",
    "                            if(debug): print(\"prev word is\", sentence_grammar_map[el_ind - 1])\n",
    "                            sentence_weights[el_ind][\"diff_grammar\"] = \"past_cont\"\n",
    "                            sentence_weights[el_ind - 1][\"diff_grammar\"] = \"past_cont\"\n",
    "                        \n",
    "                \n",
    "    return weights_list\n",
    "\n",
    "test_weights = get_difficult_grammar(grammar_map_example,test_weights, debug = True)\n",
    "test_weights[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЗАДАЕМ ФНУКЦИЮ ДЛЯ ПОИСКА ФРАЗОВЫХ ГЛАГОЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrasal Verb found: agree with\n",
      "Phrasal Verb found: add up\n",
      "Phrasal Verb found: give up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0, 'word': 'she'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'dream'},\n",
       "  {'weight': 0, 'word': 'about'},\n",
       "  {'weight': 0, 'word': 'a'},\n",
       "  {'weight': 0, 'word': 'slice'},\n",
       "  {'weight': 0, 'word': 'of'},\n",
       "  {'weight': 0, 'word': 'wedding'},\n",
       "  {'weight': 0, 'word': 'cake'},\n",
       "  {'weight': 0, 'word': 'and'},\n",
       "  {'weight': 0, 'word': 'do'},\n",
       "  {'weight': 0, 'word': 'not'},\n",
       "  {'weight': 0, 'word': 'want'},\n",
       "  {'weight': 0, 'word': 'to'},\n",
       "  {'phrasal_verb': 'yes', 'weight': 0, 'word': 'give'},\n",
       "  {'phrasal_verb': 'yes', 'weight': 0, 'word': 'up'}],\n",
       " [{'weight': 0, 'word': 'dieting'},\n",
       "  {'weight': 0, 'word': 'be'},\n",
       "  {'weight': 0, 'word': 'not'},\n",
       "  {'weight': 0, 'word': 'easy'}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_phrasal_verbs(text_grammar_map, weights_list, debug = False):\n",
    "    for sentence_grammar_map, sentence_weights in zip(text_grammar_map,weights_list):\n",
    "        for el_ind in range(1, len(sentence_grammar_map)):  \n",
    "            for searh_ind in range(1, 3):\n",
    "                try_phrase = sentence_grammar_map[el_ind - searh_ind][0] + ' ' + sentence_grammar_map[el_ind][0]\n",
    "                #print(try_phrase)\n",
    "                if(try_phrase in phrasal_list):\n",
    "                    if(debug): print(\"Phrasal Verb found:\", try_phrase)\n",
    "                    sentence_weights[el_ind][\"phrasal_verb\"] = \"yes\"\n",
    "                    sentence_weights[el_ind - searh_ind][\"phrasal_verb\"] = \"yes\"\n",
    "    return weights_list\n",
    "        #print(\"\\n\")\n",
    "test_weights = get_phrasal_verbs(grammar_map_example,test_weights,debug = True)\n",
    "test_weights[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПРИСВАИВАЕМ ВЕСА ТФИДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weight': 0.05, 'word': 'this'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'my'},\n",
       "  {'weight': 0.774119582428778, 'word': 'friend'},\n",
       "  {'weight': 0.6330393922184421, 'word': 'jimmy'}],\n",
       " [{'weight': 0.05, 'word': 'he'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'from'},\n",
       "  {'weight': 1.0, 'word': 'india'}],\n",
       " [{'weight': 0.6330393922184421, 'word': 'jimmy'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.774119582428778, 'word': 'vegetarian'}],\n",
       " [{'weight': 0.05, 'word': 'the'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'good'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'breakfast'},\n",
       "  {'weight': 0.05, 'word': 'for'},\n",
       "  {'weight': 0.05, 'word': 'him'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'glass'},\n",
       "  {'weight': 0.05, 'word': 'of'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'orange'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'juice'},\n",
       "  {'weight': 0.05, 'word': 'two'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'apple'},\n",
       "  {'weight': 0.05, 'word': 'and'},\n",
       "  {'weight': 0.05, 'word': 'three'},\n",
       "  {'weight': 0.3779644730092272, 'word': 'banana'}],\n",
       " [{'weight': 0.5206541401738592, 'word': 'jimmy'},\n",
       "  {'weight': 0.6366879700309754, 'word': 'like'},\n",
       "  {'weight': 0.5688125307495037, 'word': 'watermelon'},\n",
       "  {'weight': 0.05, 'word': 'very'},\n",
       "  {'weight': 0.05, 'word': 'much'}],\n",
       " [{'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.4261771001650547, 'word': 'watermelon'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.4261771001650547, 'word': 'green'},\n",
       "  {'weight': 0.4770320942477113, 'word': 'outside'},\n",
       "  {'weight': 0.05, 'word': 'and'},\n",
       "  {'weight': 0.4770320942477113, 'word': 'red'},\n",
       "  {'weight': 0.4261771001650547, 'word': 'inside'}],\n",
       " [{'weight': 0.05, 'word': 'it'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.5978117297300675, 'word': 'hard'},\n",
       "  {'weight': 0.05, 'word': 'and'},\n",
       "  {'weight': 0.5978117297300675, 'word': 'soft'},\n",
       "  {'weight': 0.5340807725375345, 'word': 'inside'}],\n",
       " [{'weight': 0.05, 'word': 'it'},\n",
       "  {'weight': 0.05, 'word': 'be'},\n",
       "  {'weight': 0.774119582428778, 'word': 'juicy'},\n",
       "  {'weight': 0.05, 'word': 'and'},\n",
       "  {'weight': 0.6330393922184421, 'word': 'sweet'}],\n",
       " [{'weight': 0.05, 'word': 'what'},\n",
       "  {'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.7457391260354046, 'word': 'lovely'},\n",
       "  {'weight': 0.6662380624821364, 'word': 'fruit'}],\n",
       " [{'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.4878326589920418, 'word': 'month'},\n",
       "  {'weight': 0.4878326589920418, 'word': 'ago'},\n",
       "  {'weight': 0.3100215665365859, 'word': 'sally'},\n",
       "  {'weight': 0.4878326589920418, 'word': 'receive'},\n",
       "  {'weight': 0.05, 'word': 'a'},\n",
       "  {'weight': 0.43582624834268974, 'word': 'letter'}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_tf_idf(text_grammar_map, weights_list, tf_idf_dict):\n",
    "    assert (len(text_grammar_map) == len(weights_list))\n",
    "    for sentence_ind in range(len(text_grammar_map)):\n",
    "        for el_ind in range(len(text_grammar_map[sentence_ind])):\n",
    "            lemma = weights_list[sentence_ind][el_ind][\"word\"]\n",
    "            \n",
    "            if (lemma in tf_idf_dict):\n",
    "                weights_list[sentence_ind][el_ind][\"weight\"] = tf_idf_dict[lemma][sentence_ind]\n",
    "                #print(lemma, tf_idf_dict[lemma][sentence_ind])\n",
    "            else:\n",
    "                weights_list[sentence_ind][el_ind][\"weight\"] = 0.05\n",
    "                #print(lemma, \"not found\")\n",
    "                \n",
    "    return weights_list\n",
    "test_weights = assign_tf_idf(grammar_map_example, test_weights, tf_idf_dict_example)\n",
    "test_weights[:10]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЕЛИМ НА СЛОЖНЫЕ И НЕ СЛОЖНЫЕ В СООТВЕТСВИИ С ПОЛУЧЕННЫМИ ФИЧАМИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_groups(text_weights):\n",
    "    difficult_vocabulary = []\n",
    "    easy_vocabulary = []\n",
    "    for sentence_weights in text_weights:\n",
    "        for word_weight in sentence_weights:\n",
    "            #print(word_weight)\n",
    "            if('diff_grammar' in word_weight or 'phrasal_verb' in word_weight or word_weight['word'] not in basic_vocabulary):\n",
    "                difficult_vocabulary.append(word_weight)\n",
    "            else:\n",
    "                easy_vocabulary.append(word_weight)\n",
    "    return easy_vocabulary, difficult_vocabulary\n",
    "                \n",
    "easy, difficult = split_into_groups(test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 62)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(easy), len(difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weight': 0.774119582428778, 'word': 'vegetarian'},\n",
       " {'weight': 0.5688125307495037, 'word': 'watermelon'},\n",
       " {'weight': 0.4261771001650547, 'word': 'watermelon'},\n",
       " {'weight': 0.5978117297300675, 'word': 'soft'},\n",
       " {'weight': 0.774119582428778, 'word': 'juicy'},\n",
       " {'weight': 0.7457391260354046, 'word': 'lovely'},\n",
       " {'weight': 0.4878326589920418, 'word': 'receive'},\n",
       " {'weight': 0.774119582428778, 'word': 'invitation'},\n",
       " {'weight': 0.6330393922184421, 'word': 'wedding'},\n",
       " {'weight': 0.6230307846633135, 'word': 'marry'},\n",
       " {'weight': 0.05, 'word': 'such'},\n",
       " {'weight': 0.6020488713383773, 'word': 'ceremony'},\n",
       " {'weight': 0.23674868832917667, 'word': 'guest'},\n",
       " {'weight': 0.23674868832917667, 'word': 'clothe'},\n",
       " {'weight': 0.211509604231399, 'word': 'church'},\n",
       " {'weight': 0.23674868832917667, 'word': 'throw'},\n",
       " {'weight': 0.23674868832917667, 'word': 'confetti'},\n",
       " {'weight': 0.23674868832917667, 'word': 'tasty'},\n",
       " {'weight': 0.23674868832917667, 'word': 'video'},\n",
       " {'weight': 0.5144649816247601, 'word': 'bridesmaid'},\n",
       " {'weight': 0.3252464022868978, 'word': 'mean'},\n",
       " {'weight': 0.5319430989691459, 'word': 'wedding'},\n",
       " {'weight': 0.05, 'word': 'must'},\n",
       " {'weight': 0.5319430989691459, 'word': 'wedding'},\n",
       " {'weight': 0.3252464022868978, 'word': 'brush'},\n",
       " {'weight': 0.05, 'word': 'must'},\n",
       " {'weight': 0.35855816508414967, 'word': 'accompany'},\n",
       " {'weight': 0.35855816508414967, 'word': 'way'},\n",
       " {'weight': 0.3203333295153842, 'word': 'church'},\n",
       " {'weight': 0.35855816508414967, 'word': 'traditional'},\n",
       " {'weight': 0.35855816508414967, 'word': 'bunch'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.371014856687852, 'word': 'agree'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.05, 'word': 'with'},\n",
       " {'weight': 0.4152874333564036, 'word': 'pleasure'},\n",
       " {'weight': 0.371014856687852, 'word': 'ceremony'},\n",
       " {'weight': 0.4227167279649903, 'word': 'silk'},\n",
       " {'weight': 0.5773502691896257, 'word': 'fit'},\n",
       " {'weight': 1.0, 'word': 'pity'},\n",
       " {'weight': 0.8439884142349315, 'word': 'diet'},\n",
       " {'weight': 0.5054857517413035, 'word': 'fry'},\n",
       " {'weight': 0.05, 'word': 'even'},\n",
       " {'weight': 0.447213595499958, 'word': 'boil'},\n",
       " {'weight': 0.05, 'word': 'everything'},\n",
       " {'weight': 0.423477658242441, 'word': 'notebook'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.423477658242441, 'word': 'add'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.05, 'word': 'up'},\n",
       " {'weight': 0.423477658242441, 'word': 'calorie'},\n",
       " {'weight': 0.5, 'word': 'practice'},\n",
       " {'weight': 0.5, 'word': 'jog'},\n",
       " {'weight': 0.5, 'word': 'gym'},\n",
       " {'weight': 0.6742707057335887, 'word': 'lose'},\n",
       " {'weight': 0.6742707057335887, 'word': 'kilo'},\n",
       " {'weight': 0.6742707057335887, 'word': 'lose'},\n",
       " {'weight': 0.6742707057335887, 'word': 'kilo'},\n",
       " {'weight': 0.05, 'word': 'everyone'},\n",
       " {'weight': 0.45841244773177797, 'word': 'agree'},\n",
       " {'weight': 0.4917349117096412, 'word': 'dream'},\n",
       " {'weight': 0.4917349117096412, 'word': 'slice'},\n",
       " {'weight': 0.40211819556948125, 'word': 'wedding'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.05, 'word': 'give'},\n",
       " {'phrasal_verb': 'yes', 'weight': 0.05, 'word': 'up'},\n",
       " {'weight': 0.7071067811865476, 'word': 'dieting'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy words: 252 difficult words: 62\n",
      "easy tfidf sum: 47.279379344440464 easy tfidf sum: 26.094957600305506\n",
      "relative tfidf 0.64 0.36\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_level(easy_words_weights, difficults_words_weights, debug = False):\n",
    "    tf_idf_easy = 0\n",
    "    for easy_w_w in easy_words_weights:\n",
    "        tf_idf_easy += easy_w_w['weight']\n",
    "        \n",
    "    tf_idf_diff = 0\n",
    "    for diff_w_w in difficults_words_weights:\n",
    "        tf_idf_diff += diff_w_w['weight']\n",
    "        \n",
    "    if(debug):print(\"easy words:\", len(easy_words_weights), \"difficult words:\", len(difficults_words_weights))\n",
    "    if(debug):print(\"easy tfidf sum:\",tf_idf_easy, \"easy tfidf sum:\",tf_idf_diff)\n",
    "    overall_weights = tf_idf_easy + tf_idf_diff\n",
    "    if(debug):print(\"relative tfidf\", round(tf_idf_easy/overall_weights, 2), round(tf_idf_diff/overall_weights, 2))\n",
    "        \n",
    "    if(round(tf_idf_diff/overall_weights, 2) >= 0.3):\n",
    "        print(\"text is very diffcult, diff realtive weight is more then 0.3\")\n",
    "        easy_text = False\n",
    "    if(round(tf_idf_diff/overall_weights, 2) > 0.2 and round(tf_idf_diff/overall_weights, 2) < 0.3):\n",
    "        print(\"text is moderate\")\n",
    "        easy_text = False\n",
    "    if(round(tf_idf_diff/overall_weights, 2) <= 0.2):\n",
    "        print(\"text is quite easy\")\n",
    "        easy_text = True\n",
    "        \n",
    "    \n",
    "        \n",
    "    return easy_text\n",
    "calculate_level(easy, difficult,debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_level(raw_text, print_debug_message = False, show_calucated_weights = False):\n",
    "    processed_text = punct_setnence_lower(raw_text,sent_detector)\n",
    "    \n",
    "    grammar_map = get_sent_gramm_features_map(processed_text)\n",
    "    \n",
    "    text_lemm = get_lemm_text(grammar_map)\n",
    "    \n",
    "    tfidf_dict = get_tf_idf_dict(text_lemm)\n",
    "    \n",
    "    weights = get_weights_empty_list(grammar_map)\n",
    "    \n",
    "    weights = get_difficult_grammar(grammar_map,weights, debug = print_debug_message)\n",
    "    weights = get_phrasal_verbs(grammar_map,weights, debug = print_debug_message)\n",
    "    weights = assign_tf_idf(grammar_map, weights, tfidf_dict)\n",
    "    \n",
    "    if(show_calucated_weights): \n",
    "        print(\"WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\")\n",
    "        for sent_weights in weights:\n",
    "            for word_weight in sent_weights:\n",
    "                print(word_weight)\n",
    "            print(\"\\n\")\n",
    "    easy, difficult = split_into_groups(weights)\n",
    "    \n",
    "    easy_text = calculate_level(easy, difficult,  debug = print_debug_message)\n",
    "    \n",
    "    return easy_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВЫВОДИМ РЕЗУЛЬТАТЫ ПО ВСЕМ ТЕКСТАМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "Jimmy’s Breakfast level is calculated\n",
      "=====================================\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "A Girl from Green Valley level is calculated\n",
      "=====================================\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "Alice in Wonderland level is calculated\n",
      "=====================================\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "Volkswagen level is calculated\n",
      "=====================================\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "My Friends level is calculated\n",
      "=====================================\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "Good Advice From A Doctor level is calculated\n",
      "=====================================\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "About The Weather level is calculated\n",
      "=====================================\n",
      "text is quite easy\n",
      "\n",
      "\n",
      "=====================================\n",
      "A Lucky Puppy level is calculated\n",
      "=====================================\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "Anne of Green Gables level is calculated\n",
      "=====================================\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "We Like Sports! level is calculated\n",
      "=====================================\n",
      "text is quite easy\n",
      "\n",
      "\n",
      "=====================================\n",
      "James Bond 1 level is calculated\n",
      "=====================================\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "Charlotte’s Web level is calculated\n",
      "=====================================\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "easy_texts_indexes = []\n",
    "for text_ind in range(len(texts['Text'])):\n",
    "    print(\"=====================================\")\n",
    "    print(texts[\"Title\"][text_ind], \"level is calculated\")\n",
    "    print(\"=====================================\")\n",
    "    easy_text = calculate_text_level(texts['Text'][text_ind])\n",
    "    if(easy_text):\n",
    "        easy_texts_indexes.append(text_ind) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_texts_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "Jimmy’s Breakfast level is calculated\n",
      "=====================================\n",
      "Phrasal Verb found: agree with\n",
      "Phrasal Verb found: add up\n",
      "Phrasal Verb found: give up\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'this', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'friend', 'weight': 0.774119582428778}\n",
      "{'word': 'jimmy', 'weight': 0.6330393922184421}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'india', 'weight': 1.0}\n",
      "\n",
      "\n",
      "{'word': 'jimmy', 'weight': 0.6330393922184421}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'vegetarian', 'weight': 0.774119582428778}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.3779644730092272}\n",
      "{'word': 'breakfast', 'weight': 0.3779644730092272}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'glass', 'weight': 0.3779644730092272}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'orange', 'weight': 0.3779644730092272}\n",
      "{'word': 'juice', 'weight': 0.3779644730092272}\n",
      "{'word': 'two', 'weight': 0.05}\n",
      "{'word': 'apple', 'weight': 0.3779644730092272}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'three', 'weight': 0.05}\n",
      "{'word': 'banana', 'weight': 0.3779644730092272}\n",
      "\n",
      "\n",
      "{'word': 'jimmy', 'weight': 0.5206541401738592}\n",
      "{'word': 'like', 'weight': 0.6366879700309754}\n",
      "{'word': 'watermelon', 'weight': 0.5688125307495037}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'watermelon', 'weight': 0.4261771001650547}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'green', 'weight': 0.4261771001650547}\n",
      "{'word': 'outside', 'weight': 0.4770320942477113}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'red', 'weight': 0.4770320942477113}\n",
      "{'word': 'inside', 'weight': 0.4261771001650547}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'hard', 'weight': 0.5978117297300675}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'soft', 'weight': 0.5978117297300675}\n",
      "{'word': 'inside', 'weight': 0.5340807725375345}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'juicy', 'weight': 0.774119582428778}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'sweet', 'weight': 0.6330393922184421}\n",
      "\n",
      "\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'lovely', 'weight': 0.7457391260354046}\n",
      "{'word': 'fruit', 'weight': 0.6662380624821364}\n",
      "\n",
      "\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'month', 'weight': 0.4878326589920418}\n",
      "{'word': 'ago', 'weight': 0.4878326589920418}\n",
      "{'word': 'sally', 'weight': 0.3100215665365859}\n",
      "{'word': 'receive', 'weight': 0.4878326589920418}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'letter', 'weight': 0.43582624834268974}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'an', 'weight': 0.05}\n",
      "{'word': 'invitation', 'weight': 0.774119582428778}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'wedding', 'weight': 0.6330393922184421}\n",
      "\n",
      "\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'sister', 'weight': 0.6230307846633135}\n",
      "{'word': 'victoria', 'weight': 0.47293264078897274}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'get', 'weight': 0.05}\n",
      "{'word': 'marry', 'weight': 0.6230307846633135}\n",
      "\n",
      "\n",
      "{'word': 'last', 'weight': 0.05}\n",
      "{'word': 'year', 'weight': 0.6738903470477137}\n",
      "{'word': 'sally', 'weight': 0.42826271922278975}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "{'word': 'such', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'ceremony', 'weight': 0.6020488713383773}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'take', 'weight': 0.05}\n",
      "{'word': 'place', 'weight': 0.7071067811865476}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'saturday', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'lot', 'weight': 0.23674868832917667}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'guest', 'weight': 0.23674868832917667}\n",
      "{'word': 'people', 'weight': 0.23674868832917667}\n",
      "{'word': 'wear', 'weight': 0.23674868832917667}\n",
      "{'word': 'nice', 'weight': 0.211509604231399}\n",
      "{'word': 'clothe', 'weight': 0.23674868832917667}\n",
      "{'word': 'meet', 'weight': 0.23674868832917667}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'husband', 'weight': 0.23674868832917667}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'wife', 'weight': 0.23674868832917667}\n",
      "{'word': 'near', 'weight': 0.23674868832917667}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'church', 'weight': 0.211509604231399}\n",
      "{'word': 'throw', 'weight': 0.23674868832917667}\n",
      "{'word': 'confetti', 'weight': 0.23674868832917667}\n",
      "{'word': 'eat', 'weight': 0.1797121187765218}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'tasty', 'weight': 0.23674868832917667}\n",
      "{'word': 'cake', 'weight': 0.1936021890806568}\n",
      "{'word': 'make', 'weight': 0.23674868832917667}\n",
      "{'word': 'video', 'weight': 0.23674868832917667}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'take', 'weight': 0.05}\n",
      "{'word': 'photo', 'weight': 0.23674868832917667}\n",
      "\n",
      "\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'letter', 'weight': 0.4596193770800157}\n",
      "{'word': 'victoria', 'weight': 0.3905220870983632}\n",
      "{'word': 'ask', 'weight': 0.5144649816247601}\n",
      "{'word': 'sally', 'weight': 0.32694662112428574}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'bridesmaid', 'weight': 0.5144649816247601}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'mean', 'weight': 0.3252464022868978}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'day', 'weight': 0.3252464022868978}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'wedding', 'weight': 0.5319430989691459}\n",
      "{'word': 'sally', 'weight': 0.20669669668224816}\n",
      "{'word': 'must', 'weight': 0.05}\n",
      "{'word': 'help', 'weight': 0.3252464022868978}\n",
      "{'word': 'victoria', 'weight': 0.2468893090471917}\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'put', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'wedding', 'weight': 0.5319430989691459}\n",
      "{'word': 'dress', 'weight': 0.29057283616177143}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'brush', 'weight': 0.3252464022868978}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'hair', 'weight': 0.3252464022868978}\n",
      "\n",
      "\n",
      "{'word': 'sally', 'weight': 0.22786658905443488}\n",
      "{'word': 'must', 'weight': 0.05}\n",
      "{'word': 'also', 'weight': 0.05}\n",
      "{'word': 'accompany', 'weight': 0.35855816508414967}\n",
      "{'word': 'victoria', 'weight': 0.2721757320247558}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'way', 'weight': 0.35855816508414967}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'church', 'weight': 0.3203333295153842}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'carry', 'weight': 0.35855816508414967}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'traditional', 'weight': 0.35855816508414967}\n",
      "{'word': 'bunch', 'weight': 0.35855816508414967}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'flower', 'weight': 0.35855816508414967}\n",
      "\n",
      "\n",
      "{'word': 'sally', 'weight': 0.26391849393217964}\n",
      "{'word': 'agree', 'weight': 0.371014856687852, 'phrasal_verb': 'yes'}\n",
      "{'word': 'with', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'pleasure', 'weight': 0.4152874333564036}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'buy', 'weight': 0.4152874333564036}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'beautiful', 'weight': 0.4152874333564036}\n",
      "{'word': 'dress', 'weight': 0.371014856687852}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'ceremony', 'weight': 0.371014856687852}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'white', 'weight': 0.4227167279649903}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'green', 'weight': 0.37765213596264063}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'long', 'weight': 0.4227167279649903}\n",
      "{'word': 'nice', 'weight': 0.37765213596264063}\n",
      "{'word': 'silk', 'weight': 0.4227167279649903}\n",
      "{'word': 'skirt', 'weight': 0.4227167279649903}\n",
      "\n",
      "\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'home', 'weight': 0.5773502691896257}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'understand', 'weight': 0.5773502691896257}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'could', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'fit', 'weight': 0.5773502691896257}\n",
      "{'word': 'into', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'pity', 'weight': 1.0}\n",
      "\n",
      "\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'why', 'weight': 0.05}\n",
      "{'word': 'sally', 'weight': 0.5363614048728764}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'diet', 'weight': 0.8439884142349315}\n",
      "{'word': 'now', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'eat', 'weight': 0.38370609821696433}\n",
      "{'word': 'cake', 'weight': 0.41336300013678773}\n",
      "{'word': 'sweet', 'weight': 0.41336300013678773}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'fry', 'weight': 0.5054857517413035}\n",
      "{'word': 'potato', 'weight': 0.5054857517413035}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'even', 'weight': 0.05}\n",
      "{'word': 'eat', 'weight': 0.5310641670219329}\n",
      "{'word': 'some', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'sweet', 'weight': 0.5721104730037487}\n",
      "{'word': 'fruit', 'weight': 0.6250283651041187}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'cook', 'weight': 0.447213595499958}\n",
      "{'word': 'vegetable', 'weight': 0.447213595499958}\n",
      "{'word': 'fish', 'weight': 0.447213595499958}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'boil', 'weight': 0.447213595499958}\n",
      "{'word': 'meat', 'weight': 0.447213595499958}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'write', 'weight': 0.423477658242441}\n",
      "{'word': 'down', 'weight': 0.05}\n",
      "{'word': 'everything', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'eat', 'weight': 0.3214550743844179}\n",
      "{'word': 'into', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'special', 'weight': 0.423477658242441}\n",
      "{'word': 'notebook', 'weight': 0.423477658242441}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'add', 'weight': 0.423477658242441, 'phrasal_verb': 'yes'}\n",
      "{'word': 'up', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'calorie', 'weight': 0.423477658242441}\n",
      "\n",
      "\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'more', 'weight': 0.05}\n",
      "{'word': 'every', 'weight': 0.05}\n",
      "{'word': 'morning', 'weight': 0.5}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'practice', 'weight': 0.5}\n",
      "{'word': 'jog', 'weight': 0.5}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'gym', 'weight': 0.5}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'lose', 'weight': 0.6742707057335887}\n",
      "{'word': 'three', 'weight': 0.05}\n",
      "{'word': 'kilo', 'weight': 0.6742707057335887}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'want', 'weight': 0.3011943405495137}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'lose', 'weight': 0.6742707057335887}\n",
      "{'word': 'two', 'weight': 0.05}\n",
      "{'word': 'kilo', 'weight': 0.6742707057335887}\n",
      "{'word': 'more', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'everyone', 'weight': 0.05}\n",
      "{'word': 'tell', 'weight': 0.5131140314343912}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'look', 'weight': 0.5131140314343912}\n",
      "{'word': 'great', 'weight': 0.5131140314343912}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'agree', 'weight': 0.45841244773177797}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'dream', 'weight': 0.4917349117096412}\n",
      "{'word': 'about', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'slice', 'weight': 0.4917349117096412}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'wedding', 'weight': 0.40211819556948125}\n",
      "{'word': 'cake', 'weight': 0.40211819556948125}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'want', 'weight': 0.43931249332987526}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'give', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'up', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "\n",
      "\n",
      "{'word': 'dieting', 'weight': 0.7071067811865476}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'easy', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "easy words: 252 difficult words: 62\n",
      "easy tfidf sum: 47.279379344440464 easy tfidf sum: 26.094957600305506\n",
      "relative tfidf 0.64 0.36\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "A Girl from Green Valley level is calculated\n",
      "=====================================\n",
      "Phrasal Verb found: ask about\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'green', 'weight': 0.3473239689847857}\n",
      "{'word': 'valley', 'weight': 0.3473239689847857}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'small', 'weight': 0.3895464339811909}\n",
      "{'word': 'village', 'weight': 0.3895464339811909}\n",
      "{'word': 'near', 'weight': 0.3895464339811909}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'icy', 'weight': 0.3895464339811909}\n",
      "{'word': 'mountain', 'weight': 0.3895464339811909}\n",
      "\n",
      "\n",
      "{'word': 'mary', 'weight': 0.3337403464424031}\n",
      "{'word': 'live', 'weight': 0.5222614606219633}\n",
      "{'word': 'happily', 'weight': 0.5857502152407045}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'grandmother', 'weight': 0.5222614606219633}\n",
      "\n",
      "\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'day', 'weight': 0.326470070039905}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'grandma', 'weight': 0.3661574292075577}\n",
      "{'word': 'fall', 'weight': 0.3661574292075577}\n",
      "{'word': 'ill', 'weight': 0.3661574292075577}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'could', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'work', 'weight': 0.29831144058653475}\n",
      "{'word': 'anymore', 'weight': 0.3661574292075577}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'earn', 'weight': 0.3661574292075577}\n",
      "{'word': 'enough', 'weight': 0.05}\n",
      "{'word': 'money', 'weight': 0.3661574292075577}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'ask', 'weight': 0.5436537257105034}\n",
      "{'word': 'mary', 'weight': 0.34741062943310025}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'read', 'weight': 0.6097430326796476}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'letter', 'weight': 0.4603910459344447}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'granny', 'weight': 0.5175262614934703}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'letter', 'weight': 0.5175262614934703}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'mary', 'weight': 0.3905248067730812}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'uncle', 'weight': 0.5584115978595288}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'rich', 'weight': 0.46676302208846976}\n",
      "{'word': 'man', 'weight': 0.46676302208846976}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'live', 'weight': 0.41617114486261303}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'big', 'weight': 0.46676302208846976}\n",
      "{'word': 'city', 'weight': 0.41617114486261303}\n",
      "\n",
      "\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'ask', 'weight': 0.625431890371152, 'phrasal_verb': 'yes'}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'about', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'job', 'weight': 0.625431890371152}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'circus', 'weight': 0.4665510701022288}\n",
      "\n",
      "\n",
      "{'word': 'mary', 'weight': 0.4950493439759583}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'surprised', 'weight': 0.8688648612005053}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'want', 'weight': 0.7880822658760916}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'leave', 'weight': 0.23422099531941273}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'sweet', 'weight': 0.26269408862536386}\n",
      "{'word': 'home', 'weight': 0.23422099531941273}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'want', 'weight': 0.7880822658760916}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'part', 'weight': 0.05}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'beloved', 'weight': 0.26269408862536386}\n",
      "{'word': 'grandmother', 'weight': 0.23422099531941273}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'want', 'weight': 0.7880822658760916}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'work', 'weight': 0.21401901411913682}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'circus', 'weight': 0.1747209531350138}\n",
      "\n",
      "\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'granny', 'weight': 0.33004178111619376}\n",
      "{'word': 'say', 'weight': 0.35611556758810764}\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'child', 'weight': 0.4371081460116737}\n",
      "{'word': 'already', 'weight': 0.05}\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'grown', 'weight': 0.4371081460116737}\n",
      "{'word': 'up', 'weight': 0.05}\n",
      "{'word': 'young', 'weight': 0.4371081460116737}\n",
      "{'word': 'lady', 'weight': 0.4371081460116737}\n",
      "\n",
      "\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'must', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'city', 'weight': 0.48055304503076873}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'city', 'weight': 0.48055304503076873}\n",
      "{'word': 'everything', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'different', 'weight': 0.24027652251538437}\n",
      "{'word': 'practically', 'weight': 0.2694857564505413}\n",
      "{'word': 'no', 'weight': 0.05}\n",
      "{'word': 'tree', 'weight': 0.2694857564505413}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'flower', 'weight': 0.2694857564505413}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'street', 'weight': 0.24027652251538437}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'lot', 'weight': 0.2694857564505413}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'car', 'weight': 0.2694857564505413}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'high', 'weight': 0.2694857564505413}\n",
      "{'word': 'stone', 'weight': 0.2694857564505413}\n",
      "{'word': 'building', 'weight': 0.2694857564505413}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'noise', 'weight': 0.2694857564505413}\n",
      "\n",
      "\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'people', 'weight': 0.42578964696644145}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'kind', 'weight': 0.4775507981401452}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'friendly', 'weight': 0.4775507981401452}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'green', 'weight': 0.42578964696644145}\n",
      "{'word': 'valley', 'weight': 0.42578964696644145}\n",
      "\n",
      "\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'explain', 'weight': 0.2770060263688982}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'way', 'weight': 0.2770060263688982}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'circus', 'weight': 0.1842399926263267}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'easily', 'weight': 0.2770060263688982}\n",
      "{'word': 'find', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'place', 'weight': 0.2770060263688982}\n",
      "{'word': 'turn', 'weight': 0.5540120527377964}\n",
      "{'word': 'right', 'weight': 0.2770060263688982}\n",
      "{'word': 'follow', 'weight': 0.2770060263688982}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'wide', 'weight': 0.2770060263688982}\n",
      "{'word': 'street', 'weight': 0.24698167950831615}\n",
      "{'word': 'turn', 'weight': 0.5540120527377964}\n",
      "{'word': 'leave', 'weight': 0.24698167950831615}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'here', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'mary', 'weight': 0.4040604255091891}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'uncle', 'weight': 0.5777661852131544}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'glad', 'weight': 0.7091695197634851}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'director', 'weight': 0.8015492907601451}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'circus', 'weight': 0.5979287035106347}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'need', 'weight': 0.5377028053322417}\n",
      "{'word': 'more', 'weight': 0.05}\n",
      "{'word': 'people', 'weight': 0.4794218511348644}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'care', 'weight': 0.5377028053322417}\n",
      "{'word': 'about', 'weight': 0.05}\n",
      "{'word': 'animal', 'weight': 0.43807085606108814}\n",
      "\n",
      "\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'mary', 'weight': 0.23283137257668637}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'duty', 'weight': 0.4086441092764891}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'clean', 'weight': 0.4086441092764891}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'cage', 'weight': 0.4086441092764891}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'feed', 'weight': 0.4086441092764891}\n",
      "{'word': 'animal', 'weight': 0.3329256849691543}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'wash', 'weight': 0.4086441092764891}\n",
      "{'word': 'them', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'many', 'weight': 0.05}\n",
      "{'word': 'different', 'weight': 0.3445606798937622}\n",
      "{'word': 'animal', 'weight': 0.314841702873468}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'circus', 'weight': 0.2570306317834047}\n",
      "{'word': 'monkey', 'weight': 0.3864472254393397}\n",
      "{'word': 'horse', 'weight': 0.3445606798937622}\n",
      "{'word': 'elephant', 'weight': 0.3864472254393397}\n",
      "{'word': 'tiger', 'weight': 0.3864472254393397}\n",
      "{'word': 'lion', 'weight': 0.3864472254393397}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'job', 'weight': 0.5333188125890629}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'hard', 'weight': 0.598151755049917}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'difficult', 'weight': 0.598151755049917}\n",
      "\n",
      "\n",
      "{'word': 'mary', 'weight': 0.4950493439759583}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'complain', 'weight': 0.8688648612005053}\n",
      "\n",
      "\n",
      "{'word': 'every', 'weight': 0.05}\n",
      "{'word': 'week', 'weight': 0.5971907691791992}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'write', 'weight': 0.4865361848437147}\n",
      "{'word': 'letter', 'weight': 0.4509133653180392}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'granny', 'weight': 0.4509133653180392}\n",
      "\n",
      "\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'every', 'weight': 0.05}\n",
      "{'word': 'letter', 'weight': 0.3245123189720101}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'write', 'weight': 0.35014926979613303}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'fine', 'weight': 0.42978491275886643}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'happy', 'weight': 0.7664020958096976}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'happy', 'weight': 0.7664020958096976}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'all', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'evening', 'weight': 0.24706511847304882}\n",
      "{'word': 'stand', 'weight': 0.2770996085977945}\n",
      "{'word': 'behind', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'curtain', 'weight': 0.2770996085977945}\n",
      "{'word': 'during', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'performance', 'weight': 0.49413023694609765}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'dream', 'weight': 0.2770996085977945}\n",
      "{'word': 'about', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'stage', 'weight': 0.2770996085977945}\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'day', 'weight': 0.24706511847304882}\n",
      "{'word': 'director', 'weight': 0.24706511847304882}\n",
      "{'word': 'say', 'weight': 0.22575530859955797}\n",
      "{'word': 'today', 'weight': 0.2770996085977945}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'king', 'weight': 0.24706511847304882}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'visit', 'weight': 0.2770996085977945}\n",
      "{'word': 'our', 'weight': 0.05}\n",
      "{'word': 'performance', 'weight': 0.49413023694609765}\n",
      "\n",
      "\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'should', 'weight': 0.05}\n",
      "{'word': 'act', 'weight': 0.3438330956752283}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'well', 'weight': 0.05}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'unfortunately', 'weight': 0.3438330956752283}\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'most', 'weight': 0.05}\n",
      "{'word': 'popular', 'weight': 0.3438330956752283}\n",
      "{'word': 'gymnast', 'weight': 0.3438330956752283}\n",
      "{'word': 'break', 'weight': 0.3438330956752283}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'leg', 'weight': 0.3438330956752283}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'no', 'weight': 0.05}\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'else', 'weight': 0.05}\n",
      "{'word': 'could', 'weight': 0.05}\n",
      "{'word': 'work', 'weight': 0.28012362418587816}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'horse', 'weight': 0.30656544391319485}\n",
      "{'word': 'instead', 'weight': 0.3438330956752283}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'suddenly', 'weight': 0.7091695197634851}\n",
      "{'word': 'mary', 'weight': 0.4040604255091891}\n",
      "{'word': 'say', 'weight': 0.5777661852131544}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'know', 'weight': 0.3934837249053176}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'program', 'weight': 0.3934837249053176}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'try', 'weight': 0.3934837249053176}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'uncle', 'weight': 0.32057439631335005}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'no', 'weight': 0.05}\n",
      "{'word': 'choice', 'weight': 0.3934837249053176}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'let', 'weight': 0.3934837249053176}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'take', 'weight': 0.05}\n",
      "{'word': 'part', 'weight': 0.05}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'performance', 'weight': 0.3508345017262598}\n",
      "\n",
      "\n",
      "{'word': 'mary', 'weight': 0.3913532524709653}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'beautiful', 'weight': 0.6868670639124512}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'evening', 'weight': 0.6124183768410983}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'look', 'weight': 0.4286529501838809}\n",
      "{'word': 'nice', 'weight': 0.4286529501838809}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'circus', 'weight': 0.285102159748531}\n",
      "{'word': 'clothe', 'weight': 0.4286529501838809}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'all', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'trick', 'weight': 0.4286529501838809}\n",
      "{'word': 'perfectly', 'weight': 0.4286529501838809}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'king', 'weight': 0.41617114486261303}\n",
      "{'word': 'like', 'weight': 0.46676302208846976}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'applaud', 'weight': 0.46676302208846976}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'long', 'weight': 0.46676302208846976}\n",
      "{'word': 'time', 'weight': 0.41617114486261303}\n",
      "\n",
      "\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'come', 'weight': 0.388697065103494}\n",
      "{'word': 'home', 'weight': 0.34656666216844845}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'write', 'weight': 0.31667466557684704}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'granny', 'weight': 0.29348863170801714}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'really', 'weight': 0.388697065103494}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'happy', 'weight': 0.34656666216844845}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'this', 'weight': 0.05}\n",
      "{'word': 'time', 'weight': 0.34656666216844845}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'true', 'weight': 0.388697065103494}\n",
      "\n",
      "\n",
      "easy words: 325 difficult words: 83\n",
      "easy tfidf sum: 52.43970643268409 easy tfidf sum: 30.709243286499003\n",
      "relative tfidf 0.63 0.37\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "Alice in Wonderland level is calculated\n",
      "=====================================\n",
      "PAST CONTINIOUS\n",
      "('begin', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "('peep', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "PAST CONTINIOUS\n",
      "('read', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "PAST CONTINIOUS\n",
      "('consider', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'alice', 'weight': 0.3090813110314737}\n",
      "{'word': 'be', 'weight': 0.05, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'begin', 'weight': 0.19154915915631943, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'get', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'tired', 'weight': 0.19154915915631943}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'sit', 'weight': 0.19154915915631943}\n",
      "{'word': 'by', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'sister', 'weight': 0.38309831831263885}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'bank', 'weight': 0.19154915915631943}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'nothing', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'once', 'weight': 0.05}\n",
      "{'word': 'or', 'weight': 0.05}\n",
      "{'word': 'twice', 'weight': 0.19154915915631943}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'peep', 'weight': 0.19154915915631943, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'into', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'book', 'weight': 0.38309831831263885}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'sister', 'weight': 0.38309831831263885}\n",
      "{'word': 'be', 'weight': 0.05, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'read', 'weight': 0.19154915915631943, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'no', 'weight': 0.05}\n",
      "{'word': 'picture', 'weight': 0.38309831831263885}\n",
      "{'word': 'or', 'weight': 0.05}\n",
      "{'word': 'conversation', 'weight': 0.38309831831263885}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'use', 'weight': 0.19154915915631943}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'book', 'weight': 0.38309831831263885}\n",
      "{'word': 'think', 'weight': 0.15454065551573684}\n",
      "{'word': 'alice', 'weight': 0.3090813110314737}\n",
      "{'word': 'without', 'weight': 0.05}\n",
      "{'word': 'picture', 'weight': 0.38309831831263885}\n",
      "{'word': 'or', 'weight': 0.05}\n",
      "{'word': 'conversation', 'weight': 0.38309831831263885}\n",
      "\n",
      "\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'consider', 'weight': 0.193706381806703, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'own', 'weight': 0.05}\n",
      "{'word': 'mind', 'weight': 0.193706381806703}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'well', 'weight': 0.05}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'could', 'weight': 0.05}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'hot', 'weight': 0.193706381806703}\n",
      "{'word': 'day', 'weight': 0.193706381806703}\n",
      "{'word': 'make', 'weight': 0.387412763613406}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'feel', 'weight': 0.193706381806703}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'sleepy', 'weight': 0.193706381806703}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'stupid', 'weight': 0.193706381806703}\n",
      "{'word': 'whether', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'pleasure', 'weight': 0.193706381806703}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'make', 'weight': 0.387412763613406}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'daisy', 'weight': 0.387412763613406}\n",
      "{'word': 'chain', 'weight': 0.193706381806703}\n",
      "{'word': 'would', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'worth', 'weight': 0.193706381806703}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'trouble', 'weight': 0.193706381806703}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'get', 'weight': 0.05}\n",
      "{'word': 'up', 'weight': 0.05}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'pick', 'weight': 0.193706381806703}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'daisy', 'weight': 0.387412763613406}\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'suddenly', 'weight': 0.193706381806703}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'white', 'weight': 0.193706381806703}\n",
      "{'word': 'rabbit', 'weight': 0.156281089167088}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'pink', 'weight': 0.193706381806703}\n",
      "{'word': 'eye', 'weight': 0.193706381806703}\n",
      "{'word': 'run', 'weight': 0.193706381806703}\n",
      "{'word': 'close', 'weight': 0.193706381806703}\n",
      "{'word': 'by', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'nothing', 'weight': 0.05}\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'remarkable', 'weight': 0.371273413443256}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'nor', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'alice', 'weight': 0.29954105223851113}\n",
      "{'word': 'think', 'weight': 0.29954105223851113}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'way', 'weight': 0.371273413443256}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'hear', 'weight': 0.371273413443256}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'rabbit', 'weight': 0.29954105223851113}\n",
      "{'word': 'say', 'weight': 0.371273413443256}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'itself', 'weight': 0.05}\n",
      "{'word': 'oh', 'weight': 0.29954105223851113}\n",
      "{'word': 'dear', 'weight': 0.29954105223851113}\n",
      "\n",
      "\n",
      "{'word': 'oh', 'weight': 0.7071067811865476}\n",
      "{'word': 'dear', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'shall', 'weight': 0.7071067811865475}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'late', 'weight': 0.7071067811865475}\n",
      "\n",
      "\n",
      "easy words: 116 difficult words: 32\n",
      "easy tfidf sum: 15.765019949012538 easy tfidf sum: 5.258736204170965\n",
      "relative tfidf 0.75 0.25\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "Volkswagen level is calculated\n",
      "=====================================\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "('cheat', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "('instal', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "Phrasal Verb found: be up\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'volkswagen', 'weight': 0.23139185727138745}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'emission', 'weight': 0.30583566791537864}\n",
      "{'word': 'cheat', 'weight': 0.30583566791537864}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'cost', 'weight': 0.3649250446545867}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'company', 'weight': 0.30583566791537864}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'least', 'weight': 0.05}\n",
      "{'word': '15', 'weight': 0.3649250446545867}\n",
      "{'word': 'billion', 'weight': 0.3649250446545867}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'united', 'weight': 0.3649250446545867}\n",
      "{'word': 'state', 'weight': 0.3649250446545867}\n",
      "{'word': 'alone', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'tuesday', 'weight': 0.3145893585733343}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'u', 'weight': 0.05}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'department', 'weight': 0.3145893585733343}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'justice', 'weight': 0.3145893585733343}\n",
      "{'word': 'announce', 'weight': 0.3145893585733343}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'volkswagen', 'weight': 0.19947497992909644}\n",
      "{'word': 'would', 'weight': 0.05}\n",
      "{'word': 'make', 'weight': 0.3145893585733343}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'large', 'weight': 0.3145893585733343}\n",
      "{'word': 'ever', 'weight': 0.05}\n",
      "{'word': 'automobile', 'weight': 0.26365043454174164}\n",
      "{'word': 'buy', 'weight': 0.3145893585733343}\n",
      "{'word': 'back', 'weight': 0.05}\n",
      "{'word': 'offer', 'weight': 0.3145893585733343}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'u', 'weight': 0.05}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'history', 'weight': 0.3145893585733343}\n",
      "\n",
      "\n",
      "{'word': 'report', 'weight': 0.3974701178378703}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'volkswagen', 'weight': 0.25202805377042803}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'cheat', 'weight': 0.3331109792160477, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'emission', 'weight': 0.3331109792160477}\n",
      "{'word': 'test', 'weight': 0.28744744366221103}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'u', 'weight': 0.05}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'first', 'weight': 0.05}\n",
      "{'word': 'come', 'weight': 0.3974701178378703}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'late', 'weight': 0.3974701178378703}\n",
      "{'word': 'last', 'weight': 0.05}\n",
      "{'word': 'year', 'weight': 0.3974701178378703}\n",
      "\n",
      "\n",
      "{'word': 'investigator', 'weight': 0.33248445662948684}\n",
      "{'word': 'find', 'weight': 0.05}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'german', 'weight': 0.33248445662948684}\n",
      "{'word': 'automaker', 'weight': 0.33248445662948684}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'instal', 'weight': 0.33248445662948684, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'software', 'weight': 0.33248445662948684}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'its', 'weight': 0.05}\n",
      "{'word': 'vehicle', 'weight': 0.24045029506985324}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'produce', 'weight': 0.33248445662948684}\n",
      "{'word': 'false', 'weight': 0.33248445662948684}\n",
      "{'word': 'test', 'weight': 0.24045029506985324}\n",
      "{'word': 'result', 'weight': 0.33248445662948684}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'would', 'weight': 0.05}\n",
      "{'word': 'reduce', 'weight': 0.46114911435169365}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'car', 'weight': 0.46114911435169365}\n",
      "{'word': 'pollution', 'weight': 0.38647894810790673}\n",
      "{'word': 'level', 'weight': 0.46114911435169365}\n",
      "{'word': 'during', 'weight': 0.05}\n",
      "{'word': 'testing', 'weight': 0.46114911435169365}\n",
      "\n",
      "\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'vehicle', 'weight': 0.37353524880542033}\n",
      "{'word': 'would', 'weight': 0.05}\n",
      "{'word': 'wrongly', 'weight': 0.5165086788309016}\n",
      "{'word': 'pass', 'weight': 0.5165086788309016}\n",
      "{'word': 'u', 'weight': 0.05}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'pollution', 'weight': 0.4328745836665149}\n",
      "{'word': 'test', 'weight': 0.37353524880542033}\n",
      "\n",
      "\n",
      "{'word': 'owner', 'weight': 0.2835723137971876}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'about', 'weight': 0.05}\n",
      "{'word': '500', 'weight': 0.3383602703392845}\n",
      "{'word': '000', 'weight': 0.2835723137971876}\n",
      "{'word': 'volkswagen', 'weight': 0.21454765170958165}\n",
      "{'word': 'automobile', 'weight': 0.2835723137971876}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'u', 'weight': 0.05}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'now', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'able', 'weight': 0.3383602703392845}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'sell', 'weight': 0.3383602703392845}\n",
      "{'word': 'their', 'weight': 0.05}\n",
      "{'word': 'vehicle', 'weight': 0.244699640000497}\n",
      "{'word': 'back', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'company', 'weight': 0.2835723137971876}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'fair', 'weight': 0.3383602703392845}\n",
      "{'word': 'price', 'weight': 0.3383602703392845}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'owner', 'weight': 0.45419450284733365}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'also', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'pay', 'weight': 0.5419477406385818}\n",
      "{'word': 'up', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': '10', 'weight': 0.5419477406385818}\n",
      "{'word': '000', 'weight': 0.45419450284733365}\n",
      "{'word': 'each', 'weight': 0.05}\n",
      "\n",
      "\n",
      "easy words: 71 difficult words: 62\n",
      "easy tfidf sum: 7.858247259247319 easy tfidf sum: 17.567720867086713\n",
      "relative tfidf 0.31 0.69\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "My Friends level is calculated\n",
      "=====================================\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'pres'})\n",
      "('get', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'name', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'sonya', 'weight': 1.0}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': '20', 'weight': 0.6296805322657705}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'come', 'weight': 0.45497786162734566}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'morocco', 'weight': 0.6296805322657705}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'north', 'weight': 0.7071067811865476}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'africa', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'student', 'weight': 1.0}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'study', 'weight': 0.5773502691896257}\n",
      "{'word': 'history', 'weight': 0.5773502691896257}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'university', 'weight': 0.5773502691896257}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'get', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'many', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.5856670009087421}\n",
      "{'word': 'friend', 'weight': 0.8105517651862584}\n",
      "{'word': 'here', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'come', 'weight': 0.45497786162734566}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'different', 'weight': 0.6296805322657705}\n",
      "{'word': 'part', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'world', 'weight': 0.6296805322657705}\n",
      "\n",
      "\n",
      "{'word': 'yuko', 'weight': 0.6594526782804071}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'japan', 'weight': 0.751746077547996}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'speak', 'weight': 0.5856670009087421}\n",
      "{'word': 'japanese', 'weight': 0.8105517651862584}\n",
      "\n",
      "\n",
      "{'word': 'laura', 'weight': 0.5794845762021732}\n",
      "{'word': 'come', 'weight': 0.47730877000652266}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'italy', 'weight': 0.6605860761616522}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'speak', 'weight': 0.5856670009087421}\n",
      "{'word': 'italian', 'weight': 0.8105517651862584}\n",
      "\n",
      "\n",
      "{'word': 'toby', 'weight': 0.5013843519116624}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'roby', 'weight': 0.5013843519116624}\n",
      "{'word': 'come', 'weight': 0.41297932359114203}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'canada', 'weight': 0.5715553705481625}\n",
      "\n",
      "\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'speak', 'weight': 0.47730877000652266}\n",
      "{'word': 'english', 'weight': 0.5794845762021732}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'french', 'weight': 0.6605860761616522}\n",
      "\n",
      "\n",
      "{'word': 'jimmy', 'weight': 0.6594526782804071}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'india', 'weight': 0.751746077547996}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'speak', 'weight': 0.47730877000652266}\n",
      "{'word': 'hindi', 'weight': 0.6605860761616522}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'english', 'weight': 0.5794845762021732}\n",
      "\n",
      "\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'long', 'weight': 0.5773502691896257}\n",
      "{'word': 'holiday', 'weight': 0.5773502691896257}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'miss', 'weight': 0.5773502691896257}\n",
      "{'word': 'them', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'look', 'weight': 0.4061839838496295}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'map', 'weight': 0.4061839838496295}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'say', 'weight': 0.4061839838496295}\n",
      "{'word': 'good', 'weight': 0.2934896521799722}\n",
      "{'word': 'morning', 'weight': 0.4061839838496295}\n",
      "{'word': 'yuko', 'weight': 0.35631594766404623}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'jimmy', 'weight': 0.35631594766404623}\n",
      "\n",
      "\n",
      "{'word': 'good', 'weight': 0.47730877000652266}\n",
      "{'word': 'afternoon', 'weight': 0.6605860761616522}\n",
      "{'word': 'laura', 'weight': 0.5794845762021732}\n",
      "\n",
      "\n",
      "{'word': 'good', 'weight': 0.41297932359114203}\n",
      "{'word': 'evening', 'weight': 0.5715553705481625}\n",
      "{'word': 'toby', 'weight': 0.5013843519116624}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'roby', 'weight': 0.5013843519116624}\n",
      "\n",
      "\n",
      "easy words: 92 difficult words: 16\n",
      "easy tfidf sum: 24.873168564727244 easy tfidf sum: 8.66986024335644\n",
      "relative tfidf 0.74 0.26\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "Good Advice From A Doctor level is calculated\n",
      "=====================================\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'mom', 'weight': 0.7071067811865476}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'doctor', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'know', 'weight': 0.5773502691896258}\n",
      "{'word': 'many', 'weight': 0.05}\n",
      "{'word': 'useful', 'weight': 0.5773502691896258}\n",
      "{'word': 'thing', 'weight': 0.5773502691896258}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'friend', 'weight': 0.3981252234300505}\n",
      "{'word': 'toby', 'weight': 0.5296213447576713}\n",
      "{'word': 'often', 'weight': 0.05}\n",
      "{'word': 'catch', 'weight': 0.5296213447576713}\n",
      "{'word': 'cold', 'weight': 0.5296213447576713}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'tell', 'weight': 0.25109483081077144}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'stay', 'weight': 0.25109483081077144}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'home', 'weight': 0.25109483081077144}\n",
      "{'word': 'drink', 'weight': 0.25109483081077144}\n",
      "{'word': 'hot', 'weight': 0.25109483081077144}\n",
      "{'word': 'tea', 'weight': 0.25109483081077144}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'honey', 'weight': 0.25109483081077144}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'eat', 'weight': 0.18875218419380208}\n",
      "{'word': 'lemon', 'weight': 0.25109483081077144}\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'friend', 'weight': 0.18875218419380208}\n",
      "{'word': 'laura', 'weight': 0.25109483081077144}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'headache', 'weight': 0.25109483081077144}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'advise', 'weight': 0.21462672034413371}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'listen', 'weight': 0.25109483081077144}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'loud', 'weight': 0.25109483081077144}\n",
      "{'word': 'music', 'weight': 0.25109483081077144}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'sleep', 'weight': 0.25109483081077144}\n",
      "{'word': 'more', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'yuko', 'weight': 0.46803237682201176}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'toothache', 'weight': 0.46803237682201176}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'say', 'weight': 0.46803237682201176}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'eat', 'weight': 0.3518277660807251}\n",
      "{'word': 'sweet', 'weight': 0.46803237682201176}\n",
      "\n",
      "\n",
      "{'word': 'eat', 'weight': 0.2933825421139075}\n",
      "{'word': 'more', 'weight': 0.05}\n",
      "{'word': 'vegetable', 'weight': 0.3902833765318864}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'fruit', 'weight': 0.3902833765318864}\n",
      "{'word': 'roby', 'weight': 0.3902833765318864}\n",
      "{'word': 'want', 'weight': 0.3902833765318864}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'lose', 'weight': 0.3902833765318864}\n",
      "{'word': 'weight', 'weight': 0.3902833765318864}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'fat', 'weight': 1.0}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'difficult', 'weight': 0.5}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'walk', 'weight': 0.5}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'run', 'weight': 0.5}\n",
      "{'word': 'fast', 'weight': 0.5}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'advise', 'weight': 0.6497481581610048}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'sport', 'weight': 0.7601495451333123}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.6243810431695077}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'help', 'weight': 0.6243810431695077}\n",
      "{'word': 'friend', 'weight': 0.46935767370004156}\n",
      "\n",
      "\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'profession', 'weight': 0.5773502691896258}\n",
      "{'word': 'would', 'weight': 0.05}\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'like', 'weight': 0.5773502691896258}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'choose', 'weight': 0.5773502691896258}\n",
      "\n",
      "\n",
      "easy words: 95 difficult words: 14\n",
      "easy tfidf sum: 19.223439119560908 easy tfidf sum: 5.636381573742935\n",
      "relative tfidf 0.77 0.23\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "About The Weather level is calculated\n",
      "=====================================\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'weather', 'weight': 0.6146674227977137}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'different', 'weight': 0.7887863838525718}\n",
      "\n",
      "\n",
      "{'word': 'toby', 'weight': 0.41672378883611416}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'roby', 'weight': 0.41672378883611416}\n",
      "{'word': 'live', 'weight': 0.3629137901628554}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'canada', 'weight': 0.41672378883611416}\n",
      "{'word': 'where', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'cold', 'weight': 0.41672378883611416}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'winter', 'weight': 0.41672378883611416}\n",
      "\n",
      "\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'snow', 'weight': 0.6567404191500098}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'street', 'weight': 0.7541167163341955}\n",
      "\n",
      "\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'too', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "{'word': 'snow', 'weight': 0.3629137901628554}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'road', 'weight': 0.41672378883611416}\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'even', 'weight': 0.05}\n",
      "{'word': 'drive', 'weight': 0.41672378883611416}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'supermarket', 'weight': 0.41672378883611416}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'buy', 'weight': 0.41672378883611416}\n",
      "{'word': 'food', 'weight': 0.41672378883611416}\n",
      "\n",
      "\n",
      "{'word': 'laura', 'weight': 0.6021015204487483}\n",
      "{'word': 'life', 'weight': 0.5243543822136046}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'italy', 'weight': 0.6021015204487483}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'warm', 'weight': 0.7071067811865476}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'sunny', 'weight': 0.7071067811865476}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'rain', 'weight': 1.0}\n",
      "{'word': 'often', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'great', 'weight': 0.5265169768893333}\n",
      "{'word': 'weather', 'weight': 0.41029211440381363}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'picnic', 'weight': 0.5265169768893333}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'friend', 'weight': 0.5265169768893333}\n",
      "\n",
      "\n",
      "{'word': 'jimmy', 'weight': 0.6021015204487483}\n",
      "{'word': 'life', 'weight': 0.5243543822136046}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'india', 'weight': 0.6021015204487483}\n",
      "\n",
      "\n",
      "{'word': 'sometimes', 'weight': 0.05}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'heavy', 'weight': 0.8160781821346454}\n",
      "{'word': 'rain', 'weight': 0.5779415200898896}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'call', 'weight': 0.05}\n",
      "{'word': 'tropical', 'weight': 0.8160781821346454}\n",
      "{'word': 'rain', 'weight': 0.5779415200898896}\n",
      "\n",
      "\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'such', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'weather', 'weight': 0.45194992585317634}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'like', 'weight': 0.45194992585317634}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'look', 'weight': 0.5050852737010433}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'sky', 'weight': 0.5799753402800493}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'cloud', 'weight': 0.3684631718696798}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'interesting', 'weight': 0.3684631718696798}\n",
      "{'word': '–', 'weight': 0.05}\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'look', 'weight': 0.320884887834521}\n",
      "{'word': 'like', 'weight': 0.2871275580884492}\n",
      "{'word': 'horse', 'weight': 0.3684631718696798}\n",
      "{'word': 'dragon', 'weight': 0.3684631718696798}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'other', 'weight': 0.05}\n",
      "{'word': 'funny', 'weight': 0.3684631718696798}\n",
      "{'word': 'creature', 'weight': 0.3684631718696798}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'live', 'weight': 0.4428501489955992}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'hot', 'weight': 0.5085124814167843}\n",
      "{'word': 'morocco', 'weight': 0.5085124814167843}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'like', 'weight': 0.39626198272629576}\n",
      "{'word': 'rain', 'weight': 0.3601253935327104}\n",
      "{'word': 'too', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'ask', 'weight': 1.0}\n",
      "{'word': 'me', 'weight': 0.05}\n",
      "{'word': 'why', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'because', 'weight': 0.05}\n",
      "{'word': 'after', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'rainbow', 'weight': 1.0}\n",
      "\n",
      "\n",
      "easy words: 120 difficult words: 13\n",
      "easy tfidf sum: 25.789249909673185 easy tfidf sum: 5.641351790181041\n",
      "relative tfidf 0.82 0.18\n",
      "text is quite easy\n",
      "\n",
      "\n",
      "=====================================\n",
      "A Lucky Puppy level is calculated\n",
      "=====================================\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'mr', 'weight': 0.5150489443796332}\n",
      "{'word': 'smith', 'weight': 0.5150489443796332}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'farmer', 'weight': 0.6851636080432548}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'live', 'weight': 0.45944132869911597}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'country', 'weight': 0.5375068980796348}\n",
      "{'word': 'near', 'weight': 0.5375068980796348}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'forest', 'weight': 0.45944132869911597}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'ten', 'weight': 0.05}\n",
      "{'word': 'cow', 'weight': 0.5296213447576713}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'make', 'weight': 0.5296213447576713}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.5296213447576713}\n",
      "{'word': 'cheese', 'weight': 0.3981252234300505}\n",
      "\n",
      "\n",
      "{'word': 'many', 'weight': 0.05}\n",
      "{'word': 'people', 'weight': 0.46803237682201176}\n",
      "{'word': 'want', 'weight': 0.46803237682201176}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'buy', 'weight': 0.46803237682201176}\n",
      "{'word': 'his', 'weight': 0.05}\n",
      "{'word': 'famous', 'weight': 0.46803237682201176}\n",
      "{'word': 'cheese', 'weight': 0.3518277660807251}\n",
      "\n",
      "\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'sunny', 'weight': 0.473447827447549}\n",
      "{'word': 'morning', 'weight': 0.473447827447549}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'walk', 'weight': 0.473447827447549}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'his', 'weight': 0.05}\n",
      "{'word': 'dog', 'weight': 0.40468596717429406}\n",
      "{'word': 'spot', 'weight': 0.40468596717429406}\n",
      "\n",
      "\n",
      "{'word': 'suddenly', 'weight': 0.3660955486168265}\n",
      "{'word': 'sport', 'weight': 0.3129251473576937}\n",
      "{'word': 'start', 'weight': 0.3660955486168265}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'funny', 'weight': 0.3660955486168265}\n",
      "{'word': 'thing', 'weight': 0.3660955486168265}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'run', 'weight': 0.3660955486168265}\n",
      "{'word': 'quickly', 'weight': 0.3660955486168265}\n",
      "{'word': 'into', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'forest', 'weight': 0.3129251473576937}\n",
      "\n",
      "\n",
      "{'word': 'mr', 'weight': 0.5150489443796332}\n",
      "{'word': 'smith', 'weight': 0.5150489443796332}\n",
      "{'word': 'follow', 'weight': 0.6851636080432548}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'sport', 'weight': 0.3224646153372414}\n",
      "{'word': 'show', 'weight': 0.05}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'little', 'weight': 0.37725590691009325}\n",
      "{'word': 'puppy', 'weight': 0.2835895752985732}\n",
      "{'word': 'dog', 'weight': 0.3224646153372414}\n",
      "{'word': 'sit', 'weight': 0.37725590691009325}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'grass', 'weight': 0.37725590691009325}\n",
      "{'word': 'under', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'big', 'weight': 0.37725590691009325}\n",
      "{'word': 'tree', 'weight': 0.37725590691009325}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'puppy', 'weight': 0.46935767370004156}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'alone', 'weight': 0.05}\n",
      "{'word': 'hungry', 'weight': 0.6243810431695077}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'cold', 'weight': 0.6243810431695077}\n",
      "\n",
      "\n",
      "{'word': 'now', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'puppy', 'weight': 0.2705215481468599}\n",
      "{'word': 'live', 'weight': 0.3076051962480269}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'farm', 'weight': 0.3598716626921778}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'spot', 'weight': 0.3076051962480269}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'mr', 'weight': 0.2705215481468599}\n",
      "{'word': 'smith', 'weight': 0.2705215481468599}\n",
      "{'word': 'eat', 'weight': 0.3598716626921778}\n",
      "{'word': 'cheese', 'weight': 0.2705215481468599}\n",
      "{'word': 'every', 'weight': 0.05}\n",
      "{'word': 'day', 'weight': 0.3598716626921778}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'happy', 'weight': 0.3598716626921778}\n",
      "\n",
      "\n",
      "easy words: 89 difficult words: 16\n",
      "easy tfidf sum: 18.522494303458327 easy tfidf sum: 6.434712014298444\n",
      "relative tfidf 0.74 0.26\n",
      "text is moderate\n",
      "\n",
      "\n",
      "=====================================\n",
      "Anne of Green Gables level is calculated\n",
      "=====================================\n",
      "PAST CONTINIOUS\n",
      "('go', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'inf'})\n",
      "('happen', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'pres', 'Number': 'sing', 'Person': 3})\n",
      "('take', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "('go', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'here', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'matthew', 'weight': 0.2463040120183952}\n",
      "{'word': 'cuthbert', 'weight': 0.2809189594541142}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'wear', 'weight': 0.38844280712522317}\n",
      "{'word': 'his', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.38844280712522317}\n",
      "{'word': 'suit', 'weight': 0.38844280712522317}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'clothe', 'weight': 0.38844280712522317}\n",
      "{'word': 'which', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'proof', 'weight': 0.38844280712522317}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'go', 'weight': 0.05, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'avonlea', 'weight': 0.32554538830437063}\n",
      "\n",
      "\n",
      "{'word': 'now', 'weight': 0.05}\n",
      "{'word': 'where', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'matthew', 'weight': 0.6592621372425836}\n",
      "{'word': 'cuthbert', 'weight': 0.7519131827533952}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'why', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'I', 'weight': 0.05}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'just', 'weight': 0.3145893585733343}\n",
      "{'word': 'step', 'weight': 0.3145893585733343}\n",
      "{'word': 'over', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'green', 'weight': 0.3145893585733343}\n",
      "{'word': 'gable', 'weight': 0.3145893585733343}\n",
      "{'word': 'after', 'weight': 0.05}\n",
      "{'word': 'tea', 'weight': 0.3145893585733343}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'find', 'weight': 0.05}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'from', 'weight': 0.05}\n",
      "{'word': 'marilla', 'weight': 0.26365043454174164}\n",
      "{'word': 'where', 'weight': 0.05}\n",
      "{'word': 'matthew', 'weight': 0.19947497992909644}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'why', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'worthy', 'weight': 0.3145893585733343}\n",
      "{'word': 'woman', 'weight': 0.3145893585733343}\n",
      "{'word': 'finally', 'weight': 0.3145893585733343}\n",
      "{'word': 'conclude', 'weight': 0.3145893585733343}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'generally', 'weight': 0.35355339059327373}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'town', 'weight': 0.35355339059327373}\n",
      "{'word': 'this', 'weight': 0.05}\n",
      "{'word': 'time', 'weight': 0.35355339059327373}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'year', 'weight': 0.35355339059327373}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'never', 'weight': 0.05}\n",
      "{'word': 'visit', 'weight': 0.35355339059327373}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'drive', 'weight': 0.35355339059327373}\n",
      "{'word': 'fast', 'weight': 0.35355339059327373}\n",
      "{'word': 'enough', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'doctor', 'weight': 0.35355339059327373}\n",
      "\n",
      "\n",
      "{'word': 'yet', 'weight': 0.05}\n",
      "{'word': 'something', 'weight': 0.05}\n",
      "{'word': 'must', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'happen', 'weight': 0.7071067811865475, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'since', 'weight': 0.05}\n",
      "{'word': 'last', 'weight': 0.05}\n",
      "{'word': 'night', 'weight': 0.7071067811865475}\n",
      "\n",
      "\n",
      "{'word': 'I', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'puzzle', 'weight': 0.3222883300855695}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'know', 'weight': 0.644576660171139}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'minute', 'weight': 0.3222883300855695}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'peace', 'weight': 0.3222883300855695}\n",
      "{'word': 'until', 'weight': 0.05}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'know', 'weight': 0.644576660171139}\n",
      "{'word': 'what', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'take', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'matthew', 'weight': 0.20435674768768322}\n",
      "{'word': 'cuthbert', 'weight': 0.23307653191440278}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'avonlea', 'weight': 0.2701027735335328}\n",
      "{'word': 'today', 'weight': 0.3222883300855695}\n",
      "\n",
      "\n",
      "{'word': 'I', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "{'word': 'marilla', 'weight': 1.0}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'find', 'weight': 0.05}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'where', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'go', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'hope', 'weight': 0.7071067811865475}\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'tell', 'weight': 0.7071067811865475}\n",
      "{'word': 'me', 'weight': 0.05}\n",
      "{'word': 'everything', 'weight': 0.05}\n",
      "\n",
      "\n",
      "easy words: 107 difficult words: 32\n",
      "easy tfidf sum: 14.296853313733108 easy tfidf sum: 7.946430986609288\n",
      "relative tfidf 0.64 0.36\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "We Like Sports! level is calculated\n",
      "=====================================\n",
      "Phrasal Verb found: ball up\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'friend', 'weight': 0.6104754843093458}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'like', 'weight': 0.6104754843093458}\n",
      "{'word': 'sport', 'weight': 0.5046180398227349}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'much', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'big', 'weight': 0.4482240481737431}\n",
      "{'word': 'sport', 'weight': 0.3705012672977909}\n",
      "{'word': 'center', 'weight': 0.4482240481737431}\n",
      "{'word': 'near', 'weight': 0.4482240481737431}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'university', 'weight': 0.5099160900059508}\n",
      "\n",
      "\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'play', 'weight': 0.35636712396822556}\n",
      "{'word': 'football', 'weight': 0.49046345181456014}\n",
      "{'word': 'volleyball', 'weight': 0.49046345181456014}\n",
      "{'word': 'basketball', 'weight': 0.3890234957647869}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'tennis', 'weight': 0.49046345181456014}\n",
      "{'word': 'there', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'like', 'weight': 0.6327730305200935}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'play', 'weight': 0.5230491551597477}\n",
      "{'word': 'basketball', 'weight': 0.5709797484439277}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'exciting', 'weight': 0.32526835586894126}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'bounce', 'weight': 0.32526835586894126}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'ball', 'weight': 0.6505367117378825, 'phrasal_verb': 'yes'}\n",
      "{'word': 'up', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'down', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'run', 'weight': 0.21864233357893434}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'jump', 'weight': 0.32526835586894126}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'throw', 'weight': 0.32526835586894126}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'ball', 'weight': 0.6505367117378825}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "{'word': 'how', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'basket', 'weight': 0.32526835586894126}\n",
      "\n",
      "\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'unfortunately', 'weight': 0.5148440654525736}\n",
      "{'word': 'today', 'weight': 0.4525558140216501}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'playground', 'weight': 0.5148440654525736}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'busy', 'weight': 0.5148440654525736}\n",
      "\n",
      "\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'must', 'weight': 0.05}\n",
      "{'word': 'wait', 'weight': 0.6592938044426995}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'little', 'weight': 0.5795293456217987}\n",
      "{'word': 'before', 'weight': 0.05}\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'play', 'weight': 0.4790380120477301}\n",
      "\n",
      "\n",
      "{'word': 'ben', 'weight': 0.4924976602342694}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.6913396359605414}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'run', 'weight': 0.528673398625008}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'say', 'weight': 0.3138716584200978}\n",
      "{'word': 'let', 'weight': 0.35707189178573673}\n",
      "{'word': 'us', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'little', 'weight': 0.3138716584200978}\n",
      "{'word': 'race', 'weight': 0.3138716584200978}\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'big', 'weight': 0.3138716584200978}\n",
      "{'word': 'square', 'weight': 0.35707189178573673}\n",
      "{'word': 'behind', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'sport', 'weight': 0.2594458010169697}\n",
      "{'word': 'center', 'weight': 0.3138716584200978}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'start', 'weight': 0.35707189178573673}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'run', 'weight': 0.2400203716925201}\n",
      "\n",
      "\n",
      "{'word': 'ben', 'weight': 0.4924976602342694}\n",
      "{'word': 'run', 'weight': 0.528673398625008}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'alice', 'weight': 0.6913396359605414}\n",
      "\n",
      "\n",
      "{'word': 'alice', 'weight': 0.7071067811865476}\n",
      "{'word': 'win', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "{'word': 'ben', 'weight': 0.351995320498093}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'angry', 'weight': 0.5621183170061951}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'want', 'weight': 0.5621183170061951}\n",
      "{'word': 'another', 'weight': 0.05}\n",
      "{'word': 'race', 'weight': 0.4941106047431996}\n",
      "\n",
      "\n",
      "{'word': 'ben', 'weight': 0.4924976602342694}\n",
      "{'word': 'run', 'weight': 0.528673398625008}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'joe', 'weight': 0.6913396359605414}\n",
      "\n",
      "\n",
      "{'word': 'joe', 'weight': 0.7071067811865476}\n",
      "{'word': 'win', 'weight': 0.7071067811865476}\n",
      "\n",
      "\n",
      "{'word': 'ben', 'weight': 0.5307266711085283}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'sad', 'weight': 0.8475430375939973}\n",
      "\n",
      "\n",
      "{'word': 'my', 'weight': 0.05}\n",
      "{'word': 'leg', 'weight': 0.7071067811865475}\n",
      "{'word': 'hurt', 'weight': 0.7071067811865475}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'cry', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'take', 'weight': 0.05}\n",
      "{'word': 'him', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'hospital', 'weight': 0.3452120821353869}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'doctor', 'weight': 0.3452120821353869}\n",
      "{'word': 'say', 'weight': 0.3034467042046154}\n",
      "{'word': 'no', 'weight': 0.05}\n",
      "{'word': 'more', 'weight': 0.05}\n",
      "{'word': 'sport', 'weight': 0.2508285508625101}\n",
      "{'word': 'today', 'weight': 0.3034467042046154}\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'near', 'weight': 0.3034467042046154}\n",
      "{'word': 'supermarket', 'weight': 0.3452120821353869}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'buy', 'weight': 0.3452120821353869}\n",
      "{'word': 'some', 'weight': 0.05}\n",
      "{'word': 'ice', 'weight': 0.3034467042046154}\n",
      "{'word': 'cream', 'weight': 0.3034467042046154}\n",
      "\n",
      "\n",
      "{'word': 'ben', 'weight': 0.3080184827219428}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'play', 'weight': 0.7148066794703861}\n",
      "{'word': 'basketball', 'weight': 0.3901546671128718}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'can', 'weight': 0.05}\n",
      "{'word': 'play', 'weight': 0.7148066794703861}\n",
      "{'word': 'karaoke', 'weight': 0.4918895821383463}\n",
      "\n",
      "\n",
      "{'word': 'we', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'home', 'weight': 0.3236714504172421}\n",
      "{'word': 'sing', 'weight': 0.3236714504172421}\n",
      "{'word': 'song', 'weight': 0.3236714504172421}\n",
      "{'word': 'together', 'weight': 0.05}\n",
      "{'word': 'watch', 'weight': 0.3236714504172421}\n",
      "{'word': 'funny', 'weight': 0.3236714504172421}\n",
      "{'word': 'dvds', 'weight': 0.3236714504172421}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'enjoy', 'weight': 0.3236714504172421}\n",
      "{'word': 'tasty', 'weight': 0.3236714504172421}\n",
      "{'word': 'ice', 'weight': 0.284512159211509}\n",
      "{'word': 'cream', 'weight': 0.284512159211509}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.894427190999916}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'have', 'weight': 0.05}\n",
      "{'word': 'good', 'weight': 0.894427190999916}\n",
      "{'word': 'friend', 'weight': 0.447213595499958}\n",
      "\n",
      "\n",
      "easy words: 160 difficult words: 27\n",
      "easy tfidf sum: 37.33416713046447 easy tfidf sum: 9.302956568297356\n",
      "relative tfidf 0.8 0.2\n",
      "text is quite easy\n",
      "\n",
      "\n",
      "=====================================\n",
      "James Bond 1 level is calculated\n",
      "=====================================\n",
      "Phrasal Verb found: bear on\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'ian', 'weight': 0.31154744282300645}\n",
      "{'word': 'lancaster', 'weight': 0.3664870124440271}\n",
      "{'word': 'fleming', 'weight': 0.31154744282300645}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'bear', 'weight': 0.3664870124440271, 'phrasal_verb': 'yes'}\n",
      "{'word': 'on', 'weight': 0.05, 'phrasal_verb': 'yes'}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': '28th', 'weight': 0.3664870124440271}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'may', 'weight': 0.05}\n",
      "{'word': '1908', 'weight': 0.3664870124440271}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'canterbury', 'weight': 0.3664870124440271}\n",
      "{'word': 'england', 'weight': 0.3664870124440271}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'newspaper', 'weight': 0.6060433187339399}\n",
      "{'word': 'journalist', 'weight': 0.5151921890284284}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'writer', 'weight': 0.6060433187339399}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'create', 'weight': 0.3598459955665589}\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'most', 'weight': 0.05}\n",
      "{'word': 'famous', 'weight': 0.3059019717539992}\n",
      "{'word': 'character', 'weight': 0.3598459955665589}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'twentieth', 'weight': 0.3598459955665589}\n",
      "{'word': 'century', 'weight': 0.3598459955665589}\n",
      "{'word': 'fiction', 'weight': 0.3598459955665589}\n",
      "{'word': 'jame', 'weight': 0.3598459955665589}\n",
      "{'word': 'bond', 'weight': 0.3598459955665589}\n",
      "\n",
      "\n",
      "{'word': 'ian', 'weight': 0.3502555705694014}\n",
      "{'word': 'fleming', 'weight': 0.3502555705694014}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'educate', 'weight': 0.4120210921544398}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'eton', 'weight': 0.3502555705694014}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'famous', 'weight': 0.3502555705694014}\n",
      "{'word': 'school', 'weight': 0.4120210921544398}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'boy', 'weight': 0.4120210921544398}\n",
      "\n",
      "\n",
      "{'word': 'after', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'leave', 'weight': 0.5607136322130087}\n",
      "{'word': 'eton', 'weight': 0.23832890708322513}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'train', 'weight': 0.28035681610650437}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'soldier', 'weight': 0.28035681610650437}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'sandhurst', 'weight': 0.28035681610650437}\n",
      "{'word': 'military', 'weight': 0.28035681610650437}\n",
      "{'word': 'academy', 'weight': 0.28035681610650437}\n",
      "{'word': 'but', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'leave', 'weight': 0.5607136322130087}\n",
      "{'word': 'after', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'short', 'weight': 0.28035681610650437}\n",
      "{'word': 'time', 'weight': 0.28035681610650437}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'europe', 'weight': 0.28035681610650437}\n",
      "\n",
      "\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'study', 'weight': 0.4472135954999579}\n",
      "{'word': 'language', 'weight': 0.4472135954999579}\n",
      "{'word': 'at', 'weight': 0.05}\n",
      "{'word': 'munich', 'weight': 0.4472135954999579}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'geneva', 'weight': 0.4472135954999579}\n",
      "{'word': 'university', 'weight': 0.4472135954999579}\n",
      "\n",
      "\n",
      "{'word': 'flem', 'weight': 0.4579851637885095}\n",
      "{'word': 's', 'weight': 0.05}\n",
      "{'word': 'first', 'weight': 0.05}\n",
      "{'word': 'job', 'weight': 0.5387481691380603}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'journalist', 'weight': 0.4579851637885095}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'russia', 'weight': 0.5387481691380603}\n",
      "\n",
      "\n",
      "{'word': 'while', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'work', 'weight': 0.2807560386721712}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'news', 'weight': 0.3302657242384706}\n",
      "{'word': 'agency', 'weight': 0.3302657242384706}\n",
      "{'word': 'call', 'weight': 0.05}\n",
      "{'word': 'reuter', 'weight': 0.2807560386721712}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'hear', 'weight': 0.3302657242384706}\n",
      "{'word': 'about', 'weight': 0.05}\n",
      "{'word': 'spy', 'weight': 0.2807560386721712}\n",
      "{'word': 'who', 'weight': 0.05}\n",
      "{'word': 'sell', 'weight': 0.3302657242384706}\n",
      "{'word': 'government', 'weight': 0.3302657242384706}\n",
      "{'word': 'secret', 'weight': 0.3302657242384706}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'other', 'weight': 0.05}\n",
      "{'word': 'country', 'weight': 0.3302657242384706}\n",
      "\n",
      "\n",
      "{'word': 'flem', 'weight': 0.32384441499768674}\n",
      "{'word': 'send', 'weight': 0.3809524837493595}\n",
      "{'word': 'report', 'weight': 0.3809524837493595}\n",
      "{'word': 'about', 'weight': 0.05}\n",
      "{'word': 'these', 'weight': 0.05}\n",
      "{'word': 'spy', 'weight': 0.32384441499768674}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'their', 'weight': 0.05}\n",
      "{'word': 'special', 'weight': 0.3809524837493595}\n",
      "{'word': 'investigation', 'weight': 0.3809524837493595}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'reuter', 'weight': 0.32384441499768674}\n",
      "{'word': 'in', 'weight': 0.05}\n",
      "{'word': 'london', 'weight': 0.32384441499768674}\n",
      "\n",
      "\n",
      "{'word': 'when', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'return', 'weight': 0.5387481691380603}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'london', 'weight': 0.4579851637885095}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'work', 'weight': 0.4579851637885095}\n",
      "{'word': 'first', 'weight': 0.05}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'banker', 'weight': 0.5387481691380603}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'then', 'weight': 0.05}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "\n",
      "\n",
      "easy words: 93 difficult words: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy tfidf sum: 13.611327050628047 easy tfidf sum: 15.698806025880634\n",
      "relative tfidf 0.46 0.54\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n",
      "=====================================\n",
      "Charlotte’s Web level is calculated\n",
      "=====================================\n",
      "PAST CONTINIOUS\n",
      "('set', {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'})\n",
      "prev word is ('be', {74: 99, 'VerbForm': 'fin', 'Tense': 'past'})\n",
      "PRESENT PERFECT\n",
      "('have', {74: 99, 'VerbForm': 'fin', 'Tense': 'pres', 'Number': 'sing', 'Person': 3})\n",
      "('decide', {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'})\n",
      "WEIGHTS HAVE BEEN CALCULATED AS FOLLOWS (sentence by sentence)\n",
      "{'word': 'where', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'papa', 'weight': 0.751746077547996}\n",
      "{'word': 'go', 'weight': 0.05}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'that', 'weight': 0.05}\n",
      "{'word': 'ax', 'weight': 0.6594526782804071}\n",
      "\n",
      "\n",
      "{'word': 'say', 'weight': 0.36804737525465214}\n",
      "{'word': 'fern', 'weight': 0.3108586726281349}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'mother', 'weight': 0.4086234055059067}\n",
      "{'word': 'as', 'weight': 0.05}\n",
      "{'word': 'they', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'set', 'weight': 0.46581210813242396, 'diff_grammar': 'past_cont'}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'table', 'weight': 0.4086234055059067}\n",
      "{'word': 'for', 'weight': 0.05}\n",
      "{'word': 'breakfast', 'weight': 0.46581210813242396}\n",
      "\n",
      "\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'hoghouse', 'weight': 0.5315647491645762}\n",
      "{'word': 'reply', 'weight': 0.5315647491645762}\n",
      "{'word': 'mrs', 'weight': 0.46630346068370654}\n",
      "{'word': 'arable', 'weight': 0.46630346068370654}\n",
      "\n",
      "\n",
      "{'word': 'some', 'weight': 0.05}\n",
      "{'word': 'pig', 'weight': 0.4877385641283075}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'bear', 'weight': 0.6172969678615216}\n",
      "{'word': 'last', 'weight': 0.05}\n",
      "{'word': 'night', 'weight': 0.6172969678615216}\n",
      "\n",
      "\n",
      "{'word': 'i', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'see', 'weight': 0.05}\n",
      "{'word': 'why', 'weight': 0.05}\n",
      "{'word': 'he', 'weight': 0.05}\n",
      "{'word': 'need', 'weight': 0.5577216265505616}\n",
      "{'word': 'an', 'weight': 0.05}\n",
      "{'word': 'ax', 'weight': 0.4892490048811606}\n",
      "{'word': 'continue', 'weight': 0.5577216265505616}\n",
      "{'word': 'fern', 'weight': 0.3721942849888833}\n",
      "{'word': 'who', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'only', 'weight': 0.05}\n",
      "{'word': 'eight', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'well', 'weight': 0.05}\n",
      "{'word': 'say', 'weight': 0.45480538409054155}\n",
      "{'word': 'her', 'weight': 0.05}\n",
      "{'word': 'mother', 'weight': 0.5049462036264037}\n",
      "{'word': 'one', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'pig', 'weight': 0.45480538409054155}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'runt', 'weight': 0.5756157195975388}\n",
      "\n",
      "\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'very', 'weight': 0.05}\n",
      "{'word': 'small', 'weight': 0.6594526782804071}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'weak', 'weight': 0.751746077547996}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'will', 'weight': 0.05}\n",
      "{'word': 'never', 'weight': 0.05}\n",
      "{'word': 'amount', 'weight': 0.05}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'anything', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'so', 'weight': 0.05}\n",
      "{'word': 'your', 'weight': 0.05}\n",
      "{'word': 'father', 'weight': 0.5505238938327095}\n",
      "{'word': 'have', 'weight': 0.05, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'decide', 'weight': 0.6275722146801459, 'diff_grammar': 'pr_perf'}\n",
      "{'word': 'to', 'weight': 0.05}\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'away', 'weight': 0.5505238938327095}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'away', 'weight': 1.0}\n",
      "{'word': 'with', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'shriek', 'weight': 0.831788723784363}\n",
      "{'word': 'fern', 'weight': 0.5550923517624619}\n",
      "\n",
      "\n",
      "{'word': 'you', 'weight': 0.05}\n",
      "{'word': 'mean', 'weight': 0.7071067811865476}\n",
      "{'word': 'kill', 'weight': 0.7071067811865476}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'just', 'weight': 0.751746077547996}\n",
      "{'word': 'because', 'weight': 0.05}\n",
      "{'word': 'it', 'weight': 0.05}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'small', 'weight': 0.6594526782804071}\n",
      "{'word': 'than', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'other', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'mrs', 'weight': 0.42261513698609754}\n",
      "{'word': 'arable', 'weight': 0.42261513698609754}\n",
      "{'word': 'put', 'weight': 0.05}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'pitcher', 'weight': 0.4817620460199547}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'cream', 'weight': 0.4817620460199547}\n",
      "{'word': 'on', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'table', 'weight': 0.42261513698609754}\n",
      "\n",
      "\n",
      "{'word': 'do', 'weight': 0.05}\n",
      "{'word': 'not', 'weight': 0.05}\n",
      "{'word': 'yell', 'weight': 0.831788723784363}\n",
      "{'word': 'fern', 'weight': 0.5550923517624619}\n",
      "\n",
      "\n",
      "{'word': 'she', 'weight': 0.05}\n",
      "{'word': 'say', 'weight': 1.0}\n",
      "\n",
      "\n",
      "{'word': 'your', 'weight': 0.05}\n",
      "{'word': 'father', 'weight': 0.6594526782804071}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'right', 'weight': 0.751746077547996}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'pig', 'weight': 0.4877385641283075}\n",
      "{'word': 'would', 'weight': 0.05}\n",
      "{'word': 'probably', 'weight': 0.6172969678615216}\n",
      "{'word': 'die', 'weight': 0.6172969678615216}\n",
      "{'word': 'anyway', 'weight': 0.05}\n",
      "\n",
      "\n",
      "{'word': 'fern', 'weight': 0.28598232577950106}\n",
      "{'word': 'push', 'weight': 0.42853567164047085}\n",
      "{'word': 'a', 'weight': 0.05}\n",
      "{'word': 'chair', 'weight': 0.42853567164047085}\n",
      "{'word': 'out', 'weight': 0.05}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'way', 'weight': 0.42853567164047085}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'run', 'weight': 0.42853567164047085}\n",
      "{'word': 'outdoors', 'weight': 0.42853567164047085}\n",
      "\n",
      "\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'grass', 'weight': 0.447213595499958}\n",
      "{'word': 'be', 'weight': 0.05}\n",
      "{'word': 'wet', 'weight': 0.447213595499958}\n",
      "{'word': 'and', 'weight': 0.05}\n",
      "{'word': 'the', 'weight': 0.05}\n",
      "{'word': 'earth', 'weight': 0.447213595499958}\n",
      "{'word': 'smell', 'weight': 0.447213595499958}\n",
      "{'word': 'of', 'weight': 0.05}\n",
      "{'word': 'springtime', 'weight': 0.447213595499958}\n",
      "\n",
      "\n",
      "easy words: 108 difficult words: 39\n",
      "easy tfidf sum: 18.063410759963297 easy tfidf sum: 18.629315872577397\n",
      "relative tfidf 0.49 0.51\n",
      "text is very diffcult, diff realtive weight is more then 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_ind in range(len(texts['Text'])):\n",
    "    print(\"=====================================\")\n",
    "    print(texts[\"Title\"][text_ind], \"level is calculated\")\n",
    "    print(\"=====================================\")\n",
    "    calculate_text_level(texts['Text'][text_ind],print_debug_message = True, show_calucated_weights = True)\n",
    "    print(\"\\n\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
